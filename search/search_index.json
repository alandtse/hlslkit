{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"hlslkit","text":"<p>Tools for automating HLSL shader compilation, diagnostics, and define management, designed for projects like Skyrim Community Shaders.</p> <ul> <li>GitHub repository: https://github.com/alandtse/hlslkit/</li> <li>Documentation: https://alandtse.github.io/hlslkit/</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p><code>hlslkit</code> provides Python scripts to streamline HLSL shader workflows:</p> <ul> <li><code>compile_shaders.py</code>: Compiles shaders using <code>fxc.exe</code>, supports parallel compilation with dynamic job adjustment, and processes warnings/errors from a YAML configuration.</li> <li><code>generate_shader_defines.py</code>: Generates <code>shader_defines.yaml</code> from <code>CommunityShaders.log</code>, defining shader files, types, entries, and preprocessor defines.</li> <li><code>buffer_scan.py</code>: Scans HLSL files for buffer definitions, generates a markdown table of register usage, and detects conflicts across features.</li> </ul> <p>Key features:</p> <ul> <li>Robust path normalization (handles forward and backward slashes).</li> <li>Parallel compilation with CPU/memory-aware job scaling (requires <code>psutil</code>).</li> <li>Warning/error parsing and suppression for diagnostics.</li> <li>GitHub-integrated buffer reports with conflict detection.</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Poetry: For dependency management and virtual environment setup.</li> <li>Dependencies: Defined in <code>pyproject.toml</code>:<ul> <li>Required: <code>pyyaml</code>, <code>tqdm</code>, <code>py-markdown-table</code>, <code>psutil</code>, <code>pcpp</code>, <code>jellyfish</code>.</li> <li>Optional: Install with <code>poetry install -E gui</code>:<ul> <li><code>gui</code>: <code>gooey</code> (GUI interface, Windows recommended)</li> </ul> </li> </ul> </li> <li>fxc.exe: DirectX shader compiler (included in Windows SDK or DirectX SDK).</li> <li>CommunityShaders.log: Log file from Skyrim Community Shaders for <code>generate_shader_defines.py</code>.</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#local-development","title":"Local Development","text":"<ol> <li> <p>Clone the repository:</p> <p><code>bash git clone https://github.com/alandtse/hlslkit.git cd hlslkit</code></p> </li> <li> <p>Set up the environment using the Makefile:</p> <p><code>bash make install</code></p> <p>This installs Poetry dependencies, sets up the virtual environment, and configures pre-commit hooks.</p> </li> <li> <p>Ensure <code>fxc.exe</code> is in your PATH or specify its path with <code>--fxc</code>.</p> </li> </ol>"},{"location":"#usage","title":"Usage","text":""},{"location":"#workflow","title":"Workflow","text":"<ol> <li> <p>Generate Shader Defines:    Use <code>generate_shader_defines.py</code> to parse <code>CommunityShaders.log</code> and create <code>shader_defines.yaml</code>. This YAML defines shader configurations (files, types, entries, defines) for <code>compile_shaders.py</code>.</p> </li> <li> <p>Compile Shaders:    Use <code>compile_shaders.py</code> to compile shaders based on <code>shader_defines.yaml</code>, outputting compiled shaders to a specified directory.</p> </li> <li> <p>Scan Buffers:    Use <code>buffer_scan.py</code> to analyze HLSL files for buffer register usage and detect conflicts, generating a markdown report.</p> </li> </ol>"},{"location":"#example-calls","title":"Example Calls","text":""},{"location":"#1-generate-shader_definesyaml","title":"1. Generate <code>shader_defines.yaml</code>","text":"<p>Parse <code>CommunityShaders.log</code> to create the YAML configuration:</p> <pre><code>python generate_shader_defines.py --log \"E:\\Documents\\my games\\Skyrim Special Edition\\SKSE\\CommunityShaders.log\" --output shader_defines.yaml\n</code></pre> <ul> <li><code>--log</code>: Path to the log file (default: <code>CommunityShaders.log</code>).</li> <li><code>--output</code>: Output YAML file (default: <code>shader_defines.yaml</code>).</li> <li><code>--log-level</code>: Set the logging level (default: INFO, choices: DEBUG, INFO, WARNING, ERROR, CRITICAL).</li> <li><code>-d/--debug</code>: Enable debug output.</li> <li><code>-g/--gui</code>: Run with GUI (requires <code>gooey</code>).</li> </ul> <p>This generates <code>shader_defines.yaml</code>, e.g.:</p> <pre><code>common_defines: []\ncommon_pshader_defines: []\ncommon_vshader_defines: []\ncommon_cshader_defines: []\nfile_common_defines: {}\nwarnings: {}\nerrors: {}\nshaders:\n    - file: RunGrass.hlsl\n      configs:\n          VSHADER:\n              common_defines: []\n              entries:\n                  - entry: Grass:Vertex:4\n                    defines: [D3DCOMPILE_DEBUG]\n</code></pre> <p>Note: Ensure the log contains valid compilation lines (e.g., <code>[D] Compiling ...</code>). Untagged errors (e.g., <code>RunGrass.hlsl(10): error X1000: syntax error</code>) are not parsed.</p>"},{"location":"#2-compile-shaders","title":"2. Compile Shaders","text":"<p>Compile shaders using the generated YAML:</p> <pre><code>python compile_shaders.py --shader-dir build\\ALL-WITH-AUTO-DEPLOYMENT\\aio\\Shaders --output-dir build\\ShaderCache --config shader_defines.yaml --max-warnings 0\n</code></pre> <p>Or with custom options:</p> <pre><code>python compile_shaders.py --shader-dir build\\ALL-WITH-AUTO-DEPLOYMENT\\aio\\Shaders --output-dir build\\ShaderCache --config shader_defines.yaml --jobs 4 --max-warnings 0 --suppress-warnings X1519\n</code></pre> <ul> <li><code>--shader-dir</code>: Directory or file with HLSL files (default: <code>build/aio/Shaders</code>). If a file is provided, only that shader (with all its config variants) will be compiled.</li> <li><code>--output-dir</code>: Output directory for compiled shaders (default: <code>build/ShaderCache</code>).</li> <li><code>--config</code>: Path to <code>shader_defines.yaml</code> (default: <code>shader_defines.yaml</code>).</li> <li><code>--jobs</code>: Number of parallel jobs (default: dynamic based on CPU).</li> <li> <p><code>--max-warnings</code>: Maximum allowed warnings:</p> <ul> <li>Positive values (e.g., <code>5</code>): Maximum number of NEW warnings allowed.</li> <li>Negative values (e.g., <code>-3</code>): Must eliminate this many existing baseline warnings up to complete elimination of all warnings.</li> <li>Zero (<code>0</code>): No new warnings allowed (default).</li> </ul> </li> <li> <p><code>--suppress-warnings</code>: Comma-separated warning codes to suppress (e.g., <code>X1519,X3206</code>).</p> </li> <li><code>--fxc</code>: Path to <code>fxc.exe</code> (optional if in PATH).</li> <li><code>--strip-debug-defines</code>: Remove debug defines (e.g., <code>D3DCOMPILE_DEBUG</code>).</li> <li><code>--optimization-level</code>: Optimization level (0-3, default: 1 or 3 if stripping debug defines).</li> <li><code>--force-partial-precision</code>: Use 16-bit floats for performance.</li> <li><code>--extra-includes</code>: Comma-separated list of additional include directories for <code>fxc.exe</code> (these will be added as <code>/I</code> flags in addition to the shader's parent directory and shader-dir).</li> <li><code>-d/--debug</code>: Enable debug output.</li> <li><code>-g/--gui</code>: Run with GUI (requires <code>gooey</code>).</li> </ul> <p>Note: The parent directory of each shader file is always included as an <code>/I</code> path for <code>fxc.exe</code>.</p> <p>Compile a Single Shader File Example:</p> <p>You can compile just one shader file (with all its variants from the config) by passing the file path to <code>--shader-dir</code>:</p> <pre><code>python compile_shaders.py --shader-dir path/to/Lighting.hlsl --output-dir build/ShaderCache --config shader_defines.yaml\n</code></pre> <p>Additional Include Directories Example:</p> <p>You can specify extra include directories for <code>fxc.exe</code> using <code>--extra-includes</code>:</p> <pre><code>python compile_shaders.py --shader-dir src --output-dir build --config shader_defines.yaml --extra-includes path/to/includes1,path/to/includes2\n</code></pre>"},{"location":"#advanced-warning-control","title":"Advanced Warning Control","text":"<p>The <code>--max-warnings</code> parameter supports sophisticated warning management for CI/CD environments:</p> <p>Positive Values (Standard Limits):</p> <pre><code># Allow up to 5 new warnings\npython compile_shaders.py --max-warnings 5 [other options...]\n</code></pre> <p>Negative Values (Warning Reduction Requirements):</p> <pre><code># Require elimination of 3 existing baseline warnings\npython compile_shaders.py --max-warnings -3 [other options...]\n\n# Require elimination of 10 warnings (if baseline has fewer, target becomes zero warnings)\npython compile_shaders.py --max-warnings -10 [other options...]\n</code></pre> <p>How it works:</p> <ul> <li>Baseline warnings: Stored in <code>shader_defines.yaml</code> from previous compilations</li> <li>New warnings: Detected by comparing current compilation against baseline</li> <li>Negative values: Calculate target warning count as <code>baseline_count - abs(max_warnings)</code></li> <li>Zero targeting: If required reduction exceeds baseline count, target becomes 0 warnings</li> </ul> <p>Example scenarios:</p> <ul> <li>10 baseline warnings, <code>--max-warnings -3</code> \u2192 Target: 7 total warnings (must eliminate 3)</li> <li>5 baseline warnings, <code>--max-warnings -10</code> \u2192 Target: 0 total warnings (must eliminate all)</li> <li>0 baseline warnings, <code>--max-warnings -5</code> \u2192 Target: 0 total warnings (already achieved)</li> </ul> <p>This enables progressive warning cleanup in CI environments where teams can set requirements like \"each PR must eliminate at least 2 warnings\" while still allowing some new warnings if the overall count decreases.</p>"},{"location":"#3-scan-buffer-usage","title":"3. Scan Buffer Usage","text":"<p>Generate a markdown table of buffer register usage:</p> <pre><code>python buffer_scan.py\n</code></pre> <ul> <li>Run in a directory with HLSL files.</li> <li> <p>Outputs a markdown table to stdout, e.g.:</p> <p>```markdown</p> </li> <li> <p>Detects conflicts (e.g., same register used by multiple features).</p> </li> </ul>"},{"location":"#table-generated-on-2025-05-24-200923","title":"Table generated on 2025-05-24 20:09:23","text":"Register Feature Type Name File Register Type Buffer Type Number PSHADER VSHADER VR t0 Grass Buffer myBuffer src/RunGrass.hlsl:5 SRV t 0 False True False ```"},{"location":"#workflow-example","title":"Workflow Example","text":"<p>To compile shaders for Skyrim Community Shaders:</p> <ol> <li>Generate the YAML:     <code>bash     python generate_shader_defines.py --log \"E:\\Documents\\my games\\Skyrim Special Edition\\SKSE\\CommunityShaders.log\"</code></li> <li>Compile shaders:     <code>bash     python compile_shaders.py --shader-dir build\\ALL-WITH-AUTO-DEPLOYMENT\\aio\\Shaders --output-dir build\\ShaderCache --config shader_defines.yaml --jobs 4 --max-warnings 0</code></li> <li>Check buffer conflicts:     <code>bash     python buffer_scan.py</code></li> </ol>"},{"location":"#development","title":"Development","text":"<p>To contribute to <code>hlslkit</code>, clone the repository and set up the development environment:</p> <ol> <li> <p>Clone the repository:</p> <p><code>bash git clone https://github.com/alandtse/hlslkit.git cd hlslkit</code></p> </li> <li> <p>Set up the environment:     <code>bash     make install</code></p> </li> </ol>"},{"location":"#testing","title":"Testing","text":"<p>Run tests with coverage:</p> <pre><code>make test\n</code></pre> <p>This executes <code>pytest</code> with coverage, generating an XML report. View the HTML report:</p> <pre><code>poetry run pytest --cov --cov-config=pyproject.toml --cov-report=html\nopen htmlcov/index.html\n</code></pre> <p>Tests cover:</p> <ul> <li>Path normalization (forward/backward slashes).</li> <li>Shader compilation (success, missing files, warnings, timeouts).</li> <li>YAML parsing and define flattening.</li> <li>Log parsing (configs, warnings, tagged errors).</li> <li>Buffer scanning (register usage, conflicts, <code>#line</code> directives).</li> </ul> <p>Note: Untagged errors (e.g., <code>error X1000: syntax error</code>) in logs are not parsed by <code>generate_shader_defines.py</code>.</p>"},{"location":"#code-quality","title":"Code Quality","text":"<p>Run linting, type checking, and dependency checks:</p> <pre><code>make check\n</code></pre> <p>This runs:</p> <ul> <li>Poetry lock file consistency check.</li> <li>Pre-commit hooks (linting).</li> <li>Pyright for static type checking.</li> <li>Deptry for obsolete dependencies.</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>Build and serve documentation with MkDocs:</p> <pre><code>make docs\n</code></pre> <p>Test documentation builds:</p> <pre><code>make docs-test\n</code></pre>"},{"location":"#building","title":"Building","text":"<p>Build a wheel file:</p> <pre><code>make build\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<ul> <li>Submit issues or pull requests on GitHub.</li> <li>Run <code>make check</code> and <code>make test</code> before submitting changes.</li> <li>Follow the test suite and coverage guidelines.</li> <li>See CONTRIBUTING.md for branching, testing, code quality checks, and submitting pull requests.</li> </ul>"},{"location":"#cicd","title":"CI/CD","text":"<ul> <li>Triggers: Pull requests, merges to <code>main</code>, or new releases.</li> <li>Publishing: Configure PyPI/Artifactory per cookiecutter-poetry.</li> <li>Documentation: Enable MkDocs per cookiecutter-poetry.</li> <li>Codecov: Enable coverage reports per cookiecutter-poetry.</li> </ul>"},{"location":"#cicd-integration","title":"CI/CD Integration","text":""},{"location":"#github-actions","title":"GitHub Actions","text":"<p>For CI/CD pipelines using a Windows runner (e.g., windows-latest), you can install hlslkit directly from GitHub using pip:</p> <pre><code>- name: Setup Python\n  uses: actions/setup-python@v5\n  with:\n      python-version: \"3.11\"\n\n- name: Install hlslkit\n  run: pip install git+https://github.com/alandtse/hlslkit.git\n  shell: bash\n\n- name: Validate shader compilation\n  run: hlslkit-compile --shader-dir build/Shaders --output-dir build/ShaderCache --config shader_defines.yaml --max-warnings 0\n  shell: bash\n\n- name: Run buffer scan\n  run: hlslkit-buffer-scan &gt; buffer-scan-results.md\n  shell: bash\n</code></pre>"},{"location":"#pypi-installation","title":"PyPI Installation","text":"<p>Once published to PyPI, you can install hlslkit using:</p> <pre><code>pip install hlslkit\n</code></pre>"},{"location":"#docker","title":"Docker","text":"<p>You can also use the included Dockerfile for containerized builds:</p> <pre><code>docker build -t hlslkit:latest .\ndocker run --rm -v $(pwd):/workspace -w /workspace hlslkit:latest hlslkit-compile --help\n</code></pre>"},{"location":"#limitations","title":"Limitations","text":"<ul> <li>generate_shader_defines.py: Only parses <code>[E]</code> or <code>[W]</code> tagged errors in logs. Untagged errors (e.g., <code>RunGrass.hlsl(10): error X1000: syntax error</code>) are ignored.</li> <li>compile_shaders.py: Malformed YAML in <code>shader_defines.yaml</code> raises uncaught errors. Validate YAML before running.</li> <li>buffer_scan.py: Requires <code>pcpp</code> for preprocessing and assumes HLSL files are in the project directory or subdirectories.</li> </ul>"},{"location":"#license","title":"License","text":"<p>GPL-3.0-or-later.</p> <p>Repository initiated with fpgmaas/cookiecutter-poetry.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to <code>hlslkit</code>","text":"<p>Contributions to <code>hlslkit</code> are welcome and greatly appreciated! Whether you're fixing bugs, adding features, improving documentation, or reporting issues, your efforts help enhance our HLSL shader compilation and diagnostics tools for projects like Skyrim Community Shaders.</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>File bug reports at https://github.com/alandtse/hlslkit/issues. Include:</p> <ul> <li>Operating system name and version (e.g., Windows 10).</li> <li>Local setup details (e.g., Python version, <code>fxc.exe</code> availability, Poetry version).</li> <li>Steps to reproduce the bug, including logs (e.g., <code>CommunityShaders.log</code> snippets or <code>new_warnings.log</code> from <code>compile_shaders.py</code>).</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Check https://github.com/alandtse/hlslkit/issues for issues tagged \"bug\" and \"help wanted\". These are open for contributors to fix. Example bugs:</p> <ul> <li>Uncaught <code>yaml.YAMLError</code> in <code>compile_shaders.py</code>.</li> <li>Untagged error parsing in <code>generate_shader_defines.py</code>.</li> </ul>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look for issues tagged \"enhancement\" and \"help wanted\" at https://github.com/alandtse/hlslkit/issues. Propose new features via issues, explaining:</p> <ul> <li>How the feature works (e.g., new shader type support, enhanced warning suppression).</li> <li>A narrow scope for easier implementation.</li> <li>Relevance to HLSL workflows (e.g., compilation, buffer analysis, log parsing).</li> </ul>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>Improve <code>hlslkit</code>\u2019s documentation:</p> <ul> <li>Update docstrings in <code>compile_shaders.py</code>, <code>generate_shader_defines.py</code>, or <code>buffer_scan.py</code>.</li> <li>Enhance <code>README.md</code> or MkDocs pages (https://alandtse.github.io/hlslkit/).</li> <li>Write tutorials on shader automation (e.g., integrating with Skyrim Community Shaders).</li> </ul>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>Share feedback or feature proposals at https://github.com/alandtse/hlslkit/issues. Keep suggestions focused on HLSL shader automation.</p>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started","text":"<p>Follow these steps to set up <code>hlslkit</code> for local development. This guide assumes you have Poetry, Git, and Python 3.8+ installed.</p>"},{"location":"CONTRIBUTING/#1-fork-and-clone-the-repository","title":"1. Fork and Clone the Repository","text":"<p>Fork <code>hlslkit</code> on GitHub, then clone your fork:</p> <pre><code>cd &lt;your-working-directory&gt;\ngit clone git@github.com:YOUR_USERNAME/hlslkit.git\ncd hlslkit\n</code></pre>"},{"location":"CONTRIBUTING/#2-set-up-the-environment","title":"2. Set Up the Environment","text":"<p>Install dependencies and configure the Poetry environment:</p> <pre><code>make install\n</code></pre> <p>This:</p> <ul> <li>Installs dependencies from <code>pyproject.toml</code> (e.g., <code>pyyaml</code>, <code>tqdm</code>, optional <code>psutil</code>, <code>gooey</code>, <code>py-markdown-table</code>, <code>pcpp</code>).</li> <li>Sets up the virtual environment.</li> <li>Installs pre-commit hooks for linting/formatting.</li> </ul> <p>Activate the Poetry shell:</p> <pre><code>poetry shell\n</code></pre> <p>Note: Ensure <code>fxc.exe</code> (DirectX shader compiler) is in your PATH or specify its path with <code>--fxc</code> in <code>compile_shaders.py</code>. For <code>buffer_scan.py</code>, HLSL files should be in the project directory or subdirectories.</p>"},{"location":"CONTRIBUTING/#3-create-a-branch","title":"3. Create a Branch","text":"<p>Create a branch for your changes:</p> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre>"},{"location":"CONTRIBUTING/#4-make-changes","title":"4. Make Changes","text":"<p>Modify scripts, tests, or documentation. Key scripts:</p> <ul> <li><code>compile_shaders.py</code>: Compiles HLSL shaders using <code>fxc.exe</code>, supports parallel compilation, and processes warnings/errors from <code>shader_defines.yaml</code>.</li> <li><code>generate_shader_defines.py</code>: Generates <code>shader_defines.yaml</code> from <code>CommunityShaders.log</code>, defining shader configs.</li> <li><code>buffer_scan.py</code>: Scans HLSL files for buffer registers, generates markdown tables, and detects conflicts.</li> </ul> <p>Example Usage (for reference):</p> <ul> <li>Generate <code>shader_defines.yaml</code>:     <code>bash     python generate_shader_defines.py --log \"E:\\Documents\\my games\\Skyrim Special Edition\\SKSE\\CommunityShaders.log\" --output shader_defines.yaml</code></li> <li>Compile shaders:     <code>bash     python compile_shaders.py --shader-dir build\\ALL-WITH-AUTO-DEPLOYMENT\\aio\\Shaders --output-dir build\\ShaderCache --config shader_defines.yaml --jobs 4 --max-warnings 0</code></li> <li>Scan buffers:     <code>bash     python buffer_scan.py</code></li> </ul> <p>Limitations to Note:</p> <ul> <li><code>compile_shaders.py</code>: Malformed YAML in <code>shader_defines.yaml</code> raises uncaught errors. Validate YAML syntax.</li> <li><code>generate_shader_defines.py</code>: Only parses <code>[E]</code> or <code>[W]</code> tagged errors in logs. Untagged errors (e.g., <code>RunGrass.hlsl(10): error X1000: syntax error</code>) are ignored.</li> <li><code>buffer_scan.py</code>: Requires <code>pcpp</code> and assumes HLSL files are accessible.</li> </ul>"},{"location":"CONTRIBUTING/#5-add-tests","title":"5. Add Tests","text":"<p>Add test cases in the <code>tests</code> directory. Tests cover:</p> <ul> <li>Path normalization (forward/backward slashes).</li> <li>Shader compilation (success, missing files, warnings, timeouts).</li> <li>YAML parsing and define flattening.</li> <li>Log parsing (configs, tagged <code>[E]</code>/<code>[W]</code> errors).</li> <li>Buffer scanning (register usage, <code>#line</code> directives, conflicts).</li> </ul> <p>Run tests:</p> <pre><code>make test\n</code></pre> <p>View the HTML coverage report:</p> <pre><code>poetry run pytest --cov --cov-config=pyproject.toml --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"CONTRIBUTING/#6-check-code-quality","title":"6. Check Code Quality","text":"<p>Run linting, type checking, and dependency checks:</p> <pre><code>make check\n</code></pre> <p>This executes:</p> <ul> <li><code>poetry check --lock</code> (lock file consistency).</li> <li>Pre-commit hooks (linting/formatting).</li> <li>Pyright (static type checking).</li> <li>Deptry (obsolete dependencies).</li> </ul>"},{"location":"CONTRIBUTING/#7-update-documentation","title":"7. Update Documentation","text":"<p>For new functionality:</p> <ul> <li>Update docstrings in modified scripts.</li> <li>Add features to <code>README.md</code>\u2019s feature list.</li> <li>Enhance MkDocs pages:     <code>bash     make docs</code>     Test documentation builds:     <code>bash     make docs-test</code></li> </ul>"},{"location":"CONTRIBUTING/#8-commit-and-push","title":"8. Commit and Push","text":"<p>Commit changes:</p> <pre><code>git add .\ngit commit -m \"Your detailed description of changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre>"},{"location":"CONTRIBUTING/#9-optional-run-tox","title":"9. Optional: Run Tox","text":"<p>Test across multiple Python versions:</p> <pre><code>poetry run tox\n</code></pre> <p>This requires multiple Python versions installed. Alternatively, rely on the CI/CD pipeline, which runs <code>tox</code> automatically.</p>"},{"location":"CONTRIBUTING/#10-submit-a-pull-request","title":"10. Submit a Pull Request","text":"<p>Submit a pull request (PR) via GitHub. Ensure your PR:</p> <ul> <li>Includes tests for new functionality.</li> <li>Updates documentation (docstrings, <code>README.md</code>, MkDocs).</li> <li>Passes <code>make check</code> and <code>make test</code>.</li> </ul>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<ul> <li>Include tests for new functionality or bug fixes.</li> <li>Update documentation for new features (docstrings, <code>README.md</code>, MkDocs).</li> <li>Ensure <code>make check</code> passes (linting, type checking).</li> <li>Verify <code>make test</code> passes with adequate coverage.</li> <li>Provide a clear PR description, referencing related issues.</li> </ul>"},{"location":"CONTRIBUTING/#additional-notes","title":"Additional Notes","text":"<ul> <li>CI/CD: The pipeline runs <code>tox</code>, <code>make check</code>, and <code>make test</code> on PRs, merges to <code>main</code>, and releases.</li> <li>Coverage: Aim for ~85% line coverage (check with <code>make test</code> or HTML report).</li> <li>Debugging: Use <code>-d/--debug</code> flags in scripts for verbose logs. Check <code>new_warnings.log</code> for <code>compile_shaders.py</code> issues.</li> </ul> <p>Thank you for contributing to <code>hlslkit</code>!</p>"},{"location":"modules/","title":"Modules","text":"<p>options: show_source: true </p> <p>Compile HLSL shaders using fxc.exe and generate a warning/error report.</p> <p>This script compiles HLSL shaders specified in a YAML configuration file using Microsoft's <code>fxc.exe</code> compiler. It supports parallel compilation, system-adaptive job management, and detailed logging of warnings and errors. The output includes compiled shader objects and a <code>new_issues.log</code> file summarizing new warnings and errors.</p> Example <pre><code>python compile_shaders.py --config shader_defines.yaml --shader-dir src --output-dir build\n</code></pre> Dependencies <ul> <li><code>yaml</code>: For parsing configuration files.</li> <li><code>tqdm</code>: For progress bars.</li> <li><code>psutil</code> (optional): For system resource monitoring.</li> <li><code>gooey</code> (optional): For GUI support.</li> </ul> <p>options: show_source: true </p> <p>Generate shader defines YAML from CommunityShaders log.</p> <p>This script parses a <code>CommunityShaders.log</code> file to extract shader configurations, warnings, and errors. It generates a <code>shader_defines.yaml</code> file summarizing shader entry points, preprocessor defines, and compilation issues. The output is structured for use with <code>compile_shaders.py</code>.</p> Example <pre><code>python generate_shader_defines.py --log CommunityShaders.log --output shader_defines.yaml\n</code></pre> Dependencies <ul> <li><code>yaml</code>: For generating YAML output.</li> <li><code>tqdm</code>: For progress bars.</li> <li><code>gooey</code> (optional): For GUI support.</li> </ul> <p>options: show_source: true</p>"},{"location":"modules/#hlslkit.buffer_scan.AnalysisLink","title":"<code>AnalysisLink</code>  <code>dataclass</code>","text":"<p>Represents a link to struct analysis results.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>@dataclass\nclass AnalysisLink:\n    \"\"\"Represents a link to struct analysis results.\"\"\"\n\n    link: str\n    is_match: bool\n    cpp_name: str\n    cpp_file: str\n    cpp_line: int\n    score: float\n    status: str\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.FileScanner","title":"<code>FileScanner</code>","text":"<p>Class to handle file scanning operations.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>class FileScanner:\n    \"\"\"Class to handle file scanning operations.\"\"\"\n\n    def __init__(self, cwd: str):\n        \"\"\"Initialize the scanner with current working directory.\n\n        Args:\n            cwd: Current working directory path.\n        \"\"\"\n        self.cwd = cwd\n        self.excluded_dirs = get_excluded_dirs(cwd)\n\n    def _get_short_path(self, full_path: str) -&gt; str:\n        \"\"\"Get shortened path relative to skyrim-community-shaders or cwd.\"\"\"\n        full_path = full_path.replace(\"\\\\\", \"/\")\n        short_path_start = full_path.lower().find(\"skyrim-community-shaders\")\n        if short_path_start != -1:\n            return full_path[short_path_start + len(\"skyrim-community-shaders\") + 1 :]\n        return os.path.relpath(full_path, self.cwd).replace(\"\\\\\", \"/\")\n\n    def scan_for_buffers(\n        self,\n        pattern: Pattern[str],\n        feature_pattern: Pattern[str],\n        shader_pattern: Pattern[str],\n        hlsl_types: dict[str, str],\n        defines_list: list[dict[str, str]],\n    ) -&gt; tuple[list[dict[str, Any]], dict[tuple[str, frozenset[str]], dict[str, set[str]]]]:\n        \"\"\"Scan HLSL files for buffer definitions and track compilation units.\n\n        Args:\n            pattern: Compiled regex pattern to match HLSL files.\n            feature_pattern: Compiled regex pattern to extract feature names.\n            shader_pattern: Compiled regex pattern for buffer definitions.\n            hlsl_types: Mapping of HLSL register types to descriptions.\n            defines_list: List of preprocessor define dictionaries.\n\n        Returns:\n            A tuple containing:\n            - List of buffer entries\n            - Dictionary of compilation units\n        \"\"\"\n        result_map: dict[str, dict[str, Any]] = {}\n        compilation_units: dict[tuple[str, frozenset[str]], dict[str, set[str]]] = {}\n\n        for root, dirs, files in os.walk(self.cwd):\n            dirs[:] = [d for d in dirs if d not in self.excluded_dirs]\n            feature = \"\"\n            root_normalized = root.replace(\"\\\\\", \"/\")\n            feature_match = feature_pattern.search(root_normalized)\n            if feature_match:\n                feature = feature_match.group(\"feature\")\n\n            for file in files:\n                if file.lower().endswith((\".hlsl\", \".hlsli\")):\n                    full_path = os.path.join(root, file).replace(\"\\\\\", \"/\")\n                    short_path = self._get_short_path(full_path)\n                    logging.debug(f\"Processing file: {full_path}\")\n\n                    for defines in defines_list:\n                        process_file(\n                            full_path,\n                            self.cwd,\n                            defines,\n                            shader_pattern,\n                            hlsl_types,\n                            feature,\n                            short_path,\n                            result_map,\n                            compilation_units,\n                        )\n\n        results = list(result_map.values())\n        logging.debug(f\"Scan found {len(results)} buffers\")\n        return results, compilation_units\n\n    def scan_for_structs(\n        self,\n    ) -&gt; tuple[dict[str, list[StructDict]], dict[str, list[StructDict]]]:\n        \"\"\"Scan directory for HLSL and C++ files and extract structs.\"\"\"\n        hlsl_structs: dict[str, list[StructDict]] = {}\n        cpp_structs: dict[str, list[StructDict]] = {}\n\n        for root, dirs, files in os.walk(self.cwd):\n            dirs[:] = [d for d in dirs if d not in self.excluded_dirs]\n            for f in files:\n                if not f.lower().endswith((\".cpp\", \".h\", \".hpp\", \".hlsl\", \".hlsli\")):\n                    continue\n                full_path = os.path.normpath(os.path.join(root, f))\n                try:\n                    with open(full_path, encoding=\"utf-8\", errors=\"ignore\") as file:\n                        content = file.read()\n                    short_path = self._get_short_path(full_path)\n                    is_hlsl = full_path.lower().endswith((\".hlsl\", \".hlsli\"))\n                    structs = extract_structs(content, is_hlsl, short_path)\n                    target = hlsl_structs if is_hlsl else cpp_structs\n                    for name, data in structs.items():\n                        if is_shader_io_struct(name):\n                            logging.debug(f\"Skipping shader IO buffer: {name} in {short_path}\")\n                            continue\n\n                        if not isinstance(data, dict):\n                            logging.error(f\"Invalid struct data for {name} in {short_path}: {data}\")\n                            continue\n                        data[\"name\"] = name\n\n                        # Check if we already have a real definition and this is a template\n                        if name in target and data.get(\"is_template\", False):\n                            # Check if any existing definition is not a template (has fields)\n                            has_real_definition = any(\n                                existing.get(\"fields\") and not existing.get(\"is_template\", False)\n                                for existing in target[name]\n                            )\n                            if has_real_definition:\n                                logging.debug(f\"Skipping template {name} - real struct definition already exists\")\n                                continue\n\n                        if name not in target:\n                            target[name] = []\n                        target[name].append(data)\n                        logging.debug(f\"Added {name} from {short_path} to {'hlsl' if is_hlsl else 'cpp'} structs\")\n                except Exception as e:\n                    logging.warning(f\"Failed to read or process file {full_path}: {e}\")\n\n        return hlsl_structs, cpp_structs\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.FileScanner.__init__","title":"<code>__init__(cwd)</code>","text":"<p>Initialize the scanner with current working directory.</p> <p>Parameters:</p> Name Type Description Default <code>cwd</code> <code>str</code> <p>Current working directory path.</p> required Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def __init__(self, cwd: str):\n    \"\"\"Initialize the scanner with current working directory.\n\n    Args:\n        cwd: Current working directory path.\n    \"\"\"\n    self.cwd = cwd\n    self.excluded_dirs = get_excluded_dirs(cwd)\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.FileScanner.scan_for_buffers","title":"<code>scan_for_buffers(pattern, feature_pattern, shader_pattern, hlsl_types, defines_list)</code>","text":"<p>Scan HLSL files for buffer definitions and track compilation units.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>Pattern[str]</code> <p>Compiled regex pattern to match HLSL files.</p> required <code>feature_pattern</code> <code>Pattern[str]</code> <p>Compiled regex pattern to extract feature names.</p> required <code>shader_pattern</code> <code>Pattern[str]</code> <p>Compiled regex pattern for buffer definitions.</p> required <code>hlsl_types</code> <code>dict[str, str]</code> <p>Mapping of HLSL register types to descriptions.</p> required <code>defines_list</code> <code>list[dict[str, str]]</code> <p>List of preprocessor define dictionaries.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A tuple containing:</p> <code>dict[tuple[str, frozenset[str]], dict[str, set[str]]]</code> <ul> <li>List of buffer entries</li> </ul> <code>tuple[list[dict[str, Any]], dict[tuple[str, frozenset[str]], dict[str, set[str]]]]</code> <ul> <li>Dictionary of compilation units</li> </ul> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def scan_for_buffers(\n    self,\n    pattern: Pattern[str],\n    feature_pattern: Pattern[str],\n    shader_pattern: Pattern[str],\n    hlsl_types: dict[str, str],\n    defines_list: list[dict[str, str]],\n) -&gt; tuple[list[dict[str, Any]], dict[tuple[str, frozenset[str]], dict[str, set[str]]]]:\n    \"\"\"Scan HLSL files for buffer definitions and track compilation units.\n\n    Args:\n        pattern: Compiled regex pattern to match HLSL files.\n        feature_pattern: Compiled regex pattern to extract feature names.\n        shader_pattern: Compiled regex pattern for buffer definitions.\n        hlsl_types: Mapping of HLSL register types to descriptions.\n        defines_list: List of preprocessor define dictionaries.\n\n    Returns:\n        A tuple containing:\n        - List of buffer entries\n        - Dictionary of compilation units\n    \"\"\"\n    result_map: dict[str, dict[str, Any]] = {}\n    compilation_units: dict[tuple[str, frozenset[str]], dict[str, set[str]]] = {}\n\n    for root, dirs, files in os.walk(self.cwd):\n        dirs[:] = [d for d in dirs if d not in self.excluded_dirs]\n        feature = \"\"\n        root_normalized = root.replace(\"\\\\\", \"/\")\n        feature_match = feature_pattern.search(root_normalized)\n        if feature_match:\n            feature = feature_match.group(\"feature\")\n\n        for file in files:\n            if file.lower().endswith((\".hlsl\", \".hlsli\")):\n                full_path = os.path.join(root, file).replace(\"\\\\\", \"/\")\n                short_path = self._get_short_path(full_path)\n                logging.debug(f\"Processing file: {full_path}\")\n\n                for defines in defines_list:\n                    process_file(\n                        full_path,\n                        self.cwd,\n                        defines,\n                        shader_pattern,\n                        hlsl_types,\n                        feature,\n                        short_path,\n                        result_map,\n                        compilation_units,\n                    )\n\n    results = list(result_map.values())\n    logging.debug(f\"Scan found {len(results)} buffers\")\n    return results, compilation_units\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.FileScanner.scan_for_structs","title":"<code>scan_for_structs()</code>","text":"<p>Scan directory for HLSL and C++ files and extract structs.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def scan_for_structs(\n    self,\n) -&gt; tuple[dict[str, list[StructDict]], dict[str, list[StructDict]]]:\n    \"\"\"Scan directory for HLSL and C++ files and extract structs.\"\"\"\n    hlsl_structs: dict[str, list[StructDict]] = {}\n    cpp_structs: dict[str, list[StructDict]] = {}\n\n    for root, dirs, files in os.walk(self.cwd):\n        dirs[:] = [d for d in dirs if d not in self.excluded_dirs]\n        for f in files:\n            if not f.lower().endswith((\".cpp\", \".h\", \".hpp\", \".hlsl\", \".hlsli\")):\n                continue\n            full_path = os.path.normpath(os.path.join(root, f))\n            try:\n                with open(full_path, encoding=\"utf-8\", errors=\"ignore\") as file:\n                    content = file.read()\n                short_path = self._get_short_path(full_path)\n                is_hlsl = full_path.lower().endswith((\".hlsl\", \".hlsli\"))\n                structs = extract_structs(content, is_hlsl, short_path)\n                target = hlsl_structs if is_hlsl else cpp_structs\n                for name, data in structs.items():\n                    if is_shader_io_struct(name):\n                        logging.debug(f\"Skipping shader IO buffer: {name} in {short_path}\")\n                        continue\n\n                    if not isinstance(data, dict):\n                        logging.error(f\"Invalid struct data for {name} in {short_path}: {data}\")\n                        continue\n                    data[\"name\"] = name\n\n                    # Check if we already have a real definition and this is a template\n                    if name in target and data.get(\"is_template\", False):\n                        # Check if any existing definition is not a template (has fields)\n                        has_real_definition = any(\n                            existing.get(\"fields\") and not existing.get(\"is_template\", False)\n                            for existing in target[name]\n                        )\n                        if has_real_definition:\n                            logging.debug(f\"Skipping template {name} - real struct definition already exists\")\n                            continue\n\n                    if name not in target:\n                        target[name] = []\n                    target[name].append(data)\n                    logging.debug(f\"Added {name} from {short_path} to {'hlsl' if is_hlsl else 'cpp'} structs\")\n            except Exception as e:\n                logging.warning(f\"Failed to read or process file {full_path}: {e}\")\n\n    return hlsl_structs, cpp_structs\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.StructAnalyzer","title":"<code>StructAnalyzer</code>","text":"<p>Class to handle struct comparison and analysis.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>class StructAnalyzer:\n    \"\"\"Class to handle struct comparison and analysis.\"\"\"\n\n    def __init__(\n        self,\n        hlsl_structs: dict[str, list[StructDict]],\n        cpp_structs: dict[str, list[StructDict]],\n    ):\n        self.hlsl_structs: dict[str, list[StructDict]] = hlsl_structs\n        self.cpp_structs: dict[str, list[StructDict]] = cpp_structs\n        self.composite_buffers: dict[str, list[str]] = {}\n        self.comparison_tables: list[str] = []\n        self.buffer_locations: dict[tuple[str, str], tuple[str, int]] = {}\n        self.analysis_results: dict[str, dict[str, Any]] = {}  # Store analysis links\n        hlsl_count = sum(len(struct_list) for struct_list in hlsl_structs.values())\n        cpp_count = sum(len(struct_list) for struct_list in cpp_structs.values())\n        logging.info(f\"Initialized with {hlsl_count} HLSL structs and {cpp_count} C++ structs\")\n\n    def add_buffer_location(self, file: str, buffer_name: str, line: int) -&gt; None:\n        \"\"\"Add a buffer location to the map.\n\n        Args:\n            file: File path\n            buffer_name: Name of the buffer\n            line: Line number\n        \"\"\"\n        key = f\"{file.lower()}:{buffer_name.lower()}\"\n        self.buffer_locations[key] = (file, buffer_name)\n        logging.debug(f\"Added buffer location: {key} -&gt; ({file}, {buffer_name})\")\n\n    def get_buffer_location(self, file: str, buffer_name: str, line: int) -&gt; tuple[str, int] | None:\n        \"\"\"Get the location of a buffer.\n\n        Args:\n            file: File path\n            buffer_name: Name of the buffer\n            line: Line number\n\n        Returns:\n            tuple[str, int] | None: (file, line) tuple if found, None otherwise\n        \"\"\"\n        key = f\"{file.lower()}:{buffer_name.lower()}\"\n        return self.buffer_locations.get(key)\n\n    def _is_composite_buffer(self, struct_data: StructDict) -&gt; bool:\n        if not struct_data.get(\"is_cbuffer\", False):\n            return False\n        fields = struct_data.get(\"fields\", [])\n        if not fields:\n            return False\n        # At least one field is a user-defined struct type\n        return any(field[\"type\"] in self.hlsl_structs and field[\"type\"] not in BASE_TYPE_SIZES for field in fields)\n\n    def _process_composite_buffer(\n        self,\n        buffer_name: str,\n        buffer_data: StructDict,\n        matches: list,\n        matched_cpp_structs: set[str],\n    ) -&gt; None:\n        fields = buffer_data.get(\"fields\", [])\n        if not fields:\n            return\n\n        self.composite_buffers[buffer_name] = [field[\"type\"] for field in fields]\n        logging.debug(\n            f\"Processing composite buffer {buffer_name} with contained structs: {self.composite_buffers[buffer_name]}\"\n        )\n\n        # --- Process sub-buffers first ---\n        for field in fields:\n            struct_type = field[\"type\"]\n            if struct_type in BASE_TYPE_SIZES:\n                logging.warning(\n                    f\"Field {field['name']} with built-in type {struct_type} in composite buffer {buffer_name} \"\n                    f\"at {buffer_data['file']}:{buffer_data['line']} will be skipped\"\n                )\n                continue\n            if struct_type in self.hlsl_structs:\n                for struct_data in self.hlsl_structs[struct_type]:\n                    if not isinstance(struct_data, dict):\n                        logging.error(f\"Invalid struct_data for {struct_type}: {struct_data}\")\n                        continue\n                    struct_fields = self.get_nested_fields(struct_data)\n                    candidates = self.find_struct_candidates(\n                        struct_type, struct_data, struct_fields, matched_cpp_structs\n                    )\n                    # Use shared helper to find best match without score boosts for sub-structs\n                    best_match, sorted_candidates = self._find_best_match_from_candidates(\n                        struct_type, struct_data, struct_fields, candidates, apply_score_boosts=False\n                    )\n\n                    if best_match and self._is_match_good_enough(\n                        best_match.score, best_match.report, sorted_candidates, struct_type, best_match.cpp_name\n                    ):\n                        unique_id = f\"{best_match.cpp_file}:{best_match.cpp_name}\"\n                        matched_cpp_structs.add(unique_id)\n                        matches.append(best_match)\n                        logging.info(\n                            f\"Best match for sub-struct {struct_type}: {best_match.cpp_name} (score={best_match.score:.3f}) from {best_match.cpp_file}:{best_match.cpp_line}\"\n                        )\n                    else:\n                        logging.info(\n                            f\"No suitable C++ struct found for '{struct_type}' \"\n                            f\"in {struct_data.get('file', 'unknown')}:{struct_data.get('line', 0)}\"\n                        )\n            else:\n                logging.warning(\n                    f\"Field type {struct_type} in composite buffer {buffer_name} \"\n                    f\"at {buffer_data['file']}:{buffer_data['line']} not found in hlsl_structs\"\n                )\n\n        # --- Now update fields and size ---\n        hlsl_fields = self.get_nested_fields(buffer_data)\n        buffer_data[\"fields\"] = hlsl_fields\n\n        buffer_data[\"size\"] = calculate_struct_size(hlsl_fields, align_to_16=True)\n        logging.debug(\n            f\"Updated composite buffer {buffer_name} size to {buffer_data['size']} bytes (fields: {[(f['name'], f['type'], f['size']) for f in hlsl_fields]})\"\n        )\n\n        # --- Now match the composite buffer itself ---\n        candidates = self.find_struct_candidates(buffer_name, buffer_data, hlsl_fields, matched_cpp_structs)\n\n        # Use shared helper to find best match without score boosts for composite buffer\n        best_match, sorted_candidates = self._find_best_match_from_candidates(\n            buffer_name, buffer_data, hlsl_fields, candidates, apply_score_boosts=False\n        )\n\n        if best_match and self._is_match_good_enough(\n            best_match.score, best_match.report, sorted_candidates, buffer_name, best_match.cpp_name\n        ):\n            unique_id = f\"{best_match.cpp_file}:{best_match.cpp_name}\"\n            matched_cpp_structs.add(unique_id)\n            matches.append(best_match)\n        else:\n            logging.info(\n                f\"No suitable C++ struct found for composite buffer '{buffer_name}' \"\n                f\"in {buffer_data['file']}:{buffer_data['line']}\"\n            )\n\n    def _generate_comparison_tables(self, matches: list[StructMatch], print_tables: bool = True) -&gt; None:\n        \"\"\"Generate comparison tables for all matches.\n\n        Args:\n            matches: List of StructMatch objects.\n            print_tables: Whether to print the tables immediately.\n        \"\"\"\n        self.comparison_tables = []\n        if not matches and print_tables:\n            print(\"\\nNo matching structs found between HLSL and C++.\")\n            return\n\n        composite_matches: dict[str, list[StructMatch]] = {}\n        regular_matches: list[StructMatch] = []\n\n        logging.debug(f\"Generating {len(matches)} comparison tables\")\n\n        for match in matches:\n            hlsl_name = match.hlsl_name\n            found_in_composite = False\n            for composite_name, contained_structs in self.composite_buffers.items():\n                if hlsl_name in contained_structs or hlsl_name == composite_name:\n                    if composite_name not in composite_matches:\n                        composite_matches[composite_name] = []\n                    composite_matches[composite_name].append(match)\n                    found_in_composite = True\n                    break\n            if not found_in_composite:\n                regular_matches.append(match)\n\n        for match in regular_matches:\n            self._generate_single_comparison_table(match, print_tables)\n\n        for composite_name, comp_matches in composite_matches.items():\n            if print_tables:\n                print(f\"\\n## Composite Buffer: {composite_name}\\n\")\n                # Find the match for the composite buffer itself\n                comp_match = next((m for m in comp_matches if m.hlsl_name == composite_name), None)\n                if comp_match:\n                    self._generate_single_comparison_table(comp_match, print_tables)\n                else:\n                    # Select the first StructDict for the composite buffer\n                    comp_data = self.hlsl_structs.get(composite_name, [{}])[0]\n                    if not comp_data:\n                        logging.warning(f\"No data found for composite buffer {composite_name}\")\n                        continue\n                    print(\n                        f\"**HLSL File:** [{comp_data.get('file', '')}:{comp_data.get('line', '')}]({create_link(comp_data.get('file', ''), comp_data.get('line', ''))})\\n\"\n                    )\n                    print(\"**Best Match:** None\\n\")\n            for match in comp_matches:\n                if match.hlsl_name != composite_name:\n                    self._generate_single_comparison_table(match, print_tables)\n\n    def _is_match_good_enough(\n        self,\n        score: float,\n        report: dict,\n        candidates: list,\n        hlsl_name: str = \"\",\n        cpp_name: str = \"\",\n    ) -&gt; bool:\n        \"\"\"Improved match quality assessment with more nuanced criteria.\"\"\"\n        logging.debug(f\"Checking match for {hlsl_name} vs {cpp_name}:\")\n\n        # 1. Absolute minimum score threshold (lowered)\n        if score &lt; 0.5:  # Reduced from 0.6\n            logging.debug(f\"Rejected match for {hlsl_name} vs {cpp_name}: score {score:.3f} below threshold 0.5\")\n            return False\n\n        # 2. Enhanced field count analysis\n        total_fields = report.get(\"total_fields\", 1)\n        cpp_total_fields = report.get(\"cpp_total_fields\", 1)\n        field_count_ratio = min(total_fields, cpp_total_fields) / max(total_fields, cpp_total_fields)\n\n        # More lenient for small structs\n        min_ratio = 0.4 if max(total_fields, cpp_total_fields) &lt;= 3 else 0.6\n        if field_count_ratio &lt; min_ratio:\n            logging.debug(\n                f\"Rejected match for {hlsl_name} vs {cpp_name}: field count ratio {field_count_ratio:.2f} &lt; {min_ratio}\"\n            )\n            return False\n\n        # 3. Improved exact match requirements\n        exact_matches = report.get(\"exact_matches\", 0)\n        high_sim_matches = report.get(\"high_sim_matches\", 0)\n        total_good_matches = exact_matches + high_sim_matches\n\n        min_fields = min(total_fields, cpp_total_fields)\n\n        # Require at least 30% good matches for small structs, 50% for larger ones\n        required_match_ratio = 0.3 if min_fields &lt;= 3 else 0.5\n        good_match_ratio = total_good_matches / min_fields if min_fields &gt; 0 else 0\n\n        if good_match_ratio &lt; required_match_ratio:\n            logging.debug(\n                f\"Rejected match for {hlsl_name} vs {cpp_name}: good match ratio {good_match_ratio:.2f} &lt; {required_match_ratio}\"\n            )\n            return False\n\n        # 4. Name similarity boost for very similar names\n        name_sim = compute_name_similarity(hlsl_name, cpp_name)\n        if name_sim &gt; 0.8:\n            # Very similar names get more lenient treatment\n            return score &gt; 0.4 and good_match_ratio &gt; 0.2\n\n        # 5. Size difference check (more lenient)\n        size_difference = report.get(\"size_difference\", 0)\n        max_size_diff = 128 if max(total_fields, cpp_total_fields) &gt; 5 else 64\n        if size_difference &gt; max_size_diff:\n            logging.debug(\n                f\"Rejected match for {hlsl_name} vs {cpp_name}: size difference {size_difference} &gt; {max_size_diff} bytes\"\n            )\n            return False\n\n        # 6. Relative candidate quality (more lenient)\n        if len(candidates) &gt; 1:\n            best_score = score\n            # Handle both StructCandidate objects and old tuple format\n            if hasattr(candidates[1], \"score\"):\n                second_best_score = candidates[1].score\n            else:\n                second_best_score = candidates[1][2] if len(candidates[1]) &gt; 2 else 0\n            score_gap = best_score - second_best_score\n\n            # Only reject if the best candidate isn't significantly better AND the score is low\n            if score_gap &lt; 0.05 and best_score &lt; 0.6:\n                logging.debug(f\"Rejected match for {hlsl_name} vs {cpp_name}: insufficient score gap {score_gap:.3f}\")\n                return False\n\n        return True\n\n    def compare_all_structs(self, result_map: ResultMap) -&gt; dict[str, dict[str, str | bool]]:\n        \"\"\"Compare HLSL and C++ structs, generating alignment reports.\"\"\"\n        analysis_links: dict[str, dict[str, str | bool]] = {}\n        matches: list[StructMatch] = []\n        matched_cpp_structs = set()\n\n        match_counts = {\n            \"Matched\": 0,  # Perfect matches\n            \"Mismatched\": 0,  # Partial matches\n            \"Unmatched\": 0,  # No matches\n        }\n\n        if not self.hlsl_structs:\n            print(\"\\nNo HLSL structs found.\")\n            self.analysis_links = analysis_links\n            return analysis_links\n\n        for hlsl_name, hlsl_struct_list in self.hlsl_structs.items():\n            for hlsl_data in hlsl_struct_list:\n                if not isinstance(hlsl_data, dict):\n                    logging.error(f\"Invalid hlsl_data for {hlsl_name}: {hlsl_data}\")\n                    continue\n\n                logging.debug(\n                    f\"Processing HLSL struct: {hlsl_name} : size={hlsl_data.get('size')} : {hlsl_data.get('file', '')}:{hlsl_data.get('line', '')}\"\n                )\n\n                if self._is_composite_buffer(hlsl_data):\n                    self._process_composite_buffer(hlsl_name, hlsl_data, matches, matched_cpp_structs)\n                    continue\n\n                if hlsl_data.get(\"is_template\") and \"template_type\" in hlsl_data:\n                    template_type = hlsl_data[\"template_type\"]\n                    if template_type in self.hlsl_structs:\n                        hlsl_data[\"fields\"] = self.hlsl_structs[template_type][0][\"fields\"]\n                hlsl_fields = self.get_nested_fields(hlsl_data)\n\n                if not hlsl_fields:\n                    continue\n\n                # Find all potential candidates (including exact name matches)\n                candidates = self.find_struct_candidates(hlsl_name, hlsl_data, hlsl_fields, matched_cpp_structs)\n\n                # Use shared helper to find best match with score boosts\n                best_match, sorted_candidates = self._find_best_match_from_candidates(\n                    hlsl_name, hlsl_data, hlsl_fields, candidates, apply_score_boosts=True\n                )\n\n                # ONLY AFTER finding the best match, check if it's good enough\n                if best_match:\n                    # Check if the match is good enough\n                    if self._is_match_good_enough(\n                        best_match.score, best_match.report, sorted_candidates, hlsl_name, best_match.cpp_name\n                    ):\n                        matches.append(best_match)\n                        logging.debug(\n                            f\"Accepted match for {hlsl_name}: {best_match.cpp_name} (score={best_match.score:.3f})\"\n                        )\n                    else:\n                        # For rejected matches, create a match with empty cpp info but preserve candidate data\n                        matches.append(\n                            StructMatch(\n                                hlsl_name=hlsl_name,\n                                hlsl_file=hlsl_data[\"file\"],\n                                hlsl_line=hlsl_data[\"line\"],\n                                cpp_name=\"\",  # Empty cpp_name indicates rejection\n                                cpp_file=\"\",  # Empty cpp_file\n                                cpp_line=0,  # Empty cpp_line\n                                score=0.0,  # Zero score for status determination\n                                align_matches=[],  # Empty align_matches for rejected\n                                report=best_match.report,  # Preserve the actual report from best match\n                                candidates=sorted_candidates,  # Preserve sorted candidates with full alignment info\n                            )\n                        )\n                        logging.debug(\n                            f\"Rejected match for {hlsl_name}: {best_match.cpp_name} (score={best_match.score:.3f}) - quality too low\"\n                        )\n                else:\n                    # No candidates found\n                    matches.append(\n                        StructMatch(\n                            hlsl_name=hlsl_name,\n                            hlsl_file=hlsl_data[\"file\"],\n                            hlsl_line=hlsl_data[\"line\"],\n                            cpp_name=\"\",  # Empty cpp_name\n                            cpp_file=\"\",  # Empty cpp_file\n                            cpp_line=0,  # Empty cpp_line\n                            score=0.0,  # Zero score\n                            align_matches=[],  # Empty align_matches\n                            report={  # Empty report\n                                \"score\": 0.0,\n                                \"exact_matches\": 0,\n                                \"high_sim_matches\": 0,\n                                \"total_fields\": len(hlsl_fields),\n                                \"missing_fields\": len(hlsl_fields),\n                            },\n                            candidates=sorted_candidates,  # Empty sorted_candidates\n                        )\n                    )\n\n        self.matches = matches\n\n        self._generate_comparison_tables(matches, print_tables=False)\n        # Store analysis_links as an attribute for later use in print_comparison_tables\n        self.analysis_links = analysis_links\n\n        # Update analysis links in result_map\n        for match in matches:\n            key = f\"{match.hlsl_file.lower()}:{match.hlsl_name.lower()}\"\n\n            # New logic for status:\n            if not match.cpp_name:\n                status = \"Unmatched\"\n                match_counts[\"Unmatched\"] += 1\n            else:\n                field_diff_count = match.report.get(\"field_diff_count\", 0)\n                cpp_total_fields = match.report.get(\"cpp_total_fields\", 0)\n                hlsl_total_fields = match.report.get(\"total_fields\", 0)\n                min_fields = min(hlsl_total_fields, cpp_total_fields)\n                diff_ratio = field_diff_count / max(1, min_fields)\n                debug_msg = f\"DEBUG: {match.hlsl_name} vs {match.cpp_name}: field_diff_count={field_diff_count}, min_fields={min_fields}, diff_ratio={diff_ratio}, score={match.score}\"\n                add_debug_info(debug_msg)\n                if diff_ratio &gt; 0.5 or match.score &lt; 0.75:\n                    status = \"Unmatched\"\n                    match_counts[\"Unmatched\"] += 1\n                elif field_diff_count == 0:\n                    status = \"Matched\"\n                    match_counts[\"Matched\"] += 1\n                elif field_diff_count &lt;= 1:\n                    status = f\"Mismatched ({match.cpp_name})\"\n                    match_counts[\"Mismatched\"] += 1\n                else:\n                    status = f\"Mismatched ({match.cpp_name})\"\n                    match_counts[\"Mismatched\"] += 1\n\n            analysis_links[key] = {\n                \"link\": create_struct_analysis_link(match.hlsl_name, match.hlsl_file, status),\n                \"is_match\": bool(match.cpp_name),\n                \"cpp_name\": match.cpp_name,\n                \"cpp_file\": match.cpp_file,\n                \"cpp_line\": match.cpp_line,\n                \"score\": match.score,\n                \"status\": status,\n            }\n            # Look for entries using template type for struct analysis\n            for result_key, entry in result_map.items():\n                file_path = entry.get(\"File Path\", \"\").lower()\n                buffer_name = entry.get(\"Name\", \"\").lower()\n                template_type = entry.get(\"Template Type\", \"\")\n\n                # Use template type for struct analysis if available, otherwise use buffer name\n                struct_name = template_type.lower() if template_type else buffer_name\n\n                # Check if this buffer entry should link to this struct analysis\n                if file_path == match.hlsl_file.lower() and struct_name == match.hlsl_name.lower():\n                    entry[\"Matching Struct Analysis\"] = analysis_links[key][\"link\"]\n                    logging.debug(\n                        f\"Updated result_map for {result_key} using template type {template_type} with link: {analysis_links[key]['link']}\"\n                    )\n                    break\n            else:\n                # Fallback: try name-only match for user-defined types\n                for result_key, entry in result_map.items():\n                    buffer_name = entry.get(\"Name\", \"\").lower()\n                    template_type = entry.get(\"Template Type\", \"\")\n                    struct_name = template_type.lower() if template_type else buffer_name\n\n                    if struct_name == match.hlsl_name.lower() and match.hlsl_name.lower() not in BASE_TYPE_SIZES:\n                        entry[\"Matching Struct Analysis\"] = analysis_links[key][\"link\"]\n                        logging.debug(f\"Fallback name-only match for {match.hlsl_name} to result_map key {result_key}\")\n                        break\n                else:\n                    logging.warning(f\"No buffer table entry found for struct {key}\")\n\n        for entry in result_map.values():\n            if \"Matching Struct Analysis\" not in entry:\n                entry[\"Matching Struct Analysis\"] = \"Unmatched\"\n                name = entry.get(\"Name\", \"unknown\")\n                file_path = entry.get(\"File Path\", \"unknown\")\n                logging.debug(f\"Buffer {name} in {file_path} not matched to any struct\")\n\n        return analysis_links\n\n    def _generate_single_comparison_table(self, match: StructMatch, print_tables: bool) -&gt; None:\n        \"\"\"Generate a single comparison table.\n\n        Args:\n            match: StructMatch containing match data.\n            print_tables: Whether to print the table immediately.\n        \"\"\"\n        # Use hlsl_data from match if available, else find it\n        hlsl_data = next(\n            (\n                m\n                for m in self.hlsl_structs.get(match.hlsl_name, [])\n                if m[\"file\"] == match.hlsl_file and m[\"line\"] == match.hlsl_line\n            ),\n            {},\n        )\n        if not hlsl_data:\n            logging.warning(f\"No HLSL data found for {match.hlsl_name} in {match.hlsl_file}:{match.hlsl_line}\")\n\n        # Only use cpp_data if there is a valid match (score &gt; 0 and cpp_name is not empty)\n        if match.cpp_name and match.score &gt; 0:\n            cpp_data = next(\n                (\n                    s\n                    for s in self.cpp_structs.get(match.cpp_name, [])\n                    if s[\"file\"] == match.cpp_file and s[\"line\"] == match.cpp_line\n                ),\n                {},\n            )\n        else:\n            cpp_data = {}\n\n        # Validate types\n        if not isinstance(hlsl_data, dict):\n            raise InvalidStructDictType(hlsl_data)\n        if not isinstance(cpp_data, dict):\n            raise InvalidStructDictType(cpp_data)\n\n        logging.debug(f\"Generating table for HLSL {match.hlsl_name} {match.hlsl_file}:{match.hlsl_line} \")\n\n        # Convert StructCandidate objects to the format expected by generate_comparison_table\n        candidates = [(c.name, c.data, c.score) for c in match.candidates]\n\n        table_data = {\n            \"hlsl_name\": match.hlsl_name,\n            \"cpp_name\": match.cpp_name if cpp_data else \"\",\n            \"hlsl_data\": hlsl_data,\n            \"cpp_data\": cpp_data,\n            \"align_matches\": match.align_matches if cpp_data else [],\n            \"report\": match.report,\n            \"candidates\": candidates,\n        }\n        self.comparison_tables.append(table_data)\n\n        if print_tables:\n            print(\n                generate_comparison_table(\n                    match.hlsl_name,\n                    match.cpp_name if cpp_data else \"\",\n                    hlsl_data,\n                    cpp_data,\n                    match.align_matches if cpp_data else [],\n                    match.report,\n                    candidates,\n                    status=self.analysis_links.get(f\"{match.hlsl_file.lower()}:{match.hlsl_name.lower()}\", {}).get(\n                        \"status\", \"\"\n                    ),\n                    section_id=create_struct_section_id(match.hlsl_name, match.hlsl_file),\n                )\n            )\n\n    def print_comparison_tables(self, only_matched: bool = False, show_top_candidate: bool = False) -&gt; None:\n        \"\"\"Print comparison tables for all HLSL structs, with sub-buffers nested under their parents, using result_map as the source.\"\"\"\n        print(\"\\n# Struct Comparison Results\")\n        printed_keys = set()\n        # Build a lookup for table data\n        table_lookup = {f\"{t['hlsl_name']}:{t['hlsl_data']['file']}\": t for t in self.comparison_tables}\n        logging.debug(f\"Table lookup keys: {list(table_lookup.keys())}\")\n        # Build a lookup for composite buffer relationships\n        composite_to_subs: dict[str, list[str]] = {}\n        for buffer_name, sub_names in self.composite_buffers.items():\n            for sub_name in sub_names:\n                # Find the file for the sub-buffer\n                for t in self.comparison_tables:\n                    if t[\"hlsl_name\"] == sub_name:\n                        sub_key = f\"{sub_name}:{t['hlsl_data']['file']}\"\n                        parent_key = f\"{buffer_name}:{t['hlsl_data']['file']}\"\n                        composite_to_subs.setdefault(parent_key, []).append(\n                            sub_key\n                        )  # Use buffer_locations as the source\n        logging.debug(f\"Buffer locations: {list(self.buffer_locations.items())}\")\n        for _key, entry in self.buffer_locations.items():\n            file, buffer_name = entry\n            composite_key = f\"{buffer_name}:{file}\"\n            # If this is a composite buffer, check if any sub-buffers should be printed\n            if composite_key in composite_to_subs:\n                sub_keys = composite_to_subs[composite_key]\n                # Check if composite or any sub-buffer should be printed\n                keys_to_check = [composite_key, *sub_keys]\n\n                def should_print(k):\n                    table = table_lookup.get(k)\n                    if not table:\n                        return False\n                    status = self.analysis_links.get(\n                        f\"{table['hlsl_data']['file'].lower()}:{table['hlsl_name'].lower()}\", {}\n                    ).get(\"status\", \"\")\n                    if only_matched:\n                        return status != \"Unmatched\"\n                    return True\n\n                should_print_any = any(should_print(k) for k in keys_to_check)\n                if should_print_any and composite_key not in printed_keys:\n                    print(f\"\\n## Composite Buffer: {buffer_name}\\n\")\n                    # Print composite buffer table if present\n                    table = table_lookup.get(composite_key)\n                    if table:\n                        # Handle show_top_candidate for composite buffer\n                        cpp_name = table[\"cpp_name\"]\n                        cpp_data = table[\"cpp_data\"]\n                        align_matches = table[\"align_matches\"]\n                        report = table[\"report\"]\n\n                        # If no accepted match but we want to show top candidate, extract from candidates\n                        if show_top_candidate and not cpp_name and table[\"candidates\"]:\n                            top_candidate = table[\"candidates\"][0]  # Best candidate\n                            cpp_name = f\"{top_candidate[0]} (top candidate - rejected)\"\n                            cpp_data = top_candidate[1]\n                            # For regular candidate tuples, use empty alignment data\n                            align_matches = []\n                            report = table[\"report\"]\n                            print(\n                                generate_comparison_table(\n                                    table[\"hlsl_name\"],\n                                    cpp_name,\n                                    table[\"hlsl_data\"],\n                                    cpp_data,\n                                    align_matches,\n                                    report,\n                                    table[\"candidates\"],\n                                    status=self.analysis_links.get(\n                                        f\"{table['hlsl_data']['file'].lower()}:{table['hlsl_name'].lower()}\",\n                                        {},\n                                    ).get(\"status\", \"\"),\n                                    show_top_candidate=show_top_candidate,\n                                    section_id=create_struct_section_id(table[\"hlsl_name\"], table[\"hlsl_data\"][\"file\"]),\n                                )\n                            )\n                        printed_keys.add(composite_key)\n                    # Print sub-buffers\n                    for sub_key in sub_keys:\n                        if sub_key in printed_keys:\n                            continue\n                        table = table_lookup.get(sub_key)\n                        if table and should_print(sub_key):\n                            # Handle show_top_candidate for sub-buffers\n                            cpp_name = table[\"cpp_name\"]\n                            cpp_data = table[\"cpp_data\"]\n                            align_matches = table[\"align_matches\"]\n                            report = table[\"report\"]\n\n                            # If no accepted match but we want to show top candidate, use the first candidate\n                            if show_top_candidate and not cpp_name and table[\"candidates\"]:\n                                top_candidate = table[\"candidates\"][0]  # First candidate should be best after sorting\n                                cpp_name = f\"{top_candidate[0]} (top candidate - rejected)\"\n                                cpp_data = top_candidate[1]\n                                # Use the alignment data from the candidate\n                                align_matches = top_candidate[3] if len(top_candidate) &gt; 3 else []\n                                report = top_candidate[4] if len(top_candidate) &gt; 4 else table[\"report\"]\n\n                            print(\n                                generate_comparison_table(\n                                    table[\"hlsl_name\"],\n                                    cpp_name,\n                                    table[\"hlsl_data\"],\n                                    cpp_data,\n                                    align_matches,\n                                    report,\n                                    table[\"candidates\"],\n                                    status=self.analysis_links.get(\n                                        f\"{table['hlsl_data']['file'].lower()}:{table['hlsl_name'].lower()}\",\n                                        {},\n                                    ).get(\"status\", \"\"),\n                                    depth=3,\n                                )\n                            )\n                            printed_keys.add(sub_key)\n            # Otherwise, print as a regular buffer if not already printed\n            else:\n                key_lookup = f\"{buffer_name}:{file}\"\n                logging.debug(f\"Looking for key_lookup: {key_lookup}\")\n                if key_lookup in printed_keys:\n                    logging.debug(f\"Already printed: {key_lookup}\")\n                    continue\n                table = table_lookup.get(key_lookup)\n                if table:\n                    logging.debug(f\"Found table for: {key_lookup}\")\n                    status = self.analysis_links.get(\n                        f\"{table['hlsl_data']['file'].lower()}:{table['hlsl_name'].lower()}\", {}\n                    ).get(\"status\", \"\")\n                    logging.debug(f\"Status for {key_lookup}: {status}\")\n                    if only_matched and status == \"Unmatched\":\n                        logging.debug(f\"Skipping unmatched: {key_lookup}\")\n                        continue\n                    # Handle show_top_candidate for regular buffers\n                    cpp_name = table[\"cpp_name\"]\n                    cpp_data = table[\"cpp_data\"]\n\n                    # If no accepted match but we want to show top candidate, use the first candidate\n                    if show_top_candidate and not cpp_name and table[\"candidates\"]:\n                        top_candidate = table[\"candidates\"][0]  # First candidate should be best after sorting\n                        cpp_name = f\"{top_candidate[0]} (top candidate - rejected)\"\n                        cpp_data = top_candidate[1]\n                        # Use empty alignment data for simple candidate tuples\n                        align_matches = []\n                        report = table[\"report\"]\n                    else:\n                        align_matches = table[\"align_matches\"]\n                        report = table[\"report\"]\n\n                    print(\n                        generate_comparison_table(\n                            table[\"hlsl_name\"],\n                            cpp_name,\n                            table[\"hlsl_data\"],\n                            cpp_data,\n                            align_matches,\n                            report,\n                            table[\"candidates\"],\n                            status=status,\n                        )\n                    )\n                    printed_keys.add(key_lookup)\n                else:\n                    logging.debug(f\"No table found for: {key_lookup}\")\n\n    def update_result_map(self, result_map: dict[str, dict[str, Any]]) -&gt; None:\n        \"\"\"Update the result map with stored analysis results.\n\n        Args:\n            result_map: Dictionary mapping keys to buffer metadata, to be updated with analysis links.\n        \"\"\"\n        if not self.analysis_results:\n            logging.debug(\"No analysis results available to update result_map\")\n            return\n\n        # Create template type mapping\n        template_types: dict[str, str] = {}\n        for hlsl_name, struct_list in self.hlsl_structs.items():\n            for struct in struct_list:\n                if struct.get(\"is_template\") and \"template_type\" in struct:\n                    template_types[hlsl_name] = struct[\"template_type\"]\n\n        for key, analysis in self.analysis_results.items():\n            if key in result_map:\n                result_map[key][\"Matching Struct Analysis\"] = analysis[\"link\"]\n                logging.debug(f\"Updated result_map for {key} with link: {analysis['link']}\")\n                continue\n\n            try:\n                file, buffer_name = key.split(\":\")\n            except ValueError:\n                logging.warning(f\"Invalid key format in analysis_results: {key}\")\n                continue\n\n            for result_key, entry in result_map.items():\n                if entry.get(\"File Path\", \"\").lower() == file and entry.get(\"Name\", \"\").lower() == buffer_name:\n                    result_map[result_key][\"Matching Struct Analysis\"] = analysis[\"link\"]\n                    logging.debug(f\"Matched {key} to result_map key {result_key}\")\n                    break\n                # Fallback: try file+name match (legacy)\n                elif entry.get(\"File Path\", \"\").lower() == file and entry.get(\"Name\", \"\").lower() == buffer_name:\n                    result_map[result_key][\"Matching Struct Analysis\"] = analysis[\"link\"]\n                    logging.debug(f\"Fallback file+name match for {key} to result_map key {result_key}\")\n                    break\n                # Fallback: try name-only match for user-defined types\n                elif entry.get(\"Name\", \"\").lower() == buffer_name and buffer_name not in BASE_TYPE_SIZES:\n                    result_map[result_key][\"Matching Struct Analysis\"] = analysis[\"link\"]\n                    logging.debug(f\"Fallback name-only match for {key} to result_map key {result_key}\")\n                    break\n                else:\n                    logging.warning(f\"No buffer table entry found for struct {key}\")\n        for entry in result_map.values():\n            if \"Matching Struct Analysis\" not in entry:\n                entry[\"Matching Struct Analysis\"] = \"Unmatched\"\n                name = entry.get(\"Name\", \"unknown\")\n                file_path = entry.get(\"File Path\", \"unknown\")\n                logging.debug(f\"Buffer {name} in {file_path} not matched to any struct\")\n\n    def get_nested_fields(self, struct_data: StructDict) -&gt; list[FieldDict]:\n        \"\"\"Get all fields from a struct, including nested struct fields.\n\n        Args:\n            struct_data: Dictionary containing struct metadata.\n\n        Returns:\n            list[FieldDict]: List of field dictionaries, including nested fields.\n        \"\"\"\n        fields = struct_data.get(\"fields\", [])\n        if not fields:\n            return []\n\n        processed_fields: list[FieldDict] = []\n\n        # Determine which struct dictionary to use based on the struct's origin\n        struct_file = struct_data.get(\"file\", \"\")\n        is_cpp_struct = struct_file.endswith((\".cpp\", \".h\", \".hpp\"))\n        struct_dict = self.cpp_structs if is_cpp_struct else self.hlsl_structs\n\n        for field in fields:\n            field_type = field.get(\"type\", \"\")\n            if field_type in struct_dict:\n                # Process each StructDict in the list for nested structs\n                nested_structs = struct_dict[field_type]\n                struct_file = struct_data.get(\"file\", \"\")\n                preferred_struct = None\n                for nested_struct in nested_structs:\n                    if not isinstance(nested_struct, dict):\n                        logging.error(f\"Invalid nested struct for {field_type}: {nested_struct}\")\n                        continue\n                    if nested_struct.get(\"file\", \"\") == struct_file:\n                        preferred_struct = nested_struct\n                        break\n                if not preferred_struct:\n                    preferred_struct = nested_structs[0]  # fallback to first if no file match\n                nested_fields = self.get_nested_fields(preferred_struct)\n                processed_fields.extend(nested_fields)\n            else:\n                processed_fields.append(field)\n\n        return processed_fields\n\n    def get_field_name(self, field: FieldDict) -&gt; str:\n        \"\"\"Get the field name without array notation.\n\n        Args:\n            field: Field dictionary containing name information.\n\n        Returns:\n            str: Field name without array notation.\n\n        Example:\n            &gt;&gt;&gt; analyzer.get_field_name({\"name\": \"myArray[3]\"})\n            'myArray'\n        \"\"\"\n        if not isinstance(field, dict) or \"name\" not in field:\n            logging.warning(f\"Invalid field dictionary: {field}\")\n            return \"\"\n        return strip_array_notation(field[\"name\"])\n\n    def find_struct_candidates(\n        self,\n        hlsl_name: str,\n        hlsl_data: StructDict,\n        hlsl_fields: list[FieldDict],\n        matched_cpp_structs: set[str],\n    ) -&gt; list[tuple[str, StructDict, float]]:\n        \"\"\"Find candidate C++ structs for an HLSL struct.\n\n        Args:\n            hlsl_name: Name of the HLSL struct.\n            hlsl_data: HLSL struct metadata.\n            hlsl_fields: List of HLSL fields.\n            matched_cpp_structs: Set of already matched C++ struct names.\n\n        Returns:\n            list[tuple[str, StructDict, float]]: List of (cpp_name, cpp_data, similarity) tuples.\n        \"\"\"\n        candidates: list[tuple[str, StructDict, float]] = []\n\n        for cpp_name, cpp_struct_list in self.cpp_structs.items():\n            for cpp_data in cpp_struct_list:\n                # Test original version\n                original_fields = cpp_data.get(\"fields\", [])\n                if original_fields:\n                    temp_cpp_data = dict(cpp_data)\n                    temp_cpp_data[\"fields\"] = original_fields\n\n                    result = align_structs(temp_cpp_data, hlsl_data, 0.5)\n                    if result is not None:\n                        alignment_score, _, _ = result\n                        candidates.append((cpp_name, temp_cpp_data, alignment_score))\n\n                        logging.debug(\n                            f\"\\t\\t Candidate {cpp_name} (original) from {cpp_data.get('file', 'unknown')}: \"\n                            f\"alignment_score={alignment_score:.2f}, total_size={calculate_struct_size(original_fields)}, fields: {[(f['name'], f['type'], f['size']) for f in original_fields]}\"\n                        )\n\n                # Test flattened version if different\n                flattened_fields = self.get_nested_fields(cpp_data)\n                if flattened_fields != original_fields:\n                    temp_cpp_data = dict(cpp_data)\n                    temp_cpp_data[\"fields\"] = flattened_fields\n                    temp_cpp_data[\"size\"] = calculate_struct_size(flattened_fields)\n\n                    result = align_structs(temp_cpp_data, hlsl_data, 0.5)\n                    if result is not None:\n                        alignment_score, _, _ = result\n                        candidates.append((f\"{cpp_name}\", temp_cpp_data, alignment_score))\n\n                        logging.debug(\n                            f\"\\t\\t Candidate {cpp_name} (flattened) from {cpp_data.get('file', 'unknown')}: \"\n                            f\"alignment_score={alignment_score:.2f}, total_size={calculate_struct_size(flattened_fields)}, fields: {[(f['name'], f['type'], f['size']) for f in flattened_fields]}\"\n                        )\n        candidates.sort(key=lambda x: x[2], reverse=True)\n        return candidates\n\n    def _find_best_match_from_candidates(\n        self,\n        hlsl_name: str,\n        hlsl_data: StructDict,\n        hlsl_fields: list[FieldDict],\n        candidates: list[tuple[str, StructDict, float]],\n        apply_score_boosts: bool = True,\n    ) -&gt; tuple[StructMatch | None, list[StructCandidate]]:\n        \"\"\"Find the best match among candidates and return StructMatch with candidate list.\n\n        Args:\n            hlsl_name: Name of the HLSL struct\n            hlsl_data: HLSL struct metadata\n            hlsl_fields: List of HLSL fields\n            candidates: List of (cpp_name, cpp_data, similarity) tuples\n            apply_score_boosts: Whether to apply score boosts for exact name matches and outliers\n\n        Returns:\n            tuple: (StructMatch_or_None, list_of_StructCandidates)\n        \"\"\"\n        if not candidates:\n            return None, []\n\n        evaluated_candidates = []\n        best_score = -1\n        best_match = None\n\n        for idx, (cpp_name, cpp_data, similarity) in enumerate(candidates):\n            if not isinstance(cpp_data, dict):\n                logging.error(f\"Invalid cpp_data for {cpp_name}: {cpp_data}\")\n                continue\n\n            struct_name_weight = 0.7 if len(hlsl_fields) &lt;= 3 else 0.5\n            result = align_structs(cpp_data, hlsl_data, struct_name_weight)\n\n            if result is not None:\n                score, align_matches, report = result\n                logging.debug(f\"Evaluating {hlsl_name} vs {cpp_name}: score={score:.3f}\")\n\n                # Apply score boosts if requested\n                if apply_score_boosts:\n                    if cpp_name == hlsl_name:\n                        score *= 1.05\n                    if similarity &gt; 0.85 and (\n                        idx == 0 and (len(candidates) == 1 or similarity - candidates[1][2] &gt; 0.2)\n                    ):\n                        score *= 1.05\n\n                # Store evaluated candidate with full alignment info\n                candidate = StructCandidate(\n                    name=cpp_name, data=cpp_data, score=score, align_matches=align_matches, report=report\n                )\n                evaluated_candidates.append(candidate)\n\n                if score &gt; best_score:\n                    logging.debug(f\"New best match for {hlsl_name}: {cpp_name} (score={score:.3f})\")\n                    best_score = score\n                    best_match = StructMatch(\n                        hlsl_name=hlsl_name,\n                        hlsl_file=hlsl_data[\"file\"],\n                        hlsl_line=hlsl_data[\"line\"],\n                        cpp_name=cpp_name,\n                        cpp_file=cpp_data[\"file\"],\n                        cpp_line=cpp_data[\"line\"],\n                        score=score,\n                        align_matches=align_matches,\n                        report=report,\n                        candidates=evaluated_candidates,  # Will be updated below\n                    )\n\n        # Re-sort candidates by actual evaluation scores\n        evaluated_candidates.sort(key=lambda x: x.score, reverse=True)\n\n        # Update the match to use sorted candidates\n        if best_match:\n            best_match.candidates = evaluated_candidates\n\n        return best_match, evaluated_candidates\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.StructAnalyzer.add_buffer_location","title":"<code>add_buffer_location(file, buffer_name, line)</code>","text":"<p>Add a buffer location to the map.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File path</p> required <code>buffer_name</code> <code>str</code> <p>Name of the buffer</p> required <code>line</code> <code>int</code> <p>Line number</p> required Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def add_buffer_location(self, file: str, buffer_name: str, line: int) -&gt; None:\n    \"\"\"Add a buffer location to the map.\n\n    Args:\n        file: File path\n        buffer_name: Name of the buffer\n        line: Line number\n    \"\"\"\n    key = f\"{file.lower()}:{buffer_name.lower()}\"\n    self.buffer_locations[key] = (file, buffer_name)\n    logging.debug(f\"Added buffer location: {key} -&gt; ({file}, {buffer_name})\")\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.StructAnalyzer.compare_all_structs","title":"<code>compare_all_structs(result_map)</code>","text":"<p>Compare HLSL and C++ structs, generating alignment reports.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def compare_all_structs(self, result_map: ResultMap) -&gt; dict[str, dict[str, str | bool]]:\n    \"\"\"Compare HLSL and C++ structs, generating alignment reports.\"\"\"\n    analysis_links: dict[str, dict[str, str | bool]] = {}\n    matches: list[StructMatch] = []\n    matched_cpp_structs = set()\n\n    match_counts = {\n        \"Matched\": 0,  # Perfect matches\n        \"Mismatched\": 0,  # Partial matches\n        \"Unmatched\": 0,  # No matches\n    }\n\n    if not self.hlsl_structs:\n        print(\"\\nNo HLSL structs found.\")\n        self.analysis_links = analysis_links\n        return analysis_links\n\n    for hlsl_name, hlsl_struct_list in self.hlsl_structs.items():\n        for hlsl_data in hlsl_struct_list:\n            if not isinstance(hlsl_data, dict):\n                logging.error(f\"Invalid hlsl_data for {hlsl_name}: {hlsl_data}\")\n                continue\n\n            logging.debug(\n                f\"Processing HLSL struct: {hlsl_name} : size={hlsl_data.get('size')} : {hlsl_data.get('file', '')}:{hlsl_data.get('line', '')}\"\n            )\n\n            if self._is_composite_buffer(hlsl_data):\n                self._process_composite_buffer(hlsl_name, hlsl_data, matches, matched_cpp_structs)\n                continue\n\n            if hlsl_data.get(\"is_template\") and \"template_type\" in hlsl_data:\n                template_type = hlsl_data[\"template_type\"]\n                if template_type in self.hlsl_structs:\n                    hlsl_data[\"fields\"] = self.hlsl_structs[template_type][0][\"fields\"]\n            hlsl_fields = self.get_nested_fields(hlsl_data)\n\n            if not hlsl_fields:\n                continue\n\n            # Find all potential candidates (including exact name matches)\n            candidates = self.find_struct_candidates(hlsl_name, hlsl_data, hlsl_fields, matched_cpp_structs)\n\n            # Use shared helper to find best match with score boosts\n            best_match, sorted_candidates = self._find_best_match_from_candidates(\n                hlsl_name, hlsl_data, hlsl_fields, candidates, apply_score_boosts=True\n            )\n\n            # ONLY AFTER finding the best match, check if it's good enough\n            if best_match:\n                # Check if the match is good enough\n                if self._is_match_good_enough(\n                    best_match.score, best_match.report, sorted_candidates, hlsl_name, best_match.cpp_name\n                ):\n                    matches.append(best_match)\n                    logging.debug(\n                        f\"Accepted match for {hlsl_name}: {best_match.cpp_name} (score={best_match.score:.3f})\"\n                    )\n                else:\n                    # For rejected matches, create a match with empty cpp info but preserve candidate data\n                    matches.append(\n                        StructMatch(\n                            hlsl_name=hlsl_name,\n                            hlsl_file=hlsl_data[\"file\"],\n                            hlsl_line=hlsl_data[\"line\"],\n                            cpp_name=\"\",  # Empty cpp_name indicates rejection\n                            cpp_file=\"\",  # Empty cpp_file\n                            cpp_line=0,  # Empty cpp_line\n                            score=0.0,  # Zero score for status determination\n                            align_matches=[],  # Empty align_matches for rejected\n                            report=best_match.report,  # Preserve the actual report from best match\n                            candidates=sorted_candidates,  # Preserve sorted candidates with full alignment info\n                        )\n                    )\n                    logging.debug(\n                        f\"Rejected match for {hlsl_name}: {best_match.cpp_name} (score={best_match.score:.3f}) - quality too low\"\n                    )\n            else:\n                # No candidates found\n                matches.append(\n                    StructMatch(\n                        hlsl_name=hlsl_name,\n                        hlsl_file=hlsl_data[\"file\"],\n                        hlsl_line=hlsl_data[\"line\"],\n                        cpp_name=\"\",  # Empty cpp_name\n                        cpp_file=\"\",  # Empty cpp_file\n                        cpp_line=0,  # Empty cpp_line\n                        score=0.0,  # Zero score\n                        align_matches=[],  # Empty align_matches\n                        report={  # Empty report\n                            \"score\": 0.0,\n                            \"exact_matches\": 0,\n                            \"high_sim_matches\": 0,\n                            \"total_fields\": len(hlsl_fields),\n                            \"missing_fields\": len(hlsl_fields),\n                        },\n                        candidates=sorted_candidates,  # Empty sorted_candidates\n                    )\n                )\n\n    self.matches = matches\n\n    self._generate_comparison_tables(matches, print_tables=False)\n    # Store analysis_links as an attribute for later use in print_comparison_tables\n    self.analysis_links = analysis_links\n\n    # Update analysis links in result_map\n    for match in matches:\n        key = f\"{match.hlsl_file.lower()}:{match.hlsl_name.lower()}\"\n\n        # New logic for status:\n        if not match.cpp_name:\n            status = \"Unmatched\"\n            match_counts[\"Unmatched\"] += 1\n        else:\n            field_diff_count = match.report.get(\"field_diff_count\", 0)\n            cpp_total_fields = match.report.get(\"cpp_total_fields\", 0)\n            hlsl_total_fields = match.report.get(\"total_fields\", 0)\n            min_fields = min(hlsl_total_fields, cpp_total_fields)\n            diff_ratio = field_diff_count / max(1, min_fields)\n            debug_msg = f\"DEBUG: {match.hlsl_name} vs {match.cpp_name}: field_diff_count={field_diff_count}, min_fields={min_fields}, diff_ratio={diff_ratio}, score={match.score}\"\n            add_debug_info(debug_msg)\n            if diff_ratio &gt; 0.5 or match.score &lt; 0.75:\n                status = \"Unmatched\"\n                match_counts[\"Unmatched\"] += 1\n            elif field_diff_count == 0:\n                status = \"Matched\"\n                match_counts[\"Matched\"] += 1\n            elif field_diff_count &lt;= 1:\n                status = f\"Mismatched ({match.cpp_name})\"\n                match_counts[\"Mismatched\"] += 1\n            else:\n                status = f\"Mismatched ({match.cpp_name})\"\n                match_counts[\"Mismatched\"] += 1\n\n        analysis_links[key] = {\n            \"link\": create_struct_analysis_link(match.hlsl_name, match.hlsl_file, status),\n            \"is_match\": bool(match.cpp_name),\n            \"cpp_name\": match.cpp_name,\n            \"cpp_file\": match.cpp_file,\n            \"cpp_line\": match.cpp_line,\n            \"score\": match.score,\n            \"status\": status,\n        }\n        # Look for entries using template type for struct analysis\n        for result_key, entry in result_map.items():\n            file_path = entry.get(\"File Path\", \"\").lower()\n            buffer_name = entry.get(\"Name\", \"\").lower()\n            template_type = entry.get(\"Template Type\", \"\")\n\n            # Use template type for struct analysis if available, otherwise use buffer name\n            struct_name = template_type.lower() if template_type else buffer_name\n\n            # Check if this buffer entry should link to this struct analysis\n            if file_path == match.hlsl_file.lower() and struct_name == match.hlsl_name.lower():\n                entry[\"Matching Struct Analysis\"] = analysis_links[key][\"link\"]\n                logging.debug(\n                    f\"Updated result_map for {result_key} using template type {template_type} with link: {analysis_links[key]['link']}\"\n                )\n                break\n        else:\n            # Fallback: try name-only match for user-defined types\n            for result_key, entry in result_map.items():\n                buffer_name = entry.get(\"Name\", \"\").lower()\n                template_type = entry.get(\"Template Type\", \"\")\n                struct_name = template_type.lower() if template_type else buffer_name\n\n                if struct_name == match.hlsl_name.lower() and match.hlsl_name.lower() not in BASE_TYPE_SIZES:\n                    entry[\"Matching Struct Analysis\"] = analysis_links[key][\"link\"]\n                    logging.debug(f\"Fallback name-only match for {match.hlsl_name} to result_map key {result_key}\")\n                    break\n            else:\n                logging.warning(f\"No buffer table entry found for struct {key}\")\n\n    for entry in result_map.values():\n        if \"Matching Struct Analysis\" not in entry:\n            entry[\"Matching Struct Analysis\"] = \"Unmatched\"\n            name = entry.get(\"Name\", \"unknown\")\n            file_path = entry.get(\"File Path\", \"unknown\")\n            logging.debug(f\"Buffer {name} in {file_path} not matched to any struct\")\n\n    return analysis_links\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.StructAnalyzer.find_struct_candidates","title":"<code>find_struct_candidates(hlsl_name, hlsl_data, hlsl_fields, matched_cpp_structs)</code>","text":"<p>Find candidate C++ structs for an HLSL struct.</p> <p>Parameters:</p> Name Type Description Default <code>hlsl_name</code> <code>str</code> <p>Name of the HLSL struct.</p> required <code>hlsl_data</code> <code>StructDict</code> <p>HLSL struct metadata.</p> required <code>hlsl_fields</code> <code>list[FieldDict]</code> <p>List of HLSL fields.</p> required <code>matched_cpp_structs</code> <code>set[str]</code> <p>Set of already matched C++ struct names.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, StructDict, float]]</code> <p>list[tuple[str, StructDict, float]]: List of (cpp_name, cpp_data, similarity) tuples.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def find_struct_candidates(\n    self,\n    hlsl_name: str,\n    hlsl_data: StructDict,\n    hlsl_fields: list[FieldDict],\n    matched_cpp_structs: set[str],\n) -&gt; list[tuple[str, StructDict, float]]:\n    \"\"\"Find candidate C++ structs for an HLSL struct.\n\n    Args:\n        hlsl_name: Name of the HLSL struct.\n        hlsl_data: HLSL struct metadata.\n        hlsl_fields: List of HLSL fields.\n        matched_cpp_structs: Set of already matched C++ struct names.\n\n    Returns:\n        list[tuple[str, StructDict, float]]: List of (cpp_name, cpp_data, similarity) tuples.\n    \"\"\"\n    candidates: list[tuple[str, StructDict, float]] = []\n\n    for cpp_name, cpp_struct_list in self.cpp_structs.items():\n        for cpp_data in cpp_struct_list:\n            # Test original version\n            original_fields = cpp_data.get(\"fields\", [])\n            if original_fields:\n                temp_cpp_data = dict(cpp_data)\n                temp_cpp_data[\"fields\"] = original_fields\n\n                result = align_structs(temp_cpp_data, hlsl_data, 0.5)\n                if result is not None:\n                    alignment_score, _, _ = result\n                    candidates.append((cpp_name, temp_cpp_data, alignment_score))\n\n                    logging.debug(\n                        f\"\\t\\t Candidate {cpp_name} (original) from {cpp_data.get('file', 'unknown')}: \"\n                        f\"alignment_score={alignment_score:.2f}, total_size={calculate_struct_size(original_fields)}, fields: {[(f['name'], f['type'], f['size']) for f in original_fields]}\"\n                    )\n\n            # Test flattened version if different\n            flattened_fields = self.get_nested_fields(cpp_data)\n            if flattened_fields != original_fields:\n                temp_cpp_data = dict(cpp_data)\n                temp_cpp_data[\"fields\"] = flattened_fields\n                temp_cpp_data[\"size\"] = calculate_struct_size(flattened_fields)\n\n                result = align_structs(temp_cpp_data, hlsl_data, 0.5)\n                if result is not None:\n                    alignment_score, _, _ = result\n                    candidates.append((f\"{cpp_name}\", temp_cpp_data, alignment_score))\n\n                    logging.debug(\n                        f\"\\t\\t Candidate {cpp_name} (flattened) from {cpp_data.get('file', 'unknown')}: \"\n                        f\"alignment_score={alignment_score:.2f}, total_size={calculate_struct_size(flattened_fields)}, fields: {[(f['name'], f['type'], f['size']) for f in flattened_fields]}\"\n                    )\n    candidates.sort(key=lambda x: x[2], reverse=True)\n    return candidates\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.StructAnalyzer.get_buffer_location","title":"<code>get_buffer_location(file, buffer_name, line)</code>","text":"<p>Get the location of a buffer.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File path</p> required <code>buffer_name</code> <code>str</code> <p>Name of the buffer</p> required <code>line</code> <code>int</code> <p>Line number</p> required <p>Returns:</p> Type Description <code>tuple[str, int] | None</code> <p>tuple[str, int] | None: (file, line) tuple if found, None otherwise</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def get_buffer_location(self, file: str, buffer_name: str, line: int) -&gt; tuple[str, int] | None:\n    \"\"\"Get the location of a buffer.\n\n    Args:\n        file: File path\n        buffer_name: Name of the buffer\n        line: Line number\n\n    Returns:\n        tuple[str, int] | None: (file, line) tuple if found, None otherwise\n    \"\"\"\n    key = f\"{file.lower()}:{buffer_name.lower()}\"\n    return self.buffer_locations.get(key)\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.StructAnalyzer.get_field_name","title":"<code>get_field_name(field)</code>","text":"<p>Get the field name without array notation.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>FieldDict</code> <p>Field dictionary containing name information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Field name without array notation.</p> Example <p>analyzer.get_field_name({\"name\": \"myArray[3]\"}) 'myArray'</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def get_field_name(self, field: FieldDict) -&gt; str:\n    \"\"\"Get the field name without array notation.\n\n    Args:\n        field: Field dictionary containing name information.\n\n    Returns:\n        str: Field name without array notation.\n\n    Example:\n        &gt;&gt;&gt; analyzer.get_field_name({\"name\": \"myArray[3]\"})\n        'myArray'\n    \"\"\"\n    if not isinstance(field, dict) or \"name\" not in field:\n        logging.warning(f\"Invalid field dictionary: {field}\")\n        return \"\"\n    return strip_array_notation(field[\"name\"])\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.StructAnalyzer.get_nested_fields","title":"<code>get_nested_fields(struct_data)</code>","text":"<p>Get all fields from a struct, including nested struct fields.</p> <p>Parameters:</p> Name Type Description Default <code>struct_data</code> <code>StructDict</code> <p>Dictionary containing struct metadata.</p> required <p>Returns:</p> Type Description <code>list[FieldDict]</code> <p>list[FieldDict]: List of field dictionaries, including nested fields.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def get_nested_fields(self, struct_data: StructDict) -&gt; list[FieldDict]:\n    \"\"\"Get all fields from a struct, including nested struct fields.\n\n    Args:\n        struct_data: Dictionary containing struct metadata.\n\n    Returns:\n        list[FieldDict]: List of field dictionaries, including nested fields.\n    \"\"\"\n    fields = struct_data.get(\"fields\", [])\n    if not fields:\n        return []\n\n    processed_fields: list[FieldDict] = []\n\n    # Determine which struct dictionary to use based on the struct's origin\n    struct_file = struct_data.get(\"file\", \"\")\n    is_cpp_struct = struct_file.endswith((\".cpp\", \".h\", \".hpp\"))\n    struct_dict = self.cpp_structs if is_cpp_struct else self.hlsl_structs\n\n    for field in fields:\n        field_type = field.get(\"type\", \"\")\n        if field_type in struct_dict:\n            # Process each StructDict in the list for nested structs\n            nested_structs = struct_dict[field_type]\n            struct_file = struct_data.get(\"file\", \"\")\n            preferred_struct = None\n            for nested_struct in nested_structs:\n                if not isinstance(nested_struct, dict):\n                    logging.error(f\"Invalid nested struct for {field_type}: {nested_struct}\")\n                    continue\n                if nested_struct.get(\"file\", \"\") == struct_file:\n                    preferred_struct = nested_struct\n                    break\n            if not preferred_struct:\n                preferred_struct = nested_structs[0]  # fallback to first if no file match\n            nested_fields = self.get_nested_fields(preferred_struct)\n            processed_fields.extend(nested_fields)\n        else:\n            processed_fields.append(field)\n\n    return processed_fields\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.StructAnalyzer.print_comparison_tables","title":"<code>print_comparison_tables(only_matched=False, show_top_candidate=False)</code>","text":"<p>Print comparison tables for all HLSL structs, with sub-buffers nested under their parents, using result_map as the source.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def print_comparison_tables(self, only_matched: bool = False, show_top_candidate: bool = False) -&gt; None:\n    \"\"\"Print comparison tables for all HLSL structs, with sub-buffers nested under their parents, using result_map as the source.\"\"\"\n    print(\"\\n# Struct Comparison Results\")\n    printed_keys = set()\n    # Build a lookup for table data\n    table_lookup = {f\"{t['hlsl_name']}:{t['hlsl_data']['file']}\": t for t in self.comparison_tables}\n    logging.debug(f\"Table lookup keys: {list(table_lookup.keys())}\")\n    # Build a lookup for composite buffer relationships\n    composite_to_subs: dict[str, list[str]] = {}\n    for buffer_name, sub_names in self.composite_buffers.items():\n        for sub_name in sub_names:\n            # Find the file for the sub-buffer\n            for t in self.comparison_tables:\n                if t[\"hlsl_name\"] == sub_name:\n                    sub_key = f\"{sub_name}:{t['hlsl_data']['file']}\"\n                    parent_key = f\"{buffer_name}:{t['hlsl_data']['file']}\"\n                    composite_to_subs.setdefault(parent_key, []).append(\n                        sub_key\n                    )  # Use buffer_locations as the source\n    logging.debug(f\"Buffer locations: {list(self.buffer_locations.items())}\")\n    for _key, entry in self.buffer_locations.items():\n        file, buffer_name = entry\n        composite_key = f\"{buffer_name}:{file}\"\n        # If this is a composite buffer, check if any sub-buffers should be printed\n        if composite_key in composite_to_subs:\n            sub_keys = composite_to_subs[composite_key]\n            # Check if composite or any sub-buffer should be printed\n            keys_to_check = [composite_key, *sub_keys]\n\n            def should_print(k):\n                table = table_lookup.get(k)\n                if not table:\n                    return False\n                status = self.analysis_links.get(\n                    f\"{table['hlsl_data']['file'].lower()}:{table['hlsl_name'].lower()}\", {}\n                ).get(\"status\", \"\")\n                if only_matched:\n                    return status != \"Unmatched\"\n                return True\n\n            should_print_any = any(should_print(k) for k in keys_to_check)\n            if should_print_any and composite_key not in printed_keys:\n                print(f\"\\n## Composite Buffer: {buffer_name}\\n\")\n                # Print composite buffer table if present\n                table = table_lookup.get(composite_key)\n                if table:\n                    # Handle show_top_candidate for composite buffer\n                    cpp_name = table[\"cpp_name\"]\n                    cpp_data = table[\"cpp_data\"]\n                    align_matches = table[\"align_matches\"]\n                    report = table[\"report\"]\n\n                    # If no accepted match but we want to show top candidate, extract from candidates\n                    if show_top_candidate and not cpp_name and table[\"candidates\"]:\n                        top_candidate = table[\"candidates\"][0]  # Best candidate\n                        cpp_name = f\"{top_candidate[0]} (top candidate - rejected)\"\n                        cpp_data = top_candidate[1]\n                        # For regular candidate tuples, use empty alignment data\n                        align_matches = []\n                        report = table[\"report\"]\n                        print(\n                            generate_comparison_table(\n                                table[\"hlsl_name\"],\n                                cpp_name,\n                                table[\"hlsl_data\"],\n                                cpp_data,\n                                align_matches,\n                                report,\n                                table[\"candidates\"],\n                                status=self.analysis_links.get(\n                                    f\"{table['hlsl_data']['file'].lower()}:{table['hlsl_name'].lower()}\",\n                                    {},\n                                ).get(\"status\", \"\"),\n                                show_top_candidate=show_top_candidate,\n                                section_id=create_struct_section_id(table[\"hlsl_name\"], table[\"hlsl_data\"][\"file\"]),\n                            )\n                        )\n                    printed_keys.add(composite_key)\n                # Print sub-buffers\n                for sub_key in sub_keys:\n                    if sub_key in printed_keys:\n                        continue\n                    table = table_lookup.get(sub_key)\n                    if table and should_print(sub_key):\n                        # Handle show_top_candidate for sub-buffers\n                        cpp_name = table[\"cpp_name\"]\n                        cpp_data = table[\"cpp_data\"]\n                        align_matches = table[\"align_matches\"]\n                        report = table[\"report\"]\n\n                        # If no accepted match but we want to show top candidate, use the first candidate\n                        if show_top_candidate and not cpp_name and table[\"candidates\"]:\n                            top_candidate = table[\"candidates\"][0]  # First candidate should be best after sorting\n                            cpp_name = f\"{top_candidate[0]} (top candidate - rejected)\"\n                            cpp_data = top_candidate[1]\n                            # Use the alignment data from the candidate\n                            align_matches = top_candidate[3] if len(top_candidate) &gt; 3 else []\n                            report = top_candidate[4] if len(top_candidate) &gt; 4 else table[\"report\"]\n\n                        print(\n                            generate_comparison_table(\n                                table[\"hlsl_name\"],\n                                cpp_name,\n                                table[\"hlsl_data\"],\n                                cpp_data,\n                                align_matches,\n                                report,\n                                table[\"candidates\"],\n                                status=self.analysis_links.get(\n                                    f\"{table['hlsl_data']['file'].lower()}:{table['hlsl_name'].lower()}\",\n                                    {},\n                                ).get(\"status\", \"\"),\n                                depth=3,\n                            )\n                        )\n                        printed_keys.add(sub_key)\n        # Otherwise, print as a regular buffer if not already printed\n        else:\n            key_lookup = f\"{buffer_name}:{file}\"\n            logging.debug(f\"Looking for key_lookup: {key_lookup}\")\n            if key_lookup in printed_keys:\n                logging.debug(f\"Already printed: {key_lookup}\")\n                continue\n            table = table_lookup.get(key_lookup)\n            if table:\n                logging.debug(f\"Found table for: {key_lookup}\")\n                status = self.analysis_links.get(\n                    f\"{table['hlsl_data']['file'].lower()}:{table['hlsl_name'].lower()}\", {}\n                ).get(\"status\", \"\")\n                logging.debug(f\"Status for {key_lookup}: {status}\")\n                if only_matched and status == \"Unmatched\":\n                    logging.debug(f\"Skipping unmatched: {key_lookup}\")\n                    continue\n                # Handle show_top_candidate for regular buffers\n                cpp_name = table[\"cpp_name\"]\n                cpp_data = table[\"cpp_data\"]\n\n                # If no accepted match but we want to show top candidate, use the first candidate\n                if show_top_candidate and not cpp_name and table[\"candidates\"]:\n                    top_candidate = table[\"candidates\"][0]  # First candidate should be best after sorting\n                    cpp_name = f\"{top_candidate[0]} (top candidate - rejected)\"\n                    cpp_data = top_candidate[1]\n                    # Use empty alignment data for simple candidate tuples\n                    align_matches = []\n                    report = table[\"report\"]\n                else:\n                    align_matches = table[\"align_matches\"]\n                    report = table[\"report\"]\n\n                print(\n                    generate_comparison_table(\n                        table[\"hlsl_name\"],\n                        cpp_name,\n                        table[\"hlsl_data\"],\n                        cpp_data,\n                        align_matches,\n                        report,\n                        table[\"candidates\"],\n                        status=status,\n                    )\n                )\n                printed_keys.add(key_lookup)\n            else:\n                logging.debug(f\"No table found for: {key_lookup}\")\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.StructAnalyzer.update_result_map","title":"<code>update_result_map(result_map)</code>","text":"<p>Update the result map with stored analysis results.</p> <p>Parameters:</p> Name Type Description Default <code>result_map</code> <code>dict[str, dict[str, Any]]</code> <p>Dictionary mapping keys to buffer metadata, to be updated with analysis links.</p> required Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def update_result_map(self, result_map: dict[str, dict[str, Any]]) -&gt; None:\n    \"\"\"Update the result map with stored analysis results.\n\n    Args:\n        result_map: Dictionary mapping keys to buffer metadata, to be updated with analysis links.\n    \"\"\"\n    if not self.analysis_results:\n        logging.debug(\"No analysis results available to update result_map\")\n        return\n\n    # Create template type mapping\n    template_types: dict[str, str] = {}\n    for hlsl_name, struct_list in self.hlsl_structs.items():\n        for struct in struct_list:\n            if struct.get(\"is_template\") and \"template_type\" in struct:\n                template_types[hlsl_name] = struct[\"template_type\"]\n\n    for key, analysis in self.analysis_results.items():\n        if key in result_map:\n            result_map[key][\"Matching Struct Analysis\"] = analysis[\"link\"]\n            logging.debug(f\"Updated result_map for {key} with link: {analysis['link']}\")\n            continue\n\n        try:\n            file, buffer_name = key.split(\":\")\n        except ValueError:\n            logging.warning(f\"Invalid key format in analysis_results: {key}\")\n            continue\n\n        for result_key, entry in result_map.items():\n            if entry.get(\"File Path\", \"\").lower() == file and entry.get(\"Name\", \"\").lower() == buffer_name:\n                result_map[result_key][\"Matching Struct Analysis\"] = analysis[\"link\"]\n                logging.debug(f\"Matched {key} to result_map key {result_key}\")\n                break\n            # Fallback: try file+name match (legacy)\n            elif entry.get(\"File Path\", \"\").lower() == file and entry.get(\"Name\", \"\").lower() == buffer_name:\n                result_map[result_key][\"Matching Struct Analysis\"] = analysis[\"link\"]\n                logging.debug(f\"Fallback file+name match for {key} to result_map key {result_key}\")\n                break\n            # Fallback: try name-only match for user-defined types\n            elif entry.get(\"Name\", \"\").lower() == buffer_name and buffer_name not in BASE_TYPE_SIZES:\n                result_map[result_key][\"Matching Struct Analysis\"] = analysis[\"link\"]\n                logging.debug(f\"Fallback name-only match for {key} to result_map key {result_key}\")\n                break\n            else:\n                logging.warning(f\"No buffer table entry found for struct {key}\")\n    for entry in result_map.values():\n        if \"Matching Struct Analysis\" not in entry:\n            entry[\"Matching Struct Analysis\"] = \"Unmatched\"\n            name = entry.get(\"Name\", \"unknown\")\n            file_path = entry.get(\"File Path\", \"unknown\")\n            logging.debug(f\"Buffer {name} in {file_path} not matched to any struct\")\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.StructCandidate","title":"<code>StructCandidate</code>  <code>dataclass</code>","text":"<p>Represents a candidate C++ struct for matching with an HLSL struct.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>@dataclass\nclass StructCandidate:\n    \"\"\"Represents a candidate C++ struct for matching with an HLSL struct.\"\"\"\n\n    name: str\n    data: StructDict\n    score: float\n    align_matches: list[tuple[FieldDict | None, FieldDict | None]]\n    report: dict[str, Any]\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.StructMatch","title":"<code>StructMatch</code>  <code>dataclass</code>","text":"<p>Represents a match between an HLSL struct and a C++ struct.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>@dataclass\nclass StructMatch:\n    \"\"\"Represents a match between an HLSL struct and a C++ struct.\"\"\"\n\n    hlsl_name: str\n    hlsl_file: str\n    hlsl_line: int\n    cpp_name: str\n    cpp_file: str\n    cpp_line: int\n    score: float\n    align_matches: list[tuple[FieldDict | None, FieldDict | None]]\n    report: dict[str, Any]\n    candidates: list[StructCandidate]\n\n    @property\n    def is_matched(self) -&gt; bool:\n        \"\"\"True if there's a valid C++ match.\"\"\"\n        return bool(self.cpp_name and self.score &gt; 0)\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.StructMatch.is_matched","title":"<code>is_matched: bool</code>  <code>property</code>","text":"<p>True if there's a valid C++ match.</p>"},{"location":"modules/#hlslkit.buffer_scan.add_debug_info","title":"<code>add_debug_info(message)</code>","text":"<p>Add debug information to be included in the output.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def add_debug_info(message: str) -&gt; None:\n    \"\"\"Add debug information to be included in the output.\"\"\"\n    DEBUG_INFO.append(message)\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.are_fields_equivalent","title":"<code>are_fields_equivalent(cpp_field, hlsl_field)</code>","text":"<p>Check if fields are equivalent despite different types.</p> <p>This function uses the following criteria to determine equivalence: 1. Name similarity (using compute_name_similarity) 2. Type compatibility (using normalize_field_type) 3. Size matching</p> <p>Parameters:</p> Name Type Description Default <code>cpp_field</code> <code>dict</code> <p>C++ field dictionary with name, type and size.</p> required <code>hlsl_field</code> <code>dict</code> <p>HLSL field dictionary with name, type and size.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if fields are equivalent, False otherwise.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def are_fields_equivalent(cpp_field: dict, hlsl_field: dict) -&gt; bool:\n    \"\"\"Check if fields are equivalent despite different types.\n\n    This function uses the following criteria to determine equivalence:\n    1. Name similarity (using compute_name_similarity)\n    2. Type compatibility (using normalize_field_type)\n    3. Size matching\n\n    Args:\n        cpp_field: C++ field dictionary with name, type and size.\n        hlsl_field: HLSL field dictionary with name, type and size.\n\n    Returns:\n        bool: True if fields are equivalent, False otherwise.\n    \"\"\"\n    # Get field similarities using our helper function\n    type_sim, name_sim, size_match = get_field_similarity(cpp_field, hlsl_field)\n\n    # Fields are equivalent if they have both matching names and types\n    return name_sim &gt;= NAME_SIM_THRESHOLD and type_sim == 1.0 and size_match\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.calculate_hlsl_struct_size","title":"<code>calculate_hlsl_struct_size(fields)</code>","text":"<p>Calculate HLSL struct size, handling packoffset and 16-byte alignment.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list[FieldDict]</code> <p>List of field dictionaries with type, size, and optional packoffset.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total size in bytes, aligned to 16-byte boundaries.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def calculate_hlsl_struct_size(fields: list[FieldDict]) -&gt; int:\n    \"\"\"Calculate HLSL struct size, handling packoffset and 16-byte alignment.\n\n    Args:\n        fields: List of field dictionaries with type, size, and optional packoffset.\n\n    Returns:\n        int: Total size in bytes, aligned to 16-byte boundaries.\n    \"\"\"\n    offset = 0\n    max_offset = 0\n    for field in fields:\n        packoffset = field.get(\"packoffset\")\n        field_size, _ = get_field_size(field[\"type\"], field.get(\"array_size\", 1))\n        if packoffset:\n            match = re.match(r\"c(\\d+)\\.([xyzw])\", packoffset)\n            if match:\n                register = int(match.group(1))\n                component = {\"x\": 0, \"y\": 4, \"z\": 8, \"w\": 12}[match.group(2)]\n                field_offset = register * ALIGN_TO_16 + component\n                offset = max(offset, field_offset)\n            else:\n                raise ValueError\n        else:\n            offset = (offset + (ALIGN_TO_4 - 1)) &amp; ~(ALIGN_TO_4 - 1)  # Align to 4-byte boundary\n        offset += field_size\n        max_offset = max(max_offset, offset)\n    return (max_offset + (ALIGN_TO_16 - 1)) &amp; ~(ALIGN_TO_16 - 1)  # Align to 16-byte boundary\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.calculate_struct_size","title":"<code>calculate_struct_size(fields, align_to_16=False)</code>","text":"<p>Calculate total size of a struct, accounting for alignment.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list[FieldDict]</code> <p>List of field dictionaries.</p> required <code>align_to_16</code> <code>bool</code> <p>Whether to align to 16-byte boundaries (e.g., for cbuffers).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total size in bytes.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def calculate_struct_size(fields: list[FieldDict], align_to_16: bool = False) -&gt; int:\n    \"\"\"Calculate total size of a struct, accounting for alignment.\n\n    Args:\n        fields: List of field dictionaries.\n        align_to_16: Whether to align to 16-byte boundaries (e.g., for cbuffers).\n\n    Returns:\n        int: Total size in bytes.\n    \"\"\"\n    total_size = 0\n    for field in fields:\n        size = field[\"size\"]\n        if align_to_16:\n            # Align each field to 16-byte boundary for cbuffers\n            total_size = (total_size + (ALIGN_TO_16 - 1)) &amp; ~(ALIGN_TO_16 - 1)\n        total_size += size\n    if align_to_16:\n        total_size = (total_size + (ALIGN_TO_16 - 1)) &amp; ~(ALIGN_TO_16 - 1)\n    # logging.debug(\n    #     f\"Calculated struct size: {total_size} bytes for fields: {[(f['name'], f['type'], f['size']) for f in fields]}\"\n    # )\n    return total_size\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.capture_pattern","title":"<code>capture_pattern(text, pattern)</code>","text":"<p>Capture matches of a pattern in the given text, accounting for line directives.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to search.</p> required <code>pattern</code> <code>str</code> <p>The regular expression pattern to match.</p> required <p>Returns:</p> Type Description <code>list[tuple[int, Match[str]]]</code> <p>list[tuple[int, Match[str]]]: A list of tuples containing the line number and match object.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def capture_pattern(text: str, pattern: str) -&gt; list[tuple[int, Match[str]]]:\n    \"\"\"Capture matches of a pattern in the given text, accounting for line directives.\n\n    Args:\n        text: The text to search.\n        pattern: The regular expression pattern to match.\n\n    Returns:\n        list[tuple[int, Match[str]]]: A list of tuples containing the line number and match object.\n    \"\"\"\n    # line adjust\n    # line 288 \"features/Grass Lighting/Shaders/RunGrass.hlsl\"\n    line_adjust = r\"^#line (?P&lt;line&gt;[0-9]+) \\\"(?P&lt;filename&gt;.*)\\\"\"\n    # Compile the regular expression pattern.\n    regex = re.compile(pattern)\n    results: list[tuple[int, Match[str]]] = []\n    offset = 1\n    # Iterate over the lines of the text.\n    for line_number, line in enumerate(text.splitlines()):\n        line_match = re.match(line_adjust, line)\n        if line_match:\n            offset = int(line_match.group(\"line\")) - line_number - 1\n\n        # Match the pattern against the line.\n        match = regex.match(line)\n\n        # If there is a match, return the line number.\n        if match:\n            results.append((line_number + offset, match))\n\n    # If no match is found, return None.\n    return results\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.clean_body","title":"<code>clean_body(body)</code>","text":"<p>Clean comments and empty lines from struct body.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def clean_body(body: str) -&gt; str:\n    \"\"\"Clean comments and empty lines from struct body.\"\"\"\n    body = re.sub(r\"/\\*.*?\\*/\", \"\", body, flags=re.DOTALL)\n    lines = body.splitlines()\n    cleaned_lines = [re.sub(r\"//.*$\", \"\", line).strip() for line in lines if re.sub(r\"//.*$\", \"\", line).strip()]\n    return \"\\n\".join(cleaned_lines)\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.clear_debug_info","title":"<code>clear_debug_info()</code>","text":"<p>Clear collected debug information.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def clear_debug_info() -&gt; None:\n    \"\"\"Clear collected debug information.\"\"\"\n    DEBUG_INFO.clear()\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.compute_match_score","title":"<code>compute_match_score(hlsl_name, cpp_name, hlsl_fields, cpp_fields, lcs_pairs=None, struct_name_weight=0.5)</code>","text":"<p>Compute overall match score using LCS information.</p> <p>Parameters:</p> Name Type Description Default <code>hlsl_name</code> <code>str</code> <p>Name of HLSL struct</p> required <code>cpp_name</code> <code>str</code> <p>Name of C++ struct</p> required <code>hlsl_fields</code> <code>list[FieldDict]</code> <p>List of HLSL fields</p> required <code>cpp_fields</code> <code>list[FieldDict]</code> <p>List of C++ fields</p> required <code>lcs_pairs</code> <code>Optional[list[tuple[int, int]]]</code> <p>List of matched field index pairs from LCS</p> <code>None</code> <code>struct_name_weight</code> <code>float</code> <p>Weight for struct name similarity</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Match score between 0 and 1</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def compute_match_score(\n    hlsl_name: str,\n    cpp_name: str,\n    hlsl_fields: list[FieldDict],\n    cpp_fields: list[FieldDict],\n    lcs_pairs: Optional[list[tuple[int, int]]] = None,\n    struct_name_weight: float = 0.5,\n) -&gt; float:\n    \"\"\"Compute overall match score using LCS information.\n\n    Args:\n        hlsl_name: Name of HLSL struct\n        cpp_name: Name of C++ struct\n        hlsl_fields: List of HLSL fields\n        cpp_fields: List of C++ fields\n        lcs_pairs: List of matched field index pairs from LCS\n        struct_name_weight: Weight for struct name similarity\n\n    Returns:\n        float: Match score between 0 and 1\n    \"\"\"\n    # Name similarity\n    name_sim = compute_name_similarity(hlsl_name, cpp_name)\n\n    # Field matching score based on LCS\n    lcs_length = len(lcs_pairs) if lcs_pairs else 0\n    max_fields = max(len(hlsl_fields), len(cpp_fields))\n    field_match_score = lcs_length / max_fields if max_fields &gt; 0 else 0.0\n\n    # Field order preservation score\n    order_score = 1.0\n    if lcs_pairs:\n        prev_hlsl, prev_cpp = lcs_pairs[0]\n        for hlsl_idx, cpp_idx in lcs_pairs[1:]:\n            if hlsl_idx &lt; prev_hlsl or cpp_idx &lt; prev_cpp:\n                order_score *= 0.9  # Penalize out-of-order matches\n            prev_hlsl, prev_cpp = hlsl_idx, cpp_idx\n\n    # Combine scores\n    return name_sim * struct_name_weight + field_match_score * (1 - struct_name_weight) * order_score\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.compute_name_similarity","title":"<code>compute_name_similarity(hlsl_field_name, cpp_field_name)</code>","text":"<p>Compute name similarity using difflib and handle array notation.</p> <p>Parameters:</p> Name Type Description Default <code>hlsl_field_name</code> <code>str</code> <p>HLSL field name</p> required <code>cpp_field_name</code> <code>str</code> <p>C++ field name</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Similarity score between 0 and 1</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def compute_name_similarity(hlsl_field_name: str, cpp_field_name: str) -&gt; float:\n    \"\"\"Compute name similarity using difflib and handle array notation.\n\n    Args:\n        hlsl_field_name: HLSL field name\n        cpp_field_name: C++ field name\n\n    Returns:\n        float: Similarity score between 0 and 1\n    \"\"\"\n    # Normalize to lowercase for case-insensitive comparison\n    hlsl_name_lower = hlsl_field_name.lower()\n    cpp_name_lower = cpp_field_name.lower()\n\n    # Try direct sequence matching first\n    matcher = difflib.SequenceMatcher(None, hlsl_name_lower, cpp_name_lower)\n    normal_sim = matcher.ratio()\n\n    # Enhanced: Also get LCS-based similarity\n    matching_blocks = matcher.get_matching_blocks()\n    lcs_length = sum(size for _, _, size in matching_blocks[:-1])  # Exclude final dummy block\n    max_length = max(len(hlsl_name_lower), len(cpp_name_lower))\n    lcs_sim = lcs_length / max_length if max_length &gt; 0 else 0.0\n\n    # Try with array notation stripped (preserving existing logic)\n    hlsl_stripped = strip_array_notation(hlsl_name_lower)\n    cpp_stripped = strip_array_notation(cpp_name_lower)\n    stripped_matcher = difflib.SequenceMatcher(None, hlsl_stripped, cpp_stripped)\n    stripped_sim = stripped_matcher.ratio()\n\n    # Enhanced: LCS similarity for stripped names too\n    stripped_blocks = stripped_matcher.get_matching_blocks()\n    stripped_lcs_length = sum(size for _, _, size in stripped_blocks[:-1])\n    stripped_max_length = max(len(hlsl_stripped), len(cpp_stripped))\n    stripped_lcs_sim = stripped_lcs_length / stripped_max_length if stripped_max_length &gt; 0 else 0.0\n\n    # Look for substring relationships (preserving existing logic)\n    if hlsl_stripped in cpp_stripped or cpp_stripped in hlsl_stripped:\n        min_len = min(len(hlsl_stripped), len(cpp_stripped))\n        max_len = max(len(hlsl_stripped), len(cpp_stripped))\n        if max_len &gt; 0 and min_len / max_len &gt;= 0.7:  # At least 70% coverage\n            substring_sim = 0.9  # Boost similarity for significant substring matches\n            return max(normal_sim, stripped_sim, lcs_sim, stripped_lcs_sim, substring_sim)\n\n    # Prefix/suffix boost\n    shorter, longer = (\n        (hlsl_stripped, cpp_stripped) if len(hlsl_stripped) &lt;= len(cpp_stripped) else (cpp_stripped, hlsl_stripped)\n    )\n    if longer.startswith(shorter) or longer.endswith(shorter) or shorter in longer:\n        # Full shorter string matches as prefix/suffix/substring\n        prefix_sim = 0.85\n        return max(normal_sim, stripped_sim, lcs_sim, stripped_lcs_sim, prefix_sim)\n\n    # Return highest similarity score with small penalty for stripped matches\n    best_score = max(normal_sim, lcs_sim, stripped_sim - 0.01, stripped_lcs_sim - 0.01)\n    return best_score\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.compute_struct_alignment","title":"<code>compute_struct_alignment(cpp_data, hlsl_data, struct_name_weight=0.5)</code>","text":"<p>Compute alignment between HLSL and C++ struct fields.</p> <p>This function analyzes field alignment between C++ and HLSL structs, considering: - Field name similarity using compute_name_similarity - Field type compatibility using normalize_field_type - Field size matching using calculate_struct_size - Struct name similarity using compute_name_similarity</p> <p>Parameters:</p> Name Type Description Default <code>cpp_data</code> <code>dict[str, Any]</code> <p>Dictionary containing C++ struct metadata and fields.</p> required <code>hlsl_data</code> <code>dict[str, Any]</code> <p>Dictionary containing HLSL struct metadata and fields.</p> required <code>struct_name_weight</code> <code>float</code> <p>Weight given to struct name similarity (0.0 to 1.0).</p> <code>0.5</code> <p>Returns:</p> Type Description <code>float</code> <p>A tuple containing:</p> <code>list[tuple[FieldDict | None, FieldDict | None]]</code> <ul> <li>score: Overall alignment score (0.0 to 1.0)</li> </ul> <code>dict[str, Any]</code> <ul> <li>align_matches: List of field alignment tuples</li> </ul> <code>tuple[float, list[tuple[FieldDict | None, FieldDict | None]], dict[str, Any]]</code> <ul> <li>report: Detailed alignment report dictionary</li> </ul> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def compute_struct_alignment(\n    cpp_data: dict[str, Any], hlsl_data: dict[str, Any], struct_name_weight: float = 0.5\n) -&gt; tuple[float, list[tuple[FieldDict | None, FieldDict | None]], dict[str, Any]]:\n    \"\"\"Compute alignment between HLSL and C++ struct fields.\n\n    This function analyzes field alignment between C++ and HLSL structs, considering:\n    - Field name similarity using compute_name_similarity\n    - Field type compatibility using normalize_field_type\n    - Field size matching using calculate_struct_size\n    - Struct name similarity using compute_name_similarity\n\n    Args:\n        cpp_data: Dictionary containing C++ struct metadata and fields.\n        hlsl_data: Dictionary containing HLSL struct metadata and fields.\n        struct_name_weight: Weight given to struct name similarity (0.0 to 1.0).\n\n    Returns:\n        A tuple containing:\n        - score: Overall alignment score (0.0 to 1.0)\n        - align_matches: List of field alignment tuples\n        - report: Detailed alignment report dictionary\n    \"\"\"\n    if not isinstance(cpp_data, dict) or not isinstance(hlsl_data, dict):\n        raise InvalidStructDictType(cpp_data if not isinstance(cpp_data, dict) else hlsl_data)\n\n    cpp_fields = cpp_data.get(\"fields\", [])\n    hlsl_fields = hlsl_data.get(\"fields\", [])\n\n    # Use existing helper functions for field matching\n    align_matches, report = _compute_alignment_report(cpp_fields, hlsl_fields, cpp_data, hlsl_data, struct_name_weight)\n\n    score = report.get(\"score\", 0.0)\n    return score, align_matches, report\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.count_field_differences","title":"<code>count_field_differences(align_matches)</code>","text":"<p>Count field differences, returning (total_diff, name_diff, type_diff).</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def count_field_differences(align_matches) -&gt; tuple[int, int, int]:\n    \"\"\"\n    Count field differences, returning (total_diff, name_diff, type_diff).\n    \"\"\"\n    total_diff = 0\n    name_diff = 0\n    type_diff = 0\n    for hlsl_field, cpp_field in align_matches:\n        if hlsl_field and cpp_field:\n            name_sim = compute_name_similarity(hlsl_field[\"name\"], cpp_field[\"name\"])\n            type_sim = jellyfish.jaro_winkler_similarity(hlsl_field[\"type\"], cpp_field[\"type\"])\n            if name_sim &lt; 1:\n                name_diff += 1\n                total_diff += 1\n            elif type_sim &lt; 0.7:\n                type_diff += 1\n                total_diff += 1\n        else:\n            # Unmatched field counts as both a name and type diff\n            name_diff += 1\n            type_diff += 1\n            total_diff += 1\n    return total_diff, name_diff, type_diff\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.create_link","title":"<code>create_link(text, line=None)</code>","text":"<p>Generate a GitHub link for a given file path and optional line number.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The file path to convert to a GitHub URL.</p> required <code>line</code> <code>int | None</code> <p>Line number to link to.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A URL pointing to the file in the skyrim-community-shaders repository.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def create_link(text: str, line: int | None = None) -&gt; str:\n    \"\"\"Generate a GitHub link for a given file path and optional line number.\n\n    Args:\n        text: The file path to convert to a GitHub URL.\n        line: Line number to link to.\n\n    Returns:\n        str: A URL pointing to the file in the skyrim-community-shaders repository.\n    \"\"\"\n    base_url = f\"https://github.com/doodlum/skyrim-community-shaders/blob/dev/{urllib.parse.quote(text)}\"\n    if line is not None:\n        base_url += f\"#L{line}\"\n    return base_url\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.create_struct_analysis_link","title":"<code>create_struct_analysis_link(struct_name, filename, status)</code>","text":"<p>Generate a struct analysis link with proper formatting.</p> <p>Parameters:</p> Name Type Description Default <code>struct_name</code> <code>str</code> <p>Name of the struct</p> required <code>filename</code> <code>str</code> <p>Filename containing the struct</p> required <code>status</code> <code>str</code> <p>Match status (Matched, Mismatched, or Unmatched)</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted markdown link for struct analysis</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def create_struct_analysis_link(struct_name: str, filename: str, status: str) -&gt; str:\n    \"\"\"Generate a struct analysis link with proper formatting.\n\n    Args:\n        struct_name: Name of the struct\n        filename: Filename containing the struct\n        status: Match status (Matched, Mismatched, or Unmatched)\n\n    Returns:\n        str: Formatted markdown link for struct analysis\n    \"\"\"\n    section_id = create_struct_section_id(struct_name, filename)\n\n    if status == \"Unmatched\":\n        return f\"[Unmatched](#{section_id})\"\n    elif status.startswith(\"Mismatched\"):\n        # Extract the matched struct name from status like \"Mismatched (StructName)\"\n        if \"(\" in status and \")\" in status:\n            matched_name = status[status.find(\"(\") + 1 : status.find(\")\")]\n            return f\"[Mismatched (`{matched_name}`)](#{section_id})\"\n        else:\n            return f\"[Mismatched](#{section_id})\"\n    elif status == \"Matched\":\n        return f\"[`{struct_name}`](#{section_id})\"\n    else:\n        # For any other status, use the struct name with backticks\n        return f\"[`{struct_name}`](#{section_id})\"\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.create_struct_section_id","title":"<code>create_struct_section_id(struct_name, filename)</code>","text":"<p>Generate a clean section ID for struct analysis cross-references.</p> <p>Parameters:</p> Name Type Description Default <code>struct_name</code> <code>str</code> <p>Name of the struct</p> required <code>filename</code> <code>str</code> <p>Filename containing the struct</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Clean section ID suitable for markdown anchors</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def create_struct_section_id(struct_name: str, filename: str) -&gt; str:\n    \"\"\"Generate a clean section ID for struct analysis cross-references.\n\n    Args:\n        struct_name: Name of the struct\n        filename: Filename containing the struct\n\n    Returns:\n        str: Clean section ID suitable for markdown anchors\n    \"\"\"\n    clean_filename = os.path.basename(filename).lower().replace(\".\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n    clean_struct_name = struct_name.lower().replace(\"(\", \"\").replace(\")\", \"\")\n    return f\"hlsl-{clean_struct_name}-{clean_filename}\"\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.extract_cpp_structs","title":"<code>extract_cpp_structs(content, file_path)</code>","text":"<p>Extract C++ structs, skipping any that contain pointers or only static members.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The C++ file content.</p> required <code>file_path</code> <code>str</code> <p>Path to the file for logging and linking.</p> required <p>Returns:</p> Type Description <code>dict[str, dict]</code> <p>dict[str, dict]: Dictionary of struct names to their metadata.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def extract_cpp_structs(content: str, file_path: str) -&gt; dict[str, dict]:\n    \"\"\"Extract C++ structs, skipping any that contain pointers or only static members.\n\n    Args:\n        content (str): The C++ file content.\n        file_path (str): Path to the file for logging and linking.\n\n    Returns:\n        dict[str, dict]: Dictionary of struct names to their metadata.\n    \"\"\"\n    structs = {}\n    struct_pattern = r\"struct\\s+(?:alignas\\(\\d+\\)\\s+)?(?P&lt;name&gt;\\w+)\\s*{(?P&lt;body&gt;[^{}]*?)}\"\n\n    for line_number, match in finditer_with_line_numbers(struct_pattern, content, re.MULTILINE | re.DOTALL):\n        name = match.group(\"name\")\n        if name.lower() in BASE_TYPE_SIZES:\n            logging.debug(f\"Skipping struct/buffer {name} (built-in type) in {file_path}:{line_number}\")\n            continue\n\n        body = clean_body(match.group(\"body\").strip())\n\n        # Parse all fields, including static ones\n        all_fields = []\n        non_static_fields = []\n\n        for field_line in body.split(\";\"):\n            field_line = field_line.strip()\n            if not field_line:\n                continue\n\n            # Check if this is a static declaration\n            is_static = field_line.startswith(\"static \")\n\n            if is_static:\n                logging.debug(f\"Skipping static member in {name}: {field_line}\")\n                continue\n\n            # Parse non-static field\n            parsed_field = parse_field(field_line, name, False)\n            if parsed_field:\n                all_fields.append(parsed_field)\n                non_static_fields.append(parsed_field)\n\n        # Skip struct if it has no non-static fields\n        if not non_static_fields:\n            logging.debug(f\"Skipping C++ struct {name} in {file_path}:{line_number + 1} (no non-static fields)\")\n            continue\n\n        # Skip struct if any non-static field contains a pointer\n        if any(\"*\" in field[\"type\"] for field in non_static_fields):\n            logging.debug(f\"Skipping C++ struct {name} in {file_path}:{line_number + 1} (contains pointers)\")\n            continue\n\n        adjusted_line = line_number + 1\n        structs[name] = {\n            \"fields\": non_static_fields,  # Only store non-static fields\n            \"file\": file_path,\n            \"line\": adjusted_line,\n            \"is_cbuffer\": False,\n            \"is_template\": False,\n            \"body\": body,  # Store raw body for alignment check\n        }\n        size = calculate_struct_size(non_static_fields)\n        logging.debug(\n            f\"Found C++ struct {name} in {file_path}:{adjusted_line} with {len(non_static_fields)} non-static fields: {[(f['name'], f['type'], f['size']) for f in non_static_fields]}, total size: {size} bytes\"\n        )\n\n    return structs\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.extract_hlsl_structs","title":"<code>extract_hlsl_structs(content, file_path)</code>","text":"<p>Extract HLSL structs, cbuffers, ConstantBuffers, and template buffers.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def extract_hlsl_structs(content: str, file_path: str) -&gt; dict[str, dict]:\n    \"\"\"Extract HLSL structs, cbuffers, ConstantBuffers, and template buffers.\"\"\"\n    structs = {}\n    struct_pattern = r\"(struct|cbuffer|ConstantBuffer&lt;(?P&lt;template&gt;\\w+)&gt;)\\s+(?P&lt;name&gt;\\w+)\\s*(?::\\s*register\\s*\\(\\w\\d+\\s*\\))?\\s*{(?P&lt;body&gt;[^{}]*?)}\"\n    template_pattern = (\n        r\"(?:RW)?(?:StructuredBuffer)&lt;(?P&lt;template&gt;\\w+)&gt;\\s+(?P&lt;name&gt;\\w+)\\s*:\\s*register\\s*\\([a-z]\\d+\\s*\\)\"\n    )\n    content_clean = preprocess_content(content, {})\n    content_clean = clean_body(content_clean)\n\n    # Extract structs, cbuffers, and ConstantBuffers\n    for line_number, match in finditer_with_line_numbers(struct_pattern, content, re.MULTILINE | re.DOTALL):\n        name = match.group(\"name\")\n        if name.lower() in BASE_TYPE_SIZES:\n            logging.debug(f\"Skipping struct/buffer {name} (built-in type) in {file_path}:{line_number}\")\n            continue\n        body = clean_body(match.group(\"body\").strip())\n        is_cbuffer = match.group(1) == \"cbuffer\"\n        is_constant_buffer = match.group(1).startswith(\"ConstantBuffer\")\n        fields = [f for field in body.split(\";\") if (f := parse_field(field, name, True))]\n        size = calculate_struct_size(fields, align_to_16=is_cbuffer or is_constant_buffer)\n        structs[name] = {\n            \"fields\": fields,\n            \"file\": file_path,\n            \"line\": line_number,\n            \"is_cbuffer\": is_cbuffer,\n            \"is_constant_buffer\": is_constant_buffer,\n            \"is_template\": False,\n            \"size\": size,\n        }\n        logging.debug(\n            f\"Found HLSL {'cbuffer' if is_cbuffer else 'ConstantBuffer' if is_constant_buffer else 'struct'} {name} in {file_path}:{line_number} with {len(fields)} fields, total size: {size} bytes\"\n        )\n\n    # Second pass: extract and process template buffers\n    for line_number, match in finditer_with_line_numbers(template_pattern, content, re.MULTILINE):\n        template_type = match.group(\"template\")\n        template_name = match.group(\"name\")\n\n        # Skip if template type is a base type\n        if template_type in BASE_TYPE_SIZES:\n            logging.debug(f\"Skipping base type template {template_type} in {file_path}:{line_number}\")\n            continue\n\n        # Only add template type if we don't already have a real definition\n        if template_type not in structs:\n            structs[template_type] = {\n                \"fields\": [],\n                \"file\": file_path,\n                \"line\": line_number,\n                \"is_cbuffer\": False,\n                \"is_template\": True,\n            }\n            logging.debug(f\"Found template struct {template_type} in {file_path}:{line_number}\")\n        else:\n            # Check if existing definition has actual fields (real struct vs empty template)\n            existing_struct = structs[template_type]\n            if existing_struct.get(\"fields\") and not existing_struct.get(\"is_template\", False):\n                logging.debug(\n                    f\"Skipping template {template_type} - real struct definition already exists with {len(existing_struct['fields'])} fields\"\n                )\n                continue\n            else:\n                logging.debug(f\"Template {template_type} found but existing definition is also a template or empty\")\n\n        # Also add the instance as its own entry (only if not already exists)\n        if template_name not in structs:\n            structs[template_name] = {\n                \"fields\": [],\n                \"file\": file_path,\n                \"line\": line_number,\n                \"is_cbuffer\": False,\n                \"is_template\": True,\n                \"template_type\": template_type,\n            }\n\n    return structs\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.extract_matrix_size","title":"<code>extract_matrix_size(field_type)</code>","text":"<p>Extract matrix dimensions from names like XMFLOAT3X4.</p> <p>Parameters:</p> Name Type Description Default <code>field_type</code> <code>str</code> <p>The field type, potentially namespaced (e.g., REX::XMFLOAT3X4).</p> required <p>Returns:</p> Type Description <code>tuple[int, int] | None</code> <p>tuple[int, int] | None: (rows, columns) if matched, else None.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def extract_matrix_size(field_type: str) -&gt; tuple[int, int] | None:\n    \"\"\"\n    Extract matrix dimensions from names like XMFLOAT3X4.\n\n    Args:\n        field_type (str): The field type, potentially namespaced (e.g., REX::XMFLOAT3X4).\n\n    Returns:\n        tuple[int, int] | None: (rows, columns) if matched, else None.\n    \"\"\"\n    match = re.search(r\"FLOAT(\\d)X(\\d)\", field_type.upper())\n    if match:\n        return int(match.group(1)), int(match.group(2))\n    return None\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.extract_structs","title":"<code>extract_structs(content, is_hlsl, file_path)</code>","text":"<p>Extract structs from HLSL or C++ content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The file content to parse.</p> required <code>is_hlsl</code> <code>bool</code> <p>Whether the content is HLSL (True) or C++ (False).</p> required <code>file_path</code> <code>str</code> <p>Path to the file for logging and linking.</p> required <p>Returns:</p> Type Description <code>dict[str, dict]</code> <p>dict[str, dict]: Dictionary of struct names to their metadata.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def extract_structs(content: str, is_hlsl: bool, file_path: str) -&gt; dict[str, dict]:\n    \"\"\"Extract structs from HLSL or C++ content.\n\n    Args:\n        content (str): The file content to parse.\n        is_hlsl (bool): Whether the content is HLSL (True) or C++ (False).\n        file_path (str): Path to the file for logging and linking.\n\n    Returns:\n        dict[str, dict]: Dictionary of struct names to their metadata.\n    \"\"\"\n    logging.debug(f\"Extracting structs from {file_path}\")\n    return extract_hlsl_structs(content, file_path) if is_hlsl else extract_cpp_structs(content, file_path)\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.finditer_with_line_numbers","title":"<code>finditer_with_line_numbers(pattern, string, flags=0, line_map=None)</code>","text":"<p>Find matches of a pattern in a string, returning match objects with adjusted line numbers.</p> <p>This function accounts for <code>#line</code> directives and a line map for preprocessed content.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str | Pattern[str]</code> <p>The regular expression pattern to match.</p> required <code>string</code> <code>str</code> <p>The input string to search.</p> required <code>flags</code> <code>int</code> <p>Regular expression flags. Defaults to 0.</p> <code>0</code> <code>line_map</code> <code>dict[int, int] | None</code> <p>Mapping of preprocessed to original line numbers.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[int, Match[str]]]</code> <p>list[tuple[int, Match[str]]]: A list of tuples containing the adjusted line number and match object.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def finditer_with_line_numbers(\n    pattern: str | Pattern[str],\n    string: str,\n    flags: int = 0,\n    line_map: dict[int, int] | None = None,\n) -&gt; list[tuple[int, Match[str]]]:\n    \"\"\"Find matches of a pattern in a string, returning match objects with adjusted line numbers.\n\n    This function accounts for `#line` directives and a line map for preprocessed content.\n\n    Args:\n        pattern: The regular expression pattern to match.\n        string: The input string to search.\n        flags: Regular expression flags. Defaults to 0.\n        line_map: Mapping of preprocessed to original line numbers.\n\n    Returns:\n        list[tuple[int, Match[str]]]: A list of tuples containing the adjusted line number and match object.\n    \"\"\"\n    # Handle pcpp info on skipped lines\n    line_offsets: dict[int, int] = {}\n    for line_number, line in enumerate(string.splitlines()):\n        line_adjust = r\"^#line (?P&lt;line&gt;[0-9]+) \\\"(?P&lt;filename&gt;.*)\\\"\"\n        line_match = re.match(line_adjust, line)\n        if line_match:\n            offset = int(line_match.group(\"line\")) - line_number - 1\n            line_offsets[line_number] = offset\n\n    matches = list(re.finditer(pattern, string, flags))\n    if not matches:\n        return []\n\n    end = matches[-1].start()\n    newline_table = {-1: 0}\n    for i, m in enumerate(re.finditer(\"\\\\n\", string), 1):\n        offset = m.start()\n        if offset &gt; end:\n            break\n        newline_table[offset] = i\n\n    result: list[tuple[int, Match[str]]] = []\n    for m in matches:\n        newline_offset = string.rfind(\"\\n\", 0, m.start())\n        line_number = newline_table[newline_offset] + 1  # Add 1 since line numbers are 1-based\n        # Apply line_map if provided\n        if line_map and line_number in line_map:\n            line_number = line_map[line_number]\n        # Apply #line directive offset\n        found_offset = 0\n        for k, v in line_offsets.items():\n            if k &lt;= line_number:\n                found_offset = v\n            else:\n                break\n        adjusted_line = line_number + found_offset\n        result.append((adjusted_line, m))\n    return result\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.fuzzy_lcs","title":"<code>fuzzy_lcs(hlsl_fields, cpp_fields, name_sim_threshold=NAME_SIM_THRESHOLD)</code>","text":"<p>Fuzzy sequence matching using difflib for field alignment.</p> <p>Parameters:</p> Name Type Description Default <code>hlsl_fields</code> <code>list[dict]</code> <p>List of HLSL field dictionaries</p> required <code>cpp_fields</code> <code>list[dict]</code> <p>List of C++ field dictionaries</p> required <code>name_sim_threshold</code> <code>float</code> <p>Minimum similarity threshold</p> <code>NAME_SIM_THRESHOLD</code> <p>Returns:</p> Type Description <code>list[tuple[int, int]]</code> <p>list[tuple[int, int]]: List of matching index pairs</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def fuzzy_lcs(\n    hlsl_fields: list[dict],\n    cpp_fields: list[dict],\n    name_sim_threshold: float = NAME_SIM_THRESHOLD,\n) -&gt; list[tuple[int, int]]:\n    \"\"\"Fuzzy sequence matching using difflib for field alignment.\n\n    Args:\n        hlsl_fields: List of HLSL field dictionaries\n        cpp_fields: List of C++ field dictionaries\n        name_sim_threshold: Minimum similarity threshold\n\n    Returns:\n        list[tuple[int, int]]: List of matching index pairs\n    \"\"\"\n    matches = []\n    used_cpp_indices = set()\n\n    # For each HLSL field, find its best C++ match\n    for i, hlsl_field in enumerate(hlsl_fields):\n        best_match_idx = -1\n        best_similarity = 0.0\n\n        for j, cpp_field in enumerate(cpp_fields):\n            if j in used_cpp_indices:\n                continue\n\n            sim = compute_name_similarity(hlsl_field[\"name\"], cpp_field[\"name\"])\n\n            if sim &gt;= name_sim_threshold and sim &gt; best_similarity:\n                best_similarity = sim\n                best_match_idx = j\n\n        # Add the best match if found\n        if best_match_idx &gt;= 0:\n            matches.append((i, best_match_idx))\n            used_cpp_indices.add(best_match_idx)\n            # logging.debug(\n            #     f\"  Match found ({i},{best_match_idx}): {hlsl_field['name']} &lt;-&gt; {cpp_fields[best_match_idx]['name']} (sim: {best_similarity:.3f})\"\n            # )\n\n    return matches\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.generate_comparison_table","title":"<code>generate_comparison_table(hlsl_name, cpp_name, hlsl_data, cpp_data, align_matches, report, candidates, status='', depth=2, section_id=None, show_top_candidate=False)</code>","text":"<p>Generate a comparison table for a pair of HLSL and C++ structs.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def generate_comparison_table(\n    hlsl_name: str,\n    cpp_name: str,\n    hlsl_data: StructDict,\n    cpp_data: StructDict,\n    align_matches: list[tuple[FieldDict | None, FieldDict | None]],\n    report: dict[str, int | float],\n    candidates: list[tuple[str, StructDict, float]],\n    status: str = \"\",\n    depth: int = 2,\n    section_id: Optional[str] = None,\n    show_top_candidate: bool = False,\n) -&gt; str:\n    \"\"\"Generate a comparison table for a pair of HLSL and C++ structs.\"\"\"\n    if not isinstance(hlsl_data, dict):\n        logging.error(f\"Invalid hlsl_data type: {type(hlsl_data)}, expected dict\")\n        return f\"Error: Invalid HLSL data for {hlsl_name}\\n\"\n\n    # Determine if we have a real match\n    has_cpp_match = bool(cpp_name and cpp_data and report.get(\"score\", 0) &gt; 0)\n    is_rejected_candidate = \"(top candidate - rejected)\" in cpp_name if cpp_name else False\n\n    # If showing top candidate and no accepted match, use the top candidate from candidates list\n    if show_top_candidate and not has_cpp_match and not is_rejected_candidate and candidates:\n        top_candidate = candidates[0]  # candidates should be [(name, data, score, align_matches, report), ...]\n        cpp_name = f\"{top_candidate[0]} (top candidate - rejected)\"\n        cpp_data = top_candidate[1]\n        # Use pre-computed alignment data if available (candidates with 5 elements)\n        if len(top_candidate) &gt;= 5:\n            align_matches = top_candidate[3]\n            report = top_candidate[4]\n        else:\n            # Fallback: compute alignment for legacy candidate format\n            result = align_structs(hlsl_data, cpp_data, 0.5)\n            if result:\n                _, align_matches, report = result\n            else:\n                # Ensure we have some basic data even if alignment fails\n                align_matches = []\n                report = {\"score\": top_candidate[2], \"field_diff_count\": 0}\n        is_rejected_candidate = True\n\n    table = \"---\\n\\n\" if depth == 2 else \"\"\n    # Add a section anchor for cross-linking\n    if section_id:\n        table += f'&lt;a id=\"{section_id}\"&gt;&lt;/a&gt;\\n'\n    table += f\"{'#' * depth} HLSL `{hlsl_name}` ({os.path.basename(hlsl_data.get('file', ''))})\\n\"\n    table += f\"**HLSL File:** [{hlsl_data.get('file', '')}:{hlsl_data.get('line', '')}]({create_link(hlsl_data.get('file', ''), hlsl_data.get('line', ''))})\\n\"\n\n    # Handle unmatched case\n    if not has_cpp_match and not is_rejected_candidate:\n        table += \"\\n**No matching C++ struct found**\\n\"\n\n        # Summary section for unmatched structs\n        hlsl_fields = hlsl_data.get(\"fields\", [])\n        hlsl_total_fields = len([f for f in hlsl_fields if not is_padding_field(f)])\n        table += f\"\\n{'#' * (depth + 1)} Summary:\\n\"\n        table += f\"- Total HLSL Fields: {hlsl_total_fields}\\n\"\n        table += \"- Status: Unmatched\\n\"\n    else:  # Handle matched or rejected candidate case\n        table += f\"\\n**C++**: `{cpp_name}`\\n\"\n        table += f\"**C++ File:** [{cpp_data.get('file', '')}:{cpp_data.get('line', '')}]({create_link(cpp_data.get('file', ''), cpp_data.get('line', ''))})\\n\"\n        table += f\"\\n**Match Score:** {report.get('score', 0):.2f}\\n\"\n\n        # Field comparison table (for both matched and rejected candidates)\n        # Always try to show field comparison for matched/rejected candidates\n        if has_cpp_match or is_rejected_candidate:\n            # If we don't have alignment data, compute it now\n            if not align_matches and cpp_data:\n                result = align_structs(hlsl_data, cpp_data, 0.5)\n                if result:\n                    _, align_matches, computed_report = result\n                    # Update report with computed data if we don't have good data\n                    if not report or report.get(\"score\", 0) == 0:\n                        report = computed_report\n\n            if align_matches:\n                field_rows = []\n                rows_with_differences = report.get(\"field_diff_count\", 0)\n\n                for hlsl_field, cpp_field in align_matches:\n                    hlsl_field_name = hlsl_field[\"name\"] if hlsl_field else \"\"\n                    hlsl_field_type = hlsl_field[\"type\"] if hlsl_field else \"\"\n                    cpp_field_name = cpp_field[\"name\"] if cpp_field else \"\"\n                    cpp_field_type = cpp_field[\"type\"] if cpp_field else \"\"\n\n                    # Compute similarities and apply emphasis\n                    if hlsl_field and cpp_field:\n                        type_sim = jellyfish.jaro_winkler_similarity(hlsl_field_type, cpp_field_type)\n                        # For emphasis, use exact string comparison to catch array notation differences\n                        exact_name_match = hlsl_field_name == cpp_field_name\n\n                    # Apply emphasis using helper\n                    if not hlsl_field or not cpp_field:\n                        hlsl_field_type = emphasize_if(True, hlsl_field_type)\n                        hlsl_field_name = emphasize_if(True, hlsl_field_name)\n                        cpp_field_type = emphasize_if(True, cpp_field_type)\n                        cpp_field_name = emphasize_if(True, cpp_field_name)\n                    else:\n                        # Use exact match for emphasis to catch array notation differences\n                        hlsl_field_name = emphasize_if(not exact_name_match, hlsl_field_name)\n                        cpp_field_name = emphasize_if(not exact_name_match, cpp_field_name)\n                        hlsl_field_type = emphasize_if(type_sim &lt; 1, hlsl_field_type)\n                        cpp_field_type = emphasize_if(type_sim &lt; 1, cpp_field_type)\n\n                    field_rows.append({\n                        \"HLSL Type\": hlsl_field_type,\n                        \"HLSL Field\": hlsl_field_name,\n                        \"C++ Type\": cpp_field_type,\n                        \"C++ Field\": cpp_field_name,\n                    })\n\n                # Determine if we should default to closed\n                should_close = rows_with_differences == 0\n                table += f\"\\n&lt;details{'&gt;' if should_close else ' open&gt;'}\\n\"\n                table += f\"&lt;summary&gt;{rows_with_differences} Field Differences&lt;/summary&gt;\\n\\n\"\n\n                # Generate table using py_markdown_table\n                if field_rows:\n                    table += markdown_table(field_rows).set_params(row_sep=\"markdown\", quote=False).get_markdown()\n\n                table += \"\\n&lt;/details&gt;\\n\"\n\n        # Summary section for matched structs\n        table += f\"\\n{'#' * (depth + 1)} Summary:\\n\"\n        table += f\"- Exact Field Matches: {report.get('exact_matches', 0)}\\n\"\n        table += f\"- High Similarity Field Matches: {report.get('high_sim_matches', 0)}\\n\"\n\n        hlsl_total_fields = report.get(\"total_fields\", 0)\n        cpp_total_fields = report.get(\"cpp_total_fields\", 0)\n        table += f\"- Total HLSL Fields: {hlsl_total_fields}\\n\"\n        table += f\"- Total C++ Fields: {cpp_total_fields}\\n\"\n        table += f\"- Field Name Differences: {report.get('field_name_diff_count', 0)}\\n\"\n        table += f\"- Field Type Differences: {report.get('field_type_diff_count', 0)}\\n\"\n\n    # Always show candidates in a details section\n    if candidates:\n        total_candidates = len(candidates)\n        table += f\"\\n&lt;details&gt;\\n&lt;summary&gt;Top 5 of {total_candidates} Candidates Reviewed&lt;/summary&gt;\\n\\n\"\n        candidate_rows = []\n        for candidate in candidates[:5]:\n            # Handle both old and new candidate formats\n            if len(candidate) == 3:\n                cand_name, cand_data, cand_score = candidate\n            else:\n                cand_name, cand_data, cand_score = candidate[0], candidate[1], candidate[2]\n\n            # Count fields in candidate\n            cand_fields = cand_data.get(\"fields\", [])\n            field_count = len([f for f in cand_fields if not is_padding_field(f)])\n\n            candidate_rows.append({\n                \"Candidate Name\": cand_name,\n                \"File\": f\"[{cand_data.get('file', '')}:{cand_data.get('line', '')}]({create_link(cand_data.get('file', ''), cand_data.get('line', ''))})\",\n                \"Fields\": str(field_count),\n                \"Similarity\": f\"{cand_score:.2f}\",\n            })\n\n        table += markdown_table(candidate_rows).set_params(row_sep=\"markdown\", quote=False).get_markdown()\n        table += \"\\n&lt;/details&gt;\\n\"\n\n    return table\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.get_defines_list","title":"<code>get_defines_list()</code>","text":"<p>Return a list of preprocessor define combinations for HLSL compilation.</p> <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>list[dict[str, str]]: A list of dictionaries containing preprocessor defines.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def get_defines_list() -&gt; list[dict[str, str]]:\n    \"\"\"Return a list of preprocessor define combinations for HLSL compilation.\n\n    Returns:\n        list[dict[str, str]]: A list of dictionaries containing preprocessor defines.\n    \"\"\"\n    return [\n        {\"PSHADER\": \"\"},\n        {\"PSHADER\": \"\", \"VR\": \"\"},\n        {\"VSHADER\": \"\"},\n        {\"VSHADER\": \"\", \"VR\": \"\"},\n    ]\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.get_excluded_dirs","title":"<code>get_excluded_dirs(cwd)</code>","text":"<p>Get directories to exclude based on .gitignore and default exclusions.</p> <p>Parameters:</p> Name Type Description Default <code>cwd</code> <code>str</code> <p>Current working directory.</p> required <p>Returns:</p> Type Description <code>set[str]</code> <p>set[str]: Set of directory names to exclude.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def get_excluded_dirs(cwd: str) -&gt; set[str]:\n    \"\"\"Get directories to exclude based on .gitignore and default exclusions.\n\n    Args:\n        cwd: Current working directory.\n\n    Returns:\n        set[str]: Set of directory names to exclude.\n    \"\"\"\n    default_exclusions = {\"build\", \"extern\", \"tools\", \"include\"}\n    excluded_dirs = set(default_exclusions)\n\n    gitignore_path = os.path.join(cwd, \".gitignore\")\n    if not os.path.isfile(gitignore_path) or pathspec is None:\n        logging.warning(f\".gitignore not found or pathspec not available, using default exclusions: {excluded_dirs}\")\n        return excluded_dirs\n\n    try:\n        with open(gitignore_path) as file:\n            spec = pathspec.PathSpec.from_lines(\"gitwildmatch\", file)\n        for pattern in spec.patterns:\n            # Extract directory names from patterns\n            pattern_str = str(pattern)\n            if pattern_str.endswith(\"/\"):\n                dir_name = pattern_str.rstrip(\"/\").split(\"/\")[-1]\n                if dir_name:\n                    excluded_dirs.add(dir_name)\n    except Exception as e:\n        logging.warning(f\"Failed to parse .gitignore: {e}. Using default exclusions: {excluded_dirs}\")\n\n    return excluded_dirs\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.get_field_similarity","title":"<code>get_field_similarity(cpp_field, hlsl_field)</code>","text":"<p>Enhanced field similarity that handles array type equivalence.</p> <p>Parameters:</p> Name Type Description Default <code>cpp_field</code> <code>dict</code> <p>C++ field metadata containing name, type and size.</p> required <code>hlsl_field</code> <code>dict</code> <p>HLSL field metadata containing name, type and size.</p> required <p>Returns:</p> Type Description <code>tuple[float, float, bool]</code> <p>tuple[float, float, bool]: A tuple containing: - type_sim: Type similarity (0.0 to 1.0) - name_sim: Name similarity score (0.0 to 1.0) - size_match: Whether field sizes match exactly</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def get_field_similarity(cpp_field: dict, hlsl_field: dict) -&gt; tuple[float, float, bool]:\n    \"\"\"Enhanced field similarity that handles array type equivalence.\n\n    Args:\n        cpp_field: C++ field metadata containing name, type and size.\n        hlsl_field: HLSL field metadata containing name, type and size.\n\n    Returns:\n        tuple[float, float, bool]: A tuple containing:\n            - type_sim: Type similarity (0.0 to 1.0)\n            - name_sim: Name similarity score (0.0 to 1.0)\n            - size_match: Whether field sizes match exactly\n    \"\"\"\n    # Use compute_name_similarity which handles array notation internally\n    name_sim = compute_name_similarity(hlsl_field[\"name\"], cpp_field[\"name\"])\n\n    # Normalize types for comparison\n    hlsl_norm, cpp_norm = normalize_array_types(hlsl_field[\"type\"], cpp_field[\"type\"])\n\n    # Check for exact type match after normalization\n    if hlsl_norm == cpp_norm:\n        type_sim = 1.0\n    else:\n        # Fall back to string similarity for types that don't normalize\n        type_sim = jellyfish.jaro_winkler_similarity(hlsl_field[\"type\"], cpp_field[\"type\"])\n        if type_sim &lt; 0.7:\n            type_sim = 0.0\n\n    # Direct size comparison\n    size_match = cpp_field[\"size\"] == hlsl_field[\"size\"]\n\n    return type_sim, name_sim, size_match\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.get_field_size","title":"<code>get_field_size(field_type, array_size=1)</code>","text":"<p>Calculate the size in bytes of a field based on its type and array size.</p> <p>Parameters:</p> Name Type Description Default <code>field_type</code> <code>str</code> <p>The type name (e.g., 'float4', 'XMFLOAT4X4[3]', or pointer types).</p> required <code>array_size</code> <code>int</code> <p>Optional additional array multiplier (default: 1).</p> <code>1</code> <p>Returns:</p> Type Description <code>int</code> <p>A tuple containing:</p> <code>bool</code> <ul> <li>The size in bytes</li> </ul> <code>tuple[int, bool]</code> <ul> <li>A flag indicating if the type is unknown (True) or recognized (False)</li> </ul> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def get_field_size(field_type: str, array_size: int = 1) -&gt; tuple[int, bool]:\n    \"\"\"Calculate the size in bytes of a field based on its type and array size.\n\n    Args:\n        field_type: The type name (e.g., 'float4', 'XMFLOAT4X4[3]', or pointer types).\n        array_size: Optional additional array multiplier (default: 1).\n\n    Returns:\n        A tuple containing:\n        - The size in bytes\n        - A flag indicating if the type is unknown (True) or recognized (False)\n    \"\"\"\n    base_type, parsed_array_size = parse_type_with_array(field_type)\n    array_size *= parsed_array_size\n\n    if base_type.endswith(\"*\"):\n        return POINTER_SIZE * array_size, False  # Assume 64-bit pointer\n\n    norm_type = normalize_field_type(base_type)\n\n    # Handle floatNxM matrix\n    if match := re.match(r\"float(\\d)x(\\d)\", norm_type):\n        rows, cols = int(match.group(1)), int(match.group(2))\n        return 4 * rows * cols * array_size, False  # 4 bytes per float\n\n    size = BASE_TYPE_SIZES.get(norm_type, DEFAULT_TYPE_SIZE)\n    is_unknown = size == DEFAULT_TYPE_SIZE and norm_type not in BASE_TYPE_SIZES\n    return size * array_size, is_unknown\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.get_hlsl_types","title":"<code>get_hlsl_types()</code>","text":"<p>Return a mapping of HLSL register types to their descriptions.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: A dictionary mapping register types to their descriptions.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def get_hlsl_types() -&gt; dict[str, str]:\n    \"\"\"Return a mapping of HLSL register types to their descriptions.\n\n    Returns:\n        dict[str, str]: A dictionary mapping register types to their descriptions.\n    \"\"\"\n    # https://learn.microsoft.com/en-us/windows/win32/direct3d12/resource-binding-in-hlsl\n    return {\"t\": \"SRV\", \"u\": \"UAV\", \"s\": \"Sampler\", \"b\": \"CBV\"}\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.get_struct_signature","title":"<code>get_struct_signature(fields)</code>","text":"<p>Generate a signature for a struct based on field names, types, and sizes.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list[dict]</code> <p>List of field dictionaries.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Struct signature string.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def get_struct_signature(fields: list[dict]) -&gt; str:\n    \"\"\"Generate a signature for a struct based on field names, types, and sizes.\n\n    Args:\n        fields (list[dict]): List of field dictionaries.\n\n    Returns:\n        str: Struct signature string.\n    \"\"\"\n    signature = []\n    for field in fields:\n        norm_type = normalize_field_type(field[\"type\"])\n        size = field[\"size\"]\n        signature.append(f\"{norm_type}:{field['name']}:{size}\")\n    sig_str = \";\".join(signature)\n    return sig_str\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.is_padding_field","title":"<code>is_padding_field(field)</code>","text":"<p>Check if a field is a padding field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>dict</code> <p>Field dictionary containing name and type.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the field is a padding field.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def is_padding_field(field: dict) -&gt; bool:\n    \"\"\"Check if a field is a padding field.\n\n    Args:\n        field: Field dictionary containing name and type.\n\n    Returns:\n        bool: True if the field is a padding field.\n    \"\"\"\n    name = field[\"name\"].lower()\n    # Remove array size from name for comparison\n    base_name = name.split(\"[\")[0] if \"[\" in name else name\n    return (\n        base_name == \"pad\"\n        or base_name.startswith(\"_pad\")\n        or base_name.startswith(\"pad\")\n        or base_name.endswith(\"pad\")\n        or base_name.startswith(\"padding\")\n        or base_name.endswith(\"padding\")\n        or base_name.startswith(\"_padding\")\n        or base_name.endswith(\"_padding\")\n    )\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.main","title":"<code>main()</code>","text":"<p>Main entry point for scanning HLSL shaders and generating a buffer table.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Main entry point for scanning HLSL shaders and generating a buffer table.\"\"\"\n    logging.basicConfig(level=logging.DEBUG)\n    parser = argparse.ArgumentParser(description=\"Scan HLSL/C++ buffers and print struct analysis.\")\n    parser.add_argument(\n        \"--only-matched\",\n        action=\"store_true\",\n        help=\"Only print buffers with matched or mismatched struct analysis (default: print all discovered buffers)\",\n    )\n    parser.add_argument(\n        \"--show-top-candidate\",\n        action=\"store_true\",\n        help=\"Show fields of the top candidate struct even if the match was rejected\",\n    )\n    parser.add_argument(\n        \"--show-conflicts\",\n        action=\"store_true\",\n        help=\"Show register conflicts analysis (default: disabled)\",\n    )\n    args = parser.parse_args()\n    cwd = os.getcwd()\n\n    scanner = FileScanner(cwd)\n    defines_list = get_defines_list()\n    shader_pattern = re.compile(\n        r\"\"\"(?P&lt;type&gt;\n                (?:cbuffer|ConstantBuffer&lt;(?P&lt;template_type&gt;\\w+)&gt;) |\n                (?:(?:RW)?(?:StructuredBuffer|Buffer|Texture1D|Texture2D|Texture3D|TextureCube|RWBuffer|RWTexture1D|RWTexture2D|RWTexture3D|RWTextureCube|SamplerState|SamplerComparisonState))\n                (?:&lt;(?P&lt;template_name&gt;\\w+)&gt;)?\n            )\n            \\s+\n            (?P&lt;name&gt;\\w+)\n            \\s*:\\s*register\\s*\\(\n                (?P&lt;buffer_type&gt;[a-z])\n                (?P&lt;buffer_number&gt;\\d+)\n            \\)\n            (?:\\s*;\\s*|$)\n        \"\"\",\n        re.MULTILINE | re.VERBOSE,\n    )\n    hlsl_types = {\"b\": \"CBV\", \"t\": \"SRV\", \"u\": \"UAV\", \"s\": \"Sampler\"}\n    pattern = re.compile(r\".*\\.(hlsl|hlsli)$\", re.IGNORECASE)\n    feature_pattern = re.compile(r\"features[/\\\\](?P&lt;feature&gt;[^/\\\\]+)\")\n\n    results, compilation_units = scanner.scan_for_buffers(\n        pattern=pattern,\n        feature_pattern=feature_pattern,\n        shader_pattern=shader_pattern,\n        hlsl_types=hlsl_types,\n        defines_list=defines_list,\n    )\n\n    result_map = {f\"{entry['File Path'].lower()}:{entry['Name'].lower()}\": entry for entry in results}\n    logging.debug(f\"Result map contains {len(result_map)} entries: {list(result_map.keys())}\")\n\n    hlsl_structs, cpp_structs = scanner.scan_for_structs()\n    analyzer = StructAnalyzer(hlsl_structs, cpp_structs)\n\n    for entry in result_map.values():\n        file = entry.get(\"File Path\")\n        buffer_name = entry[\"Name\"]\n        line = entry.get(\"Original Line\")\n        if file is not None and buffer_name is not None:\n            analyzer.add_buffer_location(file, buffer_name, line)\n\n    analyzer.compare_all_structs(result_map)\n    analyzer.update_result_map(result_map)\n\n    # Add all struct definitions to buffer_locations for printing\n    for hlsl_name, hlsl_struct_list in analyzer.hlsl_structs.items():\n        for hlsl_data in hlsl_struct_list:\n            if isinstance(hlsl_data, dict):\n                file = hlsl_data.get(\"file\", \"\")\n                line = hlsl_data.get(\"line\", 0)\n                if file:\n                    analyzer.add_buffer_location(file, hlsl_name, line)\n                    logging.debug(f\"Added struct definition to buffer_locations: {hlsl_name} from {file}:{line}\")\n                else:\n                    logging.debug(f\"Skipping struct {hlsl_name} - no file information\")\n    print_buffers_and_conflicts(result_map, compilation_units, show_conflicts=args.show_conflicts)\n    analyzer.print_comparison_tables(only_matched=args.only_matched, show_top_candidate=args.show_top_candidate)\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.normalize_array_types","title":"<code>normalize_array_types(hlsl_type, cpp_type)</code>","text":"<p>Normalize HLSL vector types and C++ array types for comparison.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def normalize_array_types(hlsl_type: str, cpp_type: str) -&gt; tuple[str, str]:\n    \"\"\"Normalize HLSL vector types and C++ array types for comparison.\"\"\"\n    # HLSL vector to C++ array mapping\n    hlsl_to_cpp = {\n        \"float2\": \"float[2]\",\n        \"float3\": \"float[3]\",\n        \"float4\": \"float[4]\",\n        \"int2\": \"int[2]\",\n        \"int3\": \"int[3]\",\n        \"int4\": \"int[4]\",\n        \"uint2\": \"uint[2]\",\n        \"uint3\": \"uint[3]\",\n        \"uint4\": \"uint[4]\",\n    }\n\n    # Normalize HLSL type to equivalent C++ array\n    normalized_hlsl = hlsl_to_cpp.get(hlsl_type, hlsl_type)\n\n    # Extract base type and size from C++ array notation\n    cpp_array_match = re.match(r\"(\\w+)\\[(\\d+)\\]\", cpp_type)\n    if cpp_array_match:\n        base_type, size = cpp_array_match.groups()\n        normalized_cpp = f\"{base_type}[{size}]\"\n    else:\n        normalized_cpp = cpp_type\n\n    return normalized_hlsl, normalized_cpp\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.normalize_field_type","title":"<code>normalize_field_type(field_type)</code>","text":"<p>Normalize a field type string to a canonical HLSL-like form (e.g., float4x4).</p> <p>Parameters:</p> Name Type Description Default <code>field_type</code> <code>str</code> <p>Original field type (e.g., 'REX::W32::XMFLOAT4X4', 'float3').</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Normalized type, e.g., 'float4x4' or 'float3'.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def normalize_field_type(field_type: str) -&gt; str:\n    \"\"\"\n    Normalize a field type string to a canonical HLSL-like form (e.g., float4x4).\n\n    Args:\n        field_type (str): Original field type (e.g., 'REX::W32::XMFLOAT4X4', 'float3').\n\n    Returns:\n        str: Normalized type, e.g., 'float4x4' or 'float3'.\n    \"\"\"\n    base_type, _ = parse_type_with_array(field_type)\n\n    # Handle XMFLOAT matrix types\n    if dims := extract_matrix_size(base_type):\n        if \"XM\" in base_type.upper():\n            return f\"xmfloat{dims[0]}x{dims[1]}\"\n        return f\"float{dims[0]}x{dims[1]}\"\n\n    # Strip namespaces and return last component\n    return base_type.split(\"::\")[-1].lower()\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.parse_type_with_array","title":"<code>parse_type_with_array(field_type)</code>","text":"<p>Parse field type and extract array size if present.</p> <p>Parameters:</p> Name Type Description Default <code>field_type</code> <code>str</code> <p>The type name, e.g., 'float4[4]'.</p> required <p>Returns:</p> Type Description <code>tuple[str, int]</code> <p>tuple[str, int]: A tuple containing the base type and array size (default 1).</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def parse_type_with_array(field_type: str) -&gt; tuple[str, int]:\n    \"\"\"\n    Parse field type and extract array size if present.\n\n    Args:\n        field_type (str): The type name, e.g., 'float4[4]'.\n\n    Returns:\n        tuple[str, int]: A tuple containing the base type and array size (default 1).\n    \"\"\"\n    match = re.match(r\"(.+?)\\[(\\d+)\\]$\", field_type)\n    if match:\n        return match.group(1), int(match.group(2))\n    return field_type, 1\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.preprocess_content","title":"<code>preprocess_content(content, defines)</code>","text":"<p>Preprocess HLSL content to include/exclude code based on defines.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The HLSL content to preprocess.</p> required <code>defines</code> <code>dict[str, str]</code> <p>Preprocessor defines to apply.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Preprocessed content with applied defines.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def preprocess_content(content: str, defines: dict[str, str]) -&gt; str:\n    \"\"\"Preprocess HLSL content to include/exclude code based on defines.\n\n    Args:\n        content (str): The HLSL content to preprocess.\n        defines (dict[str, str]): Preprocessor defines to apply.\n\n    Returns:\n        str: Preprocessed content with applied defines.\n    \"\"\"\n    lines = content.splitlines()\n    output = []\n    include = True\n    skip_depth = 0\n\n    for line in lines:\n        line = line.strip()\n        if line.startswith(\"#ifdef\") or line.startswith(\"#ifndef\"):\n            macro = line.split()[1]\n            is_ifdef = line.startswith(\"#ifdef\")\n            should_include = (macro in defines) if is_ifdef else (macro not in defines)\n            if not include:\n                skip_depth += 1\n            elif not should_include:\n                include = False\n                skip_depth += 1\n        elif line.startswith(\"#else\"):\n            if skip_depth == 0:\n                include = not include\n        elif line.startswith(\"#endif\"):\n            if skip_depth &gt; 0:\n                skip_depth -= 1\n            if skip_depth == 0:\n                include = True\n        elif include:\n            output.append(line)\n\n    return \"\\n\".join(output)\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.print_buffers_and_conflicts","title":"<code>print_buffers_and_conflicts(result_map, compilation_units, show_conflicts=False)</code>","text":"<p>Print the buffers and any conflicts as a markdown table and report.</p> <p>Parameters:</p> Name Type Description Default <code>result_map</code> <code>dict[str, dict[str, Any]]</code> <p>Dictionary of unique buffer entries.</p> required <code>compilation_units</code> <code>dict[tuple[str, frozenset[str]], dict[str, set[str]]]</code> <p>Dictionary of register usage per compilation unit.</p> required <code>show_conflicts</code> <code>bool</code> <p>Whether to print conflict analysis sections.</p> <code>False</code> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def print_buffers_and_conflicts(\n    result_map: dict[str, dict[str, Any]],\n    compilation_units: dict[tuple[str, frozenset[str]], dict[str, set[str]]],\n    show_conflicts: bool = False,\n) -&gt; None:\n    \"\"\"Print the buffers and any conflicts as a markdown table and report.\n\n    Args:\n        result_map: Dictionary of unique buffer entries.\n        compilation_units: Dictionary of register usage per compilation unit.\n        show_conflicts: Whether to print conflict analysis sections.\n    \"\"\"\n    if not result_map:\n        print(\"No results found.\")\n        return\n\n    sorted_results = sorted(\n        result_map.values(),\n        key=lambda x: (str(x.get(\"Register\", \"\")), str(x.get(\"Name\", \"\"))),\n    )\n    rows: list[dict[str, str]] = [\n        {\n            \"Register\": str(entry.get(\"Register\", \"\")),\n            \"Feature\": str(entry.get(\"Feature\", \"\")),\n            \"Type\": str(entry.get(\"Type\", \"\")),\n            \"Name\": str(entry.get(\"Name\", \"\")),\n            \"File\": str(entry.get(\"File\", \"\")),\n            \"Shaders\": _format_shader_usage(entry),\n            \"Struct Analysis\": str(entry.get(\"Matching Struct Analysis\", \"\")),\n        }\n        for entry in sorted_results\n    ]\n\n    # Filter out entries with blank Type, Name, or File\n    filtered_results = [\n        entry\n        for entry in rows\n        if entry.get(\"Type\", \"\").strip() and entry.get(\"Name\", \"\").strip() and entry.get(\"File\", \"\").strip()\n    ]\n\n    print(\"&lt;!--\")\n    print(\"DEBUG INFORMATION (hidden):\")\n    # Print any debug information that was collected during analysis\n    for debug_msg in DEBUG_INFO:\n        print(debug_msg)\n    print(\"--&gt;\")\n    print()\n    print(f\"# Buffer Table (generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\")\n    table = markdown_table(filtered_results).set_params(quote=False, row_sep=\"markdown\").get_markdown()\n    print(table)\n\n    if show_conflicts:\n        conflicts: dict[str, list[dict[str, Any]]] = {}\n        for (path, defines), registers in compilation_units.items():\n            for reg, features in registers.items():\n                if len(features) &gt; 1:\n                    if reg not in conflicts:\n                        conflicts[reg] = []\n                    conflicts[reg].append({\"path\": path, \"defines\": defines, \"features\": features})\n\n        if conflicts:\n            print(\"\\n# Conflicts\")\n            for reg, conflict_list in sorted(conflicts.items()):\n                print(f\"\\n## {reg}\")\n                for conflict in conflict_list:\n                    print(f\"- **Path**: {conflict['path']}\")\n                    print(f\"  **Defines**: {', '.join(sorted(str(d) for d in conflict['defines']))}\")\n                    print(f\"  **Features**: {', '.join(sorted(str(f) for f in conflict['features']))}\")\n\n        # Additional analysis: Check for register conflicts within same define context\n        register_conflicts = {}\n        for entry in sorted_results:\n            reg = entry.get(\"Register\", \"\")\n            if not reg:\n                continue\n\n            # Group by register and define combination\n            define_combos = entry.get(\"Define Combinations\", set())\n            for combo in define_combos:\n                key = f\"{reg}_{combo}\"\n                if key not in register_conflicts:\n                    register_conflicts[key] = []\n                register_conflicts[key].append({\n                    \"name\": entry.get(\"Name\", \"\"),\n                    \"file\": entry.get(\"File Path\", \"\"),\n                    \"feature\": entry.get(\"Feature\", \"\"),\n                    \"type\": entry.get(\"Type\", \"\"),\n                })\n\n        # Report register conflicts\n        true_conflicts = {k: v for k, v in register_conflicts.items() if len(v) &gt; 1}\n        if true_conflicts:\n            print(\"\\n# Register Conflicts (Same Register + Define Combination)\")\n            for key, conflicts in sorted(true_conflicts.items()):\n                reg, combo = key.rsplit(\"_\", 1)\n                print(f\"\\n## Register {reg} with defines: {combo.replace('_', ', ')}\")\n                for conflict in conflicts:\n                    print(\n                        f\"- **{conflict['name']}** ({conflict['type']}) in `{conflict['file']}` - Feature: {conflict['feature']}\"\n                    )\n        else:\n            print(\"\\n# Register Conflicts\")\n            print(\n                \"No register conflicts detected - all buffers use unique register slots within their define contexts.\"\n            )\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.process_file","title":"<code>process_file(path, cwd, defines, shader_pattern, hlsl_types, feature, short_path, result_map, compilation_units)</code>","text":"<p>Process a shader file to extract buffers and update result maps.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def process_file(\n    path: str,\n    cwd: str,\n    defines: dict[str, str],\n    shader_pattern: Pattern[str],\n    hlsl_types: dict[str, str],\n    feature: str,\n    short_path: str,\n    result_map: dict[str, dict[str, Any]],\n    compilation_units: dict[tuple[str, frozenset[str]], dict[str, set[str]]],\n) -&gt; None:\n    \"\"\"Process a shader file to extract buffers and update result maps.\"\"\"\n\n    def _should_skip_buffer(buffer_name, mapped_line, original_lines):\n        if buffer_name.lower() in BASE_TYPE_SIZES:\n            logging.debug(f\"Skipping buffer {buffer_name} (built-in type) in {path}:{mapped_line}\")\n            return True\n\n        if is_shader_io_struct(buffer_name):\n            logging.debug(f\"Skipping shader IO buffer: {buffer_name} in {path}:{mapped_line}\")\n            return True\n        if mapped_line is None:\n            logging.debug(f\"Skipping buffer {buffer_name} from include at preprocessed line {mapped_line}\")\n            return True\n        if mapped_line - 1 &gt;= len(original_lines):\n            return True\n        context_window = 5\n        start = max(0, mapped_line - context_window - 1)\n        end = min(len(original_lines), mapped_line + context_window)\n        buffer_decl_filter = re.compile(\n            rf\"\"\"\n(?:\\s*\n    (?:cbuffer|struct)\\s+{re.escape(buffer_name)}\\s*:\\s*register\\s*\\([butsg]\\d+\\)\n    |\n    (?:[\\w&lt;&gt;]+\\s+)?{re.escape(buffer_name)}\\s*:\\s*register\\s*\\([butsg]\\d+\\)\n)\n\"\"\",\n            re.IGNORECASE | re.VERBOSE,\n        )\n        for i in range(start, end):\n            line = original_lines[i]\n            if buffer_decl_filter.search(line):\n                return False\n        logging.debug(f\"Skipping buffer {buffer_name} in {path}:{mapped_line}\")\n        return True\n\n    path = os.path.normpath(path).replace(\"\\\\\", \"/\")\n    cwd = os.path.normpath(cwd).replace(\"\\\\\", \"/\")\n    if not path.startswith(cwd) or not os.path.isfile(path):\n        logging.error(f\"Skipping invalid or missing file: {path}\")\n        return\n\n    try:\n        with open(path, encoding=\"utf-8\", errors=\"ignore\") as file:\n            original_contents = file.read()\n\n        # Preprocess with pcpp, adding include paths\n        preprocessor = pcpp.Preprocessor()\n        include_dirs: list[str] = [\n            os.path.join(cwd, \"package\", \"Shaders\"),\n            os.path.dirname(path),\n        ]\n\n        common_dir = os.path.join(cwd, \"package\", \"Shaders\", \"Common\")\n        if os.path.isdir(common_dir):\n            for root, _, _ in os.walk(common_dir):\n                include_dirs.append(root)\n\n        features_dir = os.path.join(cwd, \"features\")\n        if os.path.isdir(features_dir):\n            for feature_dir in Path(features_dir).glob(\"*/Shaders\"):\n                include_dirs.append(str(feature_dir))\n\n        aio_shaders_dir = os.path.join(cwd, \"Shaders\")\n        if os.path.isdir(aio_shaders_dir) and \"aio\" in cwd.lower():\n            include_dirs.append(aio_shaders_dir)\n        include_dirs = list(set(include_dirs))  # remove duplicates\n\n        for inc_dir in include_dirs:\n            preprocessor.add_path(os.path.normpath(inc_dir))\n\n        for key, value in defines.items():\n            preprocessor.define(f\"{key} {value or '1'}\")\n\n        preprocessor.parse(original_contents, path)\n        with io.StringIO() as io_buffer:\n            preprocessor.write(io_buffer)\n            contents = io_buffer.getvalue()\n\n        # Build line map: preprocessed line -&gt; original line\n        line_map: dict[int, int] = {}\n        preprocessed_line = 0\n        current_file = path\n        current_line = 0\n        original_lines = original_contents.splitlines()\n        orig_line_idx = 0\n        for line in contents.splitlines():\n            line_match = re.match(r'^#line\\s+(\\d+)\\s+\"([^\"]*)\"', line)\n            if line_match:\n                current_line = int(line_match.group(1))\n                current_file = line_match.group(2).replace(\"\\\\\", \"/\")\n                orig_line_idx = min(current_line - 1, len(original_lines) - 1)\n                continue\n            if current_file == path:\n                line_map[preprocessed_line] = current_line\n                preprocessed_line += 1\n            if current_file == path and orig_line_idx &lt; len(original_lines):\n                if line.strip() == original_lines[orig_line_idx].strip():\n                    current_line = orig_line_idx + 1\n                orig_line_idx += 1\n            current_line += 1\n\n        if not line_map:\n            logging.debug(f\"Empty line map for {path}, assuming identity mapping\")\n            contents = original_contents\n            line_map = {i + 1: i + 1 for i in range(len(contents.splitlines()))}\n        logging.debug(f\"Preprocessing {path} with defines: {defines}\")\n\n        # Process buffers\n        capture_list: list[tuple[int, Match[str]]] = finditer_with_line_numbers(\n            shader_pattern, contents, line_map=line_map\n        )\n        for line_number, result in capture_list:\n            mapped_line = line_map.get(line_number)\n            buffer_name = result.group(\"name\")\n            if _should_skip_buffer(buffer_name, mapped_line, original_lines):\n                continue\n            path_with_line_no = f\"{short_path}:{mapped_line}\"\n            key = f\"{path_with_line_no}\"  # Consistent key format\n            entry = result_map.get(key)\n            if not entry:\n                # Extract template information from the full type match\n                full_type = result.group(\"type\")\n                template_type = \"\"\n\n                # Check for ConstantBuffer&lt;Type&gt; pattern\n                const_buffer_match = re.search(r\"ConstantBuffer&lt;(\\w+)&gt;\", full_type)\n                if const_buffer_match:\n                    template_type = const_buffer_match.group(1)\n                # Check for other template patterns like Texture2D&lt;half2&gt;\n                elif result.group(\"template_name\"):\n                    template_type = result.group(\"template_name\")\n\n                # Create a key for this define combination\n                define_key = \"_\".join(sorted(defines.keys())) if defines else \"no_defines\"\n\n                entry = {\n                    \"Register\": f\"{result.group('buffer_type').lower()}{result.group('buffer_number')}\",\n                    \"Feature\": feature,\n                    \"Type\": f\"`{full_type}`\",\n                    \"Name\": buffer_name,  # This should be the actual buffer variable name\n                    \"File\": f\"[{path_with_line_no}]({create_link(short_path, mapped_line)})\",\n                    \"File Path\": short_path,  # Store the file path separately\n                    \"Register Type\": hlsl_types.get(result.group(\"buffer_type\").lower(), \"Unknown\"),\n                    \"Buffer Type\": result.group(\"buffer_type\"),\n                    \"Number\": int(result.group(\"buffer_number\")),\n                    \"PSHADER\": False,\n                    \"VSHADER\": False,\n                    \"VR\": False,\n                    \"Matching Struct Analysis\": \"\",\n                    \"Original Line\": mapped_line,  # Store original line number,\n                    \"Template Type\": template_type,\n                    \"Define Combinations\": set(),  # Track define combinations\n                }\n                result_map[key] = entry\n            # Record this define combination with consistent ordering\n            if defines:\n                # Create consistent define key with standard ordering\n                define_parts = []\n                if \"PSHADER\" in defines:\n                    define_parts.append(\"PSHADER\")\n                if \"VSHADER\" in defines:\n                    define_parts.append(\"VSHADER\")\n                if \"VR\" in defines:\n                    define_parts.append(\"VR\")\n                define_key = \"_\".join(define_parts) if define_parts else \"no_defines\"\n            else:\n                define_key = \"no_defines\"\n\n            if \"Define Combinations\" not in entry:\n                entry[\"Define Combinations\"] = set()\n            entry[\"Define Combinations\"].add(define_key)\n\n            # Update boolean flags\n            for define in defines:\n                entry[define] = True\n            if \"PSHADER\" in defines:\n                entry[\"PSHADER\"] = True\n            if \"VSHADER\" in defines:\n                entry[\"VSHADER\"] = True\n            if \"VR\" in defines:\n                entry[\"VR\"] = True\n\n            compilation_unit_key = (short_path.lower(), frozenset(defines.keys()))\n            if compilation_unit_key not in compilation_units:\n                compilation_units[compilation_unit_key] = {}\n            reg = f\"{result.group('buffer_type').lower()}{result.group('buffer_number')}\"\n            if reg not in compilation_units[compilation_unit_key]:\n                compilation_units[compilation_unit_key][reg] = set()\n            compilation_units[compilation_unit_key][reg].add(feature)\n\n    except Exception:\n        logging.exception(\"Failed to process file %s\", path)\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.scan_files","title":"<code>scan_files(cwd, pattern, feature_pattern, shader_pattern, hlsl_types, defines_list)</code>","text":"<p>Scan HLSL files for buffer definitions and track compilation units.</p> <p>Parameters:</p> Name Type Description Default <code>cwd</code> <code>str</code> <p>The current working directory.</p> required <code>pattern</code> <code>Pattern[str]</code> <p>Compiled regex pattern to match HLSL files.</p> required <code>feature_pattern</code> <code>Pattern[str]</code> <p>Compiled regex pattern to extract feature names.</p> required <code>shader_pattern</code> <code>Pattern[str]</code> <p>Compiled regex pattern for buffer definitions.</p> required <code>hlsl_types</code> <code>dict[str, str]</code> <p>Mapping of HLSL register types to descriptions.</p> required <code>defines_list</code> <code>list[dict[str, str]]</code> <p>List of preprocessor define dictionaries.</p> required <p>Returns:</p> Type Description <code>tuple[list[dict[str, Any]], CompilationUnits]</code> <p>tuple[list[dict[str, Any]], CompilationUnits]: A tuple containing the list of buffer entries and compilation units.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def scan_files(\n    cwd: str,\n    pattern: Pattern[str],\n    feature_pattern: Pattern[str],\n    shader_pattern: Pattern[str],\n    hlsl_types: dict[str, str],\n    defines_list: list[dict[str, str]],\n) -&gt; tuple[list[dict[str, Any]], CompilationUnits]:\n    \"\"\"Scan HLSL files for buffer definitions and track compilation units.\n\n    Args:\n        cwd: The current working directory.\n        pattern: Compiled regex pattern to match HLSL files.\n        feature_pattern: Compiled regex pattern to extract feature names.\n        shader_pattern: Compiled regex pattern for buffer definitions.\n        hlsl_types: Mapping of HLSL register types to descriptions.\n        defines_list: List of preprocessor define dictionaries.\n\n    Returns:\n        tuple[list[dict[str, Any]], CompilationUnits]: A tuple containing the list of buffer entries and compilation units.\n    \"\"\"\n    result_map = {}\n    compilation_units = {}\n    excluded_dirs = get_excluded_dirs(cwd)\n    for root, dirs, files in os.walk(cwd):\n        dirs[:] = [d for d in dirs if d not in excluded_dirs]\n        feature = \"\"\n        root_normalized = root.replace(\"\\\\\", \"/\")\n        feature_match = feature_pattern.search(root_normalized)\n        if feature_match:\n            feature = feature_match.group(\"feature\")\n        for file in files:\n            if file.lower().endswith((\".hlsl\", \".hlsli\")):\n                full_path = os.path.join(root, file).replace(\"\\\\\", \"/\")\n                short_path_start = full_path.lower().find(\"skyrim-community-shaders\")\n                short_path = (\n                    full_path[short_path_start + len(\"skyrim-community-shaders\") + 1 :]\n                    if short_path_start != -1\n                    else os.path.relpath(full_path, cwd)\n                )\n                logging.debug(f\"Processing file: {full_path}\")\n                for defines in defines_list:\n                    process_file(\n                        full_path,\n                        cwd,\n                        defines,\n                        shader_pattern,\n                        hlsl_types,\n                        feature,\n                        short_path,\n                        result_map,\n                        compilation_units,\n                    )\n    results = list(result_map.values())\n    logging.debug(f\"Scan found {len(results)} buffers\")\n    return results, compilation_units\n</code></pre>"},{"location":"modules/#hlslkit.buffer_scan.strip_array_notation","title":"<code>strip_array_notation(name)</code>","text":"<p>Remove any array notation from a field name, e.g., 'pad[3]' -&gt; 'pad'.</p> Source code in <code>hlslkit/buffer_scan.py</code> <pre><code>def strip_array_notation(name: str) -&gt; str:\n    \"\"\"Remove any array notation from a field name, e.g., 'pad[3]' -&gt; 'pad'.\"\"\"\n    return re.sub(r\"\\[.*?\\]$\", \"\", name)\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.ErrorHandler","title":"<code>ErrorHandler</code>","text":"<p>               Bases: <code>IssueHandler</code></p> <p>Handler for compilation errors.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>class ErrorHandler(IssueHandler):\n    \"\"\"Handler for compilation errors.\"\"\"\n\n    def process(self, line: str, errors: dict) -&gt; dict:\n        \"\"\"Process an error line.\"\"\"\n        error_match = re.match(ERROR_REGEX, line)\n        if not error_match:\n            return errors\n\n        file_path, line_info, error_code, error_msg = error_match.groups()\n        location = self.normalize_location(file_path, line_info)\n\n        if self.shader_key_lower not in errors:\n            errors[self.shader_key_lower] = {\"instances\": {}, \"entries\": [], \"type\": self.context[\"shader_type\"]}\n\n        error_data = self.create_issue_data(error_code, error_msg, location)\n        self.add_to_instances(errors[self.shader_key_lower][\"instances\"], location, error_data)\n\n        if self.context[\"entry_point\"] not in errors[self.shader_key_lower][\"entries\"]:\n            errors[self.shader_key_lower][\"entries\"].append(self.context[\"entry_point\"])\n\n        return errors\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.ErrorHandler.process","title":"<code>process(line, errors)</code>","text":"<p>Process an error line.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def process(self, line: str, errors: dict) -&gt; dict:\n    \"\"\"Process an error line.\"\"\"\n    error_match = re.match(ERROR_REGEX, line)\n    if not error_match:\n        return errors\n\n    file_path, line_info, error_code, error_msg = error_match.groups()\n    location = self.normalize_location(file_path, line_info)\n\n    if self.shader_key_lower not in errors:\n        errors[self.shader_key_lower] = {\"instances\": {}, \"entries\": [], \"type\": self.context[\"shader_type\"]}\n\n    error_data = self.create_issue_data(error_code, error_msg, location)\n    self.add_to_instances(errors[self.shader_key_lower][\"instances\"], location, error_data)\n\n    if self.context[\"entry_point\"] not in errors[self.shader_key_lower][\"entries\"]:\n        errors[self.shader_key_lower][\"entries\"].append(self.context[\"entry_point\"])\n\n    return errors\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.IssueHandler","title":"<code>IssueHandler</code>","text":"<p>Base class for handling compilation issues (warnings and errors).</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>class IssueHandler:\n    \"\"\"Base class for handling compilation issues (warnings and errors).\"\"\"\n\n    def __init__(self, result: dict):\n        self.result = result\n        self.file_name = os.path.basename(result[\"file\"])\n        self.shader_key = f\"{self.file_name}:{result['entry']}\"\n        # Store both original and lowercase versions for lookups\n        self.shader_key_lower = self.shader_key.lower()\n        self.context = {\"shader_type\": result[\"type\"], \"entry_point\": result[\"entry\"]}\n\n    def normalize_location(self, file_path: str, line_info: str) -&gt; str:\n        \"\"\"Normalize file path and create location string.\"\"\"\n        norm_file_path = normalize_path(file_path)\n        return f\"{norm_file_path}:{line_info}\"\n\n    def create_issue_data(self, code: str, message: str, location: str) -&gt; dict:\n        \"\"\"Create a standardized issue data structure.\"\"\"\n        return {\"code\": code, \"message\": message, \"location\": location, \"context\": self.context.copy()}\n\n    def add_to_instances(self, instances: dict, location: str, issue_data: dict) -&gt; None:\n        \"\"\"Add an issue to the instances dictionary.\"\"\"\n        if location not in instances:\n            instances[location] = []\n        if not any(\n            i[\"code\"] == issue_data[\"code\"] and i[\"message\"] == issue_data[\"message\"] for i in instances[location]\n        ):\n            instances[location].append(issue_data)\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.IssueHandler.add_to_instances","title":"<code>add_to_instances(instances, location, issue_data)</code>","text":"<p>Add an issue to the instances dictionary.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def add_to_instances(self, instances: dict, location: str, issue_data: dict) -&gt; None:\n    \"\"\"Add an issue to the instances dictionary.\"\"\"\n    if location not in instances:\n        instances[location] = []\n    if not any(\n        i[\"code\"] == issue_data[\"code\"] and i[\"message\"] == issue_data[\"message\"] for i in instances[location]\n    ):\n        instances[location].append(issue_data)\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.IssueHandler.create_issue_data","title":"<code>create_issue_data(code, message, location)</code>","text":"<p>Create a standardized issue data structure.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def create_issue_data(self, code: str, message: str, location: str) -&gt; dict:\n    \"\"\"Create a standardized issue data structure.\"\"\"\n    return {\"code\": code, \"message\": message, \"location\": location, \"context\": self.context.copy()}\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.IssueHandler.normalize_location","title":"<code>normalize_location(file_path, line_info)</code>","text":"<p>Normalize file path and create location string.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def normalize_location(self, file_path: str, line_info: str) -&gt; str:\n    \"\"\"Normalize file path and create location string.\"\"\"\n    norm_file_path = normalize_path(file_path)\n    return f\"{norm_file_path}:{line_info}\"\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.WarningHandler","title":"<code>WarningHandler</code>","text":"<p>               Bases: <code>IssueHandler</code></p> <p>Handler for compilation warnings.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>class WarningHandler(IssueHandler):\n    \"\"\"Handler for compilation warnings.\"\"\"\n\n    def process(\n        self,\n        line: str,\n        baseline_warnings: dict,\n        suppress_warnings: list[str],\n        all_warnings: dict,\n        new_warnings_dict: dict,\n        suppressed_count: int,\n    ) -&gt; tuple[dict, dict, int]:\n        warning_match = re.match(WARNING_REGEX, line)\n        if not warning_match:\n            return all_warnings, new_warnings_dict, suppressed_count\n\n        file_path, line_info, warning_code, warning_msg = warning_match.groups()\n        location = self.normalize_location(file_path, line_info)\n        warning_key = f\"{warning_code}:{warning_msg}\".lower()\n\n        if warning_code.lower() in suppress_warnings:\n            suppressed_count += 1\n            logging.debug(f\"Suppressed warning: {warning_code} at {location}\")\n            return all_warnings, new_warnings_dict, suppressed_count\n\n        # Always use dict format for all_warnings\n        if warning_key not in all_warnings or not isinstance(all_warnings[warning_key], dict):\n            all_warnings[warning_key] = {\"code\": warning_code, \"message\": warning_msg, \"instances\": {}}\n        if location not in all_warnings[warning_key][\"instances\"]:\n            all_warnings[warning_key][\"instances\"][location] = {\"entries\": []}\n        if self.context[\"entry_point\"] not in all_warnings[warning_key][\"instances\"][location][\"entries\"]:\n            all_warnings[warning_key][\"instances\"][location][\"entries\"].append(self.context[\"entry_point\"])\n\n        # Check if this is a new warning\n        is_new_warning = True\n        if warning_key in baseline_warnings:\n            baseline_data = baseline_warnings[warning_key]\n            instances = baseline_data.get(\"instances\", {})\n            if isinstance(instances, dict):\n                baseline_count = sum(\n                    1\n                    for _loc, _inst in instances.items()\n                    if self.context[\"entry_point\"].lower() in [e.lower() for e in _inst.get(\"entries\", [])]\n                )\n            else:  # legacy list format - count every occurrence\n                baseline_count = len(instances)\n                # Convert all_warnings to dict format if needed\n                if not isinstance(all_warnings[warning_key], dict):\n                    all_warnings[warning_key] = {\"code\": warning_code, \"message\": warning_msg, \"instances\": {}}\n                for loc in instances:\n                    if loc not in all_warnings[warning_key][\"instances\"]:\n                        all_warnings[warning_key][\"instances\"][loc] = {\"entries\": []}\n                    if self.context[\"entry_point\"] not in all_warnings[warning_key][\"instances\"][loc][\"entries\"]:\n                        all_warnings[warning_key][\"instances\"][loc][\"entries\"].append(self.context[\"entry_point\"])\n            current_instances = all_warnings[warning_key][\"instances\"]\n            current_count = sum(\n                1\n                for _loc, _inst in current_instances.items()\n                if self.context[\"entry_point\"].lower() in [e.lower() for e in _inst.get(\"entries\", [])]\n            )\n            is_new_warning = current_count &gt; baseline_count\n\n        if is_new_warning:\n            warning_str = f\"{self.shader_key}:{warning_code}: {warning_msg} ({location})\"\n            context_warning_key = (\n                f\"{warning_key}:{self.context['shader_type']}:{self.context['entry_point']}:{location.lower()}\"\n            )\n            if context_warning_key not in new_warnings_dict:\n                new_warnings_dict[context_warning_key] = {\n                    \"warning_key\": warning_key,\n                    \"location\": location,\n                    \"code\": warning_code,\n                    \"message\": warning_msg,\n                    \"example\": warning_str,\n                    \"entries\": [],\n                    \"instances\": {},\n                }\n            if self.context[\"entry_point\"] not in new_warnings_dict[context_warning_key][\"entries\"]:\n                new_warnings_dict[context_warning_key][\"entries\"].append(self.context[\"entry_point\"])\n            if location not in new_warnings_dict[context_warning_key][\"instances\"]:\n                new_warnings_dict[context_warning_key][\"instances\"][location] = {\"entries\": []}\n            if (\n                self.context[\"entry_point\"]\n                not in new_warnings_dict[context_warning_key][\"instances\"][location][\"entries\"]\n            ):\n                new_warnings_dict[context_warning_key][\"instances\"][location][\"entries\"].append(\n                    self.context[\"entry_point\"]\n                )\n\n        return all_warnings, new_warnings_dict, suppressed_count\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.adjust_target_jobs","title":"<code>adjust_target_jobs(target_jobs, cpu_count, physical_cores, is_ci, cpu_usages, completed_tasks)</code>","text":"<p>Adjust the number of parallel jobs based on system resource usage.</p> <p>Parameters:</p> Name Type Description Default <code>target_jobs</code> <code>int</code> <p>Current number of jobs.</p> required <code>cpu_count</code> <code>int</code> <p>Total CPU cores.</p> required <code>physical_cores</code> <code>int | None</code> <p>Physical CPU cores.</p> required <code>is_ci</code> <code>bool</code> <p>Whether running in a CI environment.</p> required <code>cpu_usages</code> <code>deque</code> <p>Recent CPU usage measurements.</p> required <code>completed_tasks</code> <code>int</code> <p>Number of completed tasks.</p> required <p>Returns:</p> Type Description <code>tuple[int, str]</code> <p>tuple[int, str]: Adjusted number of jobs and reason.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def adjust_target_jobs(\n    target_jobs: int,\n    cpu_count: int,\n    physical_cores: int | None,\n    is_ci: bool,\n    cpu_usages: deque,\n    completed_tasks: int,\n) -&gt; tuple[int, str]:\n    \"\"\"Adjust the number of parallel jobs based on system resource usage.\n\n    Args:\n        target_jobs (int): Current number of jobs.\n        cpu_count (int): Total CPU cores.\n        physical_cores (int | None): Physical CPU cores.\n        is_ci (bool): Whether running in a CI environment.\n        cpu_usages (deque): Recent CPU usage measurements.\n        completed_tasks (int): Number of completed tasks.\n\n    Returns:\n        tuple[int, str]: Adjusted number of jobs and reason.\n    \"\"\"\n    if completed_tasks &lt; 20:\n        max_jobs = min(physical_cores or 24, cpu_count - 2) if physical_cores else min(cpu_count - 2, 24)\n        return max_jobs, \"initial max jobs for first 20 tasks\"\n\n    try:\n        cpu_usage = psutil.cpu_percent(interval=0.5)\n        mem_usage = psutil.virtual_memory().percent\n        cpu_usages.append(cpu_usage)\n        avg_cpu = sum(cpu_usages) / len(cpu_usages) if cpu_usages else cpu_usage\n        new_jobs, reason = get_system_adaptive_jobs(cpu_count, physical_cores, is_ci, avg_cpu)\n        if abs(new_jobs - target_jobs) &gt; 2:\n            fxc_count = count_fxc_processes()\n            logging.info(\n                f\"Adjusting active jobs to {new_jobs} ({reason}, avg CPU {avg_cpu:.1f}%, memory {mem_usage:.1f}%, fxc_processes={fxc_count})\"\n            )\n            return new_jobs, reason\n    except Exception as e:\n        logging.debug(f\"Failed to check system usage: {e}\")\n    return target_jobs, \"no adjustment\"\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.analyze_and_report_results","title":"<code>analyze_and_report_results(results, config_file, output_dir, suppress_warnings, max_warnings)</code>","text":"<p>Analyze compilation results and report warnings and errors.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[dict]</code> <p>Compilation results.</p> required <code>config_file</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <code>output_dir</code> <code>str</code> <p>Output directory for logs.</p> required <code>suppress_warnings</code> <code>list[str]</code> <p>Warning codes to suppress.</p> required <code>max_warnings</code> <code>int</code> <p>Maximum allowed new warnings.</p> required <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>tuple[int, int, int]: Exit code, total new warnings, and error count.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def analyze_and_report_results(\n    results: list[dict],\n    config_file: str,\n    output_dir: str,\n    suppress_warnings: list[str],\n    max_warnings: int,\n) -&gt; tuple[int, int, int]:\n    \"\"\"Analyze compilation results and report warnings and errors.\n\n    Args:\n        results (list[dict]): Compilation results.\n        config_file (str): Path to the YAML configuration file.\n        output_dir (str): Output directory for logs.\n        suppress_warnings (list[str]): Warning codes to suppress.\n        max_warnings (int): Maximum allowed new warnings.\n\n    Returns:\n        tuple[int, int, int]: Exit code, total new warnings, and error count.\n    \"\"\"\n    baseline_warnings = load_baseline_warnings(config_file)\n    defines_lookup = build_defines_lookup(config_file)\n    new_warnings, all_warnings, errors, suppressed_warnings_count = process_warnings_and_errors(\n        results, baseline_warnings, suppress_warnings, defines_lookup\n    )\n    log_new_issues(new_warnings, errors, results, output_dir, defines_lookup)\n\n    total_new_warnings = sum(get_instance_count(w) for w in new_warnings)\n    logging.info(\n        f\"Compilation complete: {total_new_warnings} new warnings, {suppressed_warnings_count} suppressed warnings, {len(errors)} errors\"\n    )\n\n    # Generate and log file-level summary\n    file_summary = get_file_issue_summary(baseline_warnings, new_warnings)\n    if file_summary:\n        logging.warning(\"\\n*** FILE-LEVEL ISSUE SUMMARY:\")\n        for file_path, stats in sorted(file_summary.items(), key=lambda x: x[1][\"new\"], reverse=True):\n            logging.warning(\n                f\"  {file_path}: {stats['new']} new issues \" f\"(baseline: {stats['baseline']}, total: {stats['total']})\"\n            )\n        logging.warning(\"\")\n\n    if new_warnings:\n        unique_warnings = sorted(new_warnings, key=lambda x: get_instance_count(x), reverse=True)\n        max_to_show = min(10, len(unique_warnings))\n        logging.warning(f\"*** NEW WARNINGS DETECTED ({len(unique_warnings)} unique, {total_new_warnings} total):\")\n        for i, warning in enumerate(unique_warnings[:max_to_show], 1):\n            count = get_instance_count(warning)\n            affected_variants = len(warning[\"entries\"])\n            percentage = (count / total_new_warnings * 100) if total_new_warnings &gt; 0 else 0\n\n            logging.warning(f\"\\n{i}. {warning['code']}: {warning['message']}\")\n            logging.warning(\n                f\"   Impact: {count} occurrences across {affected_variants} shader variants ({percentage:.1f}%)\"\n            )\n            locations_shown = 0\n            max_locations = 3\n\n            if isinstance(warning[\"instances\"], dict):\n                for location, location_data in warning[\"instances\"].items():\n                    if locations_shown &gt;= max_locations:\n                        remaining_locations = len(warning[\"instances\"]) - locations_shown\n                        logging.warning(\n                            f\"   Location: ...and {remaining_locations} more locations (see new_issues.log)\"\n                        )\n                        break\n\n                    file_part = location.split(\":\")[0] if \":\" in location else location\n                    line_part = \":\".join(location.split(\":\")[1:]) if \":\" in location else \"unknown\"\n\n                    entries_list = list(location_data[\"entries\"])[:2]\n                    entries_str = \", \".join(entries_list)\n                    if len(location_data[\"entries\"]) &gt; 2:\n                        entries_str += f\" (+{len(location_data['entries']) - 2} more)\"\n\n                    logging.warning(f\"   Location: {file_part}:{line_part} (entries: {entries_str})\")\n                    locations_shown += 1\n            else:\n                for location in warning[\"instances\"][:max_locations]:\n                    file_part = location.split(\":\")[0] if \":\" in location else location\n                    line_part = \":\".join(location.split(\":\")[1:]) if \":\" in location else \"unknown\"\n                    logging.warning(f\"   Location: {file_part}:{line_part}\")\n                    locations_shown += 1\n\n                if len(warning[\"instances\"]) &gt; max_locations:\n                    remaining_locations = len(warning[\"instances\"]) - max_locations\n                    logging.warning(f\"   Location: ...and {remaining_locations} more locations (see new_issues.log)\")\n\n        if len(unique_warnings) &gt; max_to_show:\n            remaining_count = sum(get_instance_count(w) for w in unique_warnings[max_to_show:])\n            logging.warning(\n                f\"\\n...and {len(unique_warnings) - max_to_show} more unique warnings ({remaining_count} occurrences)\"\n            )\n            logging.warning(\"See 'new_issues.log' for complete details and compilation context\")\n\n    error_count = sum(len(e) for e in errors.values())\n    if errors:\n        logging.error(f\"*** COMPILATION ERRORS DETECTED ({error_count} total errors):\")\n\n        for shader_key, error_data in errors.items():\n            shader_file = normalize_path(shader_key.split(\":\")[0])\n            entry_point = \":\".join(shader_key.split(\":\")[1:]) if \":\" in shader_key else \"unknown\"\n            shader_type = error_data.get(\"type\", \"unknown\")\n\n            logging.error(f\"\\nFile: {shader_file} (entry: {entry_point}, type: {shader_type}):\")\n            for error_list in error_data[\"instances\"].values():\n                for error in error_list:\n                    logging.error(f\"   ERROR {error['code']}: {error['message']}\")\n                    if error.get(\"location\"):\n                        logging.error(f\"      Location: {error['location']}\")\n                    if error.get(\"context\"):\n                        logging.error(\n                            f\"      Context: {error['context']['shader_type']} - {error['context']['entry_point']}\"\n                        )\n\n        logging.error(\"\\nACTION REQUIRED: Fix all compilation errors above before proceeding.\")\n        logging.error(\"Check the full compilation log for additional context and details.\")\n        return 1, total_new_warnings, error_count  # Enhanced warning threshold logic to support negative values\n    if max_warnings &lt; 0:\n        # Negative max_warnings means user must eliminate existing warnings\n        baseline_warning_count = sum(len(warning_data[\"instances\"]) for warning_data in baseline_warnings.values())\n        current_total_warnings = baseline_warning_count + total_new_warnings\n        required_reduction = abs(max_warnings)\n        # If required reduction exceeds baseline, target is 0 warnings (eliminate all)\n        target_warning_count = max(0, baseline_warning_count - required_reduction)\n\n        if current_total_warnings &gt; target_warning_count:\n            eliminated_warnings = baseline_warning_count - (current_total_warnings - total_new_warnings)\n            if target_warning_count == 0:\n                logging.error(\n                    f\"*** MUST ELIMINATE ALL WARNINGS: Required reduction {required_reduction} exceeds \"\n                    f\"baseline count {baseline_warning_count}. Current total: {current_total_warnings}, \"\n                    f\"target: {target_warning_count} warnings.\"\n                )\n                logging.error(\"ACTION REQUIRED: Eliminate ALL remaining warnings to pass this check.\")\n            else:\n                needed_elimination = required_reduction - eliminated_warnings\n                logging.error(\n                    f\"*** INSUFFICIENT WARNING REDUCTION: Need to eliminate {required_reduction} warnings, \"\n                    f\"but only eliminated {eliminated_warnings}. Need {needed_elimination} more eliminations.\"\n                )\n                logging.error(f\"ACTION REQUIRED: Eliminate {needed_elimination} more warnings to pass this check.\")\n\n            # Provide actionable guidance\n            if new_warnings:\n                logging.error(\n                    f\"TIP: Focus on eliminating existing warnings rather than adding {total_new_warnings} new ones.\"\n                )\n                logging.error(\"Check 'new_issues.log' for detailed locations and suggested fixes.\")\n            else:\n                logging.error(\"TIP: Eliminate more existing baseline warnings to meet the reduction requirement.\")\n\n            logging.info(\n                f\"SUMMARY: Baseline: {baseline_warning_count} | New: {total_new_warnings} | \"\n                f\"Target: \u2264{target_warning_count} | Current: {current_total_warnings}\"\n            )\n            # Show top existing warnings to help user prioritize elimination efforts\n            if baseline_warnings:\n                logging.error(\"\\n*** TOP EXISTING WARNINGS TO ELIMINATE (by impact):\")\n\n                # Calculate entry counts for baseline warnings and sort by impact\n                baseline_with_counts = []\n                for _warning_key, warning_data in baseline_warnings.items():\n                    total_entries = sum(\n                        len(location_data.get(\"entries\", [])) for location_data in warning_data[\"instances\"].values()\n                    )\n                    baseline_with_counts.append({\n                        \"code\": warning_data.get(\"code\", \"Unknown\"),\n                        \"message\": warning_data.get(\"message\", \"Unknown message\"),\n                        \"total_entries\": total_entries,\n                        \"instance_count\": len(warning_data[\"instances\"]),\n                        \"locations\": list(warning_data[\"instances\"].keys())[:3],  # Show first 3 locations\n                    })\n\n                # Sort by total entries (impact) descending\n                top_existing = sorted(baseline_with_counts, key=lambda x: x[\"total_entries\"], reverse=True)[:5]\n\n                # Calculate total entries across all baseline warnings for percentage calculation\n                total_baseline_entries = sum(w[\"total_entries\"] for w in baseline_with_counts)\n\n                for i, warning in enumerate(top_existing, 1):\n                    impact_percentage = (\n                        (warning[\"total_entries\"] / total_baseline_entries * 100) if total_baseline_entries &gt; 0 else 0\n                    )\n                    logging.error(f\"\\n{i}. {warning['code']}: {warning['message']}\")\n                    logging.error(\n                        f\"   Impact: {warning['total_entries']} shader combinations across {warning['instance_count']} locations ({impact_percentage:.1f}% of baseline)\"\n                    )\n                    logging.error(f\"   Sample locations: {', '.join(warning['locations'])}\")\n\n                logging.error(\n                    f\"\\nEliminating these top {len(top_existing)} warning types would reduce {sum(w['total_entries'] for w in top_existing)} shader combinations.\"\n                )\n                logging.error(\"Focus on the highest impact warnings first for maximum progress toward the target.\")\n\n            return 1, total_new_warnings, error_count\n\n        else:\n            eliminated_warnings = baseline_warning_count - (current_total_warnings - total_new_warnings)\n            if target_warning_count == 0:\n                logging.info(\n                    f\"All warnings eliminated: required reduction {required_reduction} exceeded \"\n                    f\"baseline count {baseline_warning_count}. Achieved zero warnings goal.\"\n                )\n\n            else:\n                logging.info(\n                    f\"Warning reduction goal met: eliminated {eliminated_warnings} warnings \"\n                    f\"(required: {required_reduction}), {total_new_warnings} new warnings\"\n                )\n    elif total_new_warnings &gt; max_warnings:\n        # Positive max_warnings means limit on new warnings (original behavior)\n        excess_warnings = total_new_warnings - max_warnings\n        logging.error(f\"*** TOO MANY NEW WARNINGS: {total_new_warnings} new warnings exceed limit of {max_warnings}\")\n        logging.error(f\"ACTION REQUIRED: Eliminate {excess_warnings} new warnings to pass this check.\")\n\n        if new_warnings:\n            logging.error(\"Check 'new_issues.log' for detailed locations and suggested fixes.\")\n            # Show the most common warnings first for prioritization\n            unique_warnings = sorted(new_warnings, key=lambda x: len(x[\"instances\"]), reverse=True)\n            logging.error(f\"TIP: Focus on the top {min(3, len(unique_warnings))} most frequent warning types:\")\n            for i, warning in enumerate(unique_warnings[:3], 1):\n                count = len(warning[\"instances\"])\n                logging.error(f\"   {i}. {warning['code']}: {warning['message']} ({count} occurrences)\")\n\n        return 1, total_new_warnings, error_count\n\n    return 0, total_new_warnings, error_count\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.build_defines_lookup","title":"<code>build_defines_lookup(config_file)</code>","text":"<p>Build a lookup table for shader defines from a YAML configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Lookup table mapping shader keys to (shader_type, defines).</p> Example <p>build_defines_lookup(\"shader_defines.yaml\") {'test.hlsl:main:1234': ('PSHADER', ['A=1'])}</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def build_defines_lookup(config_file: str) -&gt; dict:\n    \"\"\"Build a lookup table for shader defines from a YAML configuration.\n\n    Args:\n        config_file (str): Path to the YAML configuration file.\n\n    Returns:\n        dict: Lookup table mapping shader keys to (shader_type, defines).\n\n    Example:\n        &gt;&gt;&gt; build_defines_lookup(\"shader_defines.yaml\")\n        {'test.hlsl:main:1234': ('PSHADER', ['A=1'])}\n    \"\"\"\n    defines_lookup = {}\n    if config_file and os.path.exists(config_file):\n        try:\n            with open(config_file, encoding=\"utf-8\") as f:\n                config_data = yaml.safe_load(f)\n                if config_data and \"shaders\" in config_data:\n                    for shader in config_data[\"shaders\"]:\n                        file_name = shader[\"file\"]\n                        for shader_type, config in shader.get(\"configs\", {}).items():\n                            for entry in config.get(\"entries\", []):\n                                entry_name = entry[\"entry\"]\n                                defines = flatten_defines(config.get(\"common_defines\", []) + entry.get(\"defines\", []))\n                                defines_lookup[f\"{file_name}:{entry_name}\".lower()] = (\n                                    shader_type,\n                                    defines,\n                                )\n        except Exception as e:\n            logging.warning(f\"Failed to load shader configs from {config_file}: {e}\")\n    return defines_lookup\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.compile_shader","title":"<code>compile_shader(fxc_path, shader_file, shader_type, entry, defines, output_dir, shader_dir, debug=False, strip_debug_defines=False, optimization_level='1', force_partial_precision=False, debug_defines=None, extra_includes=None)</code>","text":"<p>Compile a shader using fxc.exe.</p> <p>Parameters:</p> Name Type Description Default <code>fxc_path</code> <code>str</code> <p>Path to fxc.exe.</p> required <code>shader_file</code> <code>str</code> <p>Path to the shader file.</p> required <code>shader_type</code> <code>str</code> <p>Type of shader (VSHADER, PSHADER, CSHADER).</p> required <code>entry</code> <code>str</code> <p>Entry point for the shader.</p> required <code>defines</code> <code>list[str]</code> <p>Preprocessor defines.</p> required <code>output_dir</code> <code>str</code> <p>Output directory for compiled shaders.</p> required <code>shader_dir</code> <code>str</code> <p>Directory containing shader files.</p> required <code>debug</code> <code>bool</code> <p>Enable debug logging.</p> <code>False</code> <code>strip_debug_defines</code> <code>bool</code> <p>Strip debug-related defines.</p> <code>False</code> <code>optimization_level</code> <code>str</code> <p>Optimization level (0-3).</p> <code>'1'</code> <code>force_partial_precision</code> <code>bool</code> <p>Force 16-bit precision.</p> <code>False</code> <code>debug_defines</code> <code>set[str] | None</code> <p>Set of debug defines to strip.</p> <code>None</code> <code>extra_includes</code> <code>list[str] | None</code> <p>List of additional include directories.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, object]</code> <p>dict[str, any]: Compilation result with file, entry, type, log, success, and command.</p> Example <p>compile_shader(\"fxc.exe\", \"test.hlsl\", \"PSHADER\", \"main:1234\", [], \"build\", \"src\") {'file': 'test.hlsl', 'entry': 'main:1234', 'type': 'PSHADER', 'log': '...', 'success': True, 'cmd': [...]}</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def compile_shader(\n    fxc_path: str,\n    shader_file: str,\n    shader_type: str,\n    entry: str,\n    defines: list[str],\n    output_dir: str,\n    shader_dir: str,\n    debug: bool = False,\n    strip_debug_defines: bool = False,\n    optimization_level: str = \"1\",\n    force_partial_precision: bool = False,\n    debug_defines: set[str] | None = None,\n    extra_includes: list[str] | None = None,\n) -&gt; dict[str, object]:\n    \"\"\"Compile a shader using fxc.exe.\n\n    Args:\n        fxc_path (str): Path to fxc.exe.\n        shader_file (str): Path to the shader file.\n        shader_type (str): Type of shader (VSHADER, PSHADER, CSHADER).\n        entry (str): Entry point for the shader.\n        defines (list[str]): Preprocessor defines.\n        output_dir (str): Output directory for compiled shaders.\n        shader_dir (str): Directory containing shader files.\n        debug (bool): Enable debug logging.\n        strip_debug_defines (bool): Strip debug-related defines.\n        optimization_level (str): Optimization level (0-3).\n        force_partial_precision (bool): Force 16-bit precision.\n        debug_defines (set[str] | None): Set of debug defines to strip.\n        extra_includes (list[str] | None): List of additional include directories.\n\n    Returns:\n        dict[str, any]: Compilation result with file, entry, type, log, success, and command.\n\n    Example:\n        &gt;&gt;&gt; compile_shader(\"fxc.exe\", \"test.hlsl\", \"PSHADER\", \"main:1234\", [], \"build\", \"src\")\n        {'file': 'test.hlsl', 'entry': 'main:1234', 'type': 'PSHADER', 'log': '...', 'success': True, 'cmd': [...]}\n    \"\"\"\n    if stop_event.is_set():\n        return {\n            \"file\": shader_file,\n            \"entry\": entry,\n            \"type\": shader_type,\n            \"log\": \"Compilation aborted.\",\n            \"success\": False,\n            \"cmd\": [],\n        }\n\n    validation_error = validate_shader_inputs(fxc_path, shader_file, output_dir, defines, shader_dir)\n    if validation_error:\n        logging.error(validation_error)\n        return {\n            \"file\": shader_file,\n            \"entry\": entry,\n            \"type\": shader_type,\n            \"log\": validation_error,\n            \"success\": False,\n            \"cmd\": [],\n        }\n\n    entry_name = \"main\"\n    shader_id = entry.split(\":\")[-1]\n    shader_basename = os.path.basename(shader_file)\n    shader_name_no_ext, _ = os.path.splitext(shader_basename)\n    output_subdir = os.path.join(output_dir, shader_name_no_ext)\n\n    ext_map = {\"VSHADER\": \".vso\", \"PSHADER\": \".pso\", \"CSHADER\": \".cso\"}\n    ext = ext_map.get(shader_type.upper())\n    if ext is None:\n        return {\n            \"file\": shader_file,\n            \"entry\": entry,\n            \"type\": shader_type,\n            \"log\": f\"Unsupported shader type: {shader_type}\",\n            \"success\": False,\n            \"cmd\": [],\n        }\n\n    os.makedirs(output_subdir, exist_ok=True)\n    output_path = os.path.join(output_subdir, shader_id + ext)\n\n    shader_model_map = {\"VSHADER\": \"vs_5_0\", \"PSHADER\": \"ps_5_0\", \"CSHADER\": \"cs_5_0\"}\n    model = shader_model_map[shader_type.upper()]\n\n    # Determine if shader_dir is a file or directory\n    if os.path.isfile(shader_dir):\n        shader_file_path = os.path.abspath(shader_file)\n    else:\n        shader_file_path = os.path.join(shader_dir, shader_basename)\n    if not os.path.exists(shader_file_path):\n        error_msg = f\"Shader file not found in {shader_dir}: {shader_basename}\"\n        logging.error(error_msg)\n        return {\n            \"file\": shader_file,\n            \"entry\": entry,\n            \"type\": shader_type,\n            \"log\": error_msg,\n            \"success\": False,\n            \"cmd\": [],\n        }\n\n    if debug_defines is None:\n        debug_defines = {\"DEBUG\", \"_DEBUG\", \"D3D_DEBUG_INFO\", \"D3DCOMPILE_DEBUG\", \"D3DCOMPILE_SKIP_OPTIMIZATION\"}\n    if strip_debug_defines:\n        # Work on a copy to avoid mutating the shared task definition\n        filtered_defines = [d for d in defines if d.split(\"=\", 1)[0].upper() not in debug_defines]\n        if \"D3DCOMPILE_AVOID_FLOW_CONTROL\" not in filtered_defines:\n            filtered_defines.append(\"D3DCOMPILE_AVOID_FLOW_CONTROL\")\n        defines = filtered_defines\n        logging.debug(\n            f\"Stripped debug defines and added D3DCOMPILE_AVOID_FLOW_CONTROL for {shader_file}:{entry}. Defines: {defines}\"\n        )\n\n    cmd = [\n        fxc_path,\n        \"/T\",\n        model,\n        \"/E\",\n        entry_name,\n        \"/Fo\",\n        output_path,\n        \"/O\" + optimization_level,\n        shader_basename,\n    ]\n    if force_partial_precision:\n        cmd.append(\"/Gfp\")\n    for d in defines:\n        cmd.extend([\"/D\", d])\n    # Always include shader_dir (if directory), or the file's parent (if file)\n    include_dirs = []\n    if os.path.isdir(shader_dir):\n        include_dirs.append(os.path.abspath(shader_dir))\n    # Always include the parent directory of the shader file\n    shader_parent = os.path.abspath(os.path.dirname(shader_file))\n    if shader_parent not in include_dirs:\n        include_dirs.append(shader_parent)\n    # Add extra includes if provided\n    if extra_includes:\n        for inc in extra_includes:\n            inc_path = os.path.abspath(inc)\n            if inc_path not in include_dirs:\n                include_dirs.append(inc_path)\n    for inc in include_dirs:\n        cmd.extend([\"/I\", inc])\n\n    log = \"\"\n    success = False\n    process = None\n\n    logging.debug(f\"Executing command: {' '.join(cmd)}\")\n    # Defines are sanitized in validate_shader_inputs to prevent injection\n    try:\n        process = subprocess.Popen(  # noqa: S603\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            cwd=os.path.abspath(shader_dir),\n        )\n        with running_processes_lock:\n            running_processes.add(process)\n        stdout, stderr = process.communicate()\n        log = stdout + stderr\n        success = process.returncode == 0\n    except Exception as e:\n        log = str(e)\n        success = False\n    finally:\n        with running_processes_lock:\n            if process in running_processes:\n                running_processes.remove(process)\n\n    if debug:\n        logging.debug(f\"Command {'failed' if not success else 'succeeded'}: {' '.join(cmd)}\")\n        logging.debug(f\"Output:\\n{log}\")\n\n    return {\n        \"file\": shader_file,\n        \"entry\": entry,\n        \"type\": shader_type,\n        \"log\": log,\n        \"success\": success,\n        \"cmd\": cmd,\n    }\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.count_fxc_processes","title":"<code>count_fxc_processes()</code>","text":"<p>Count the number of running fxc.exe processes.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of fxc.exe processes.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def count_fxc_processes() -&gt; int:\n    \"\"\"Count the number of running fxc.exe processes.\n\n    Returns:\n        int: Number of fxc.exe processes.\n    \"\"\"\n    if not HAS_PSUTIL:\n        return 0\n    count = 0\n    for proc in psutil.process_iter([\"name\"]):\n        try:\n            if proc.info[\"name\"].lower() == \"fxc.exe\":\n                count += 1\n        except (psutil.NoSuchProcess, psutil.AccessDenied):\n            pass\n    return count\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.flatten_defines","title":"<code>flatten_defines(defines)</code>","text":"<p>Flatten a list of defines, removing duplicates while preserving order.</p> <p>Parameters:</p> Name Type Description Default <code>defines</code> <code>list</code> <p>List of defines, possibly nested.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Flattened list of unique defines, sorted for deterministic output.</p> Example <p>flatten_defines([[\"A=1\", \"B\"], \"C\"]) ['A=1', 'B', 'C']</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def flatten_defines(defines: list) -&gt; list[str]:\n    \"\"\"Flatten a list of defines, removing duplicates while preserving order.\n\n    Args:\n        defines (list): List of defines, possibly nested.\n\n    Returns:\n        list[str]: Flattened list of unique defines, sorted for deterministic output.\n\n    Example:\n        &gt;&gt;&gt; flatten_defines([[\"A=1\", \"B\"], \"C\"])\n        ['A=1', 'B', 'C']\n    \"\"\"\n    flat = []\n    for d in defines:\n        if isinstance(d, list):\n            flat.extend(flatten_defines(d))\n        else:\n            flat.append(d)\n    seen = set()\n    result = [d for d in flat if d is not None and not (d in seen or seen.add(d))]\n    return sorted(result)\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.get_file_issue_summary","title":"<code>get_file_issue_summary(baseline_warnings, new_warnings)</code>","text":"<p>Generate a summary of issue changes per file, only counting truly new issues. Handles both dict and list formats for 'instances'.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def get_file_issue_summary(baseline_warnings: dict, new_warnings: list[dict]) -&gt; dict:\n    \"\"\"Generate a summary of issue changes per file, only counting truly new issues. Handles both dict and list formats for 'instances'.\"\"\"\n    file_summary = {}\n\n    # Build a set of (file_path, location, entry) for baseline\n    baseline_set = set()\n    for warning_data in baseline_warnings.values():\n        instances = warning_data.get(\"instances\", {})\n        if isinstance(instances, dict):\n            for _location, loc_data in instances.items():\n                file_path = _location.split(\":\")[0]\n                for _entry in loc_data.get(\"entries\", []):\n                    baseline_set.add((file_path, _location, _entry))\n        elif isinstance(instances, list):\n            for _location in instances:\n                file_path = _location.split(\":\")[0]\n                baseline_set.add((file_path, _location, None))\n\n    # Build a set of (file_path, location, entry) for new_warnings\n    new_set = set()\n    for warning in new_warnings:\n        instances = warning.get(\"instances\", {})\n        if isinstance(instances, dict):\n            for _location, loc_data in instances.items():\n                file_path = _location.split(\":\")[0]\n                for _entry in loc_data.get(\"entries\", []):\n                    new_set.add((file_path, _location, _entry))\n        elif isinstance(instances, list):\n            for _location in instances:\n                file_path = _location.split(\":\")[0]\n                # Use entries from warning['entries'] if available, else None\n                entries = warning.get(\"entries\", [None])\n                for _entry in entries:\n                    new_set.add((file_path, _location, _entry))\n\n    # Only count as new if not in baseline\n    truly_new = new_set - baseline_set\n\n    # Count baseline and new per file\n    for file_path, _location, _entry in baseline_set:\n        if file_path not in file_summary:\n            file_summary[file_path] = {\"baseline\": 0, \"new\": 0, \"total\": 0}\n        file_summary[file_path][\"baseline\"] += 1\n        file_summary[file_path][\"total\"] += 1\n    for file_path, _location, _entry in new_set:\n        if file_path not in file_summary:\n            file_summary[file_path] = {\"baseline\": 0, \"new\": 0, \"total\": 0}\n        file_summary[file_path][\"total\"] += 1\n    for file_path, _location, _entry in truly_new:\n        if file_path not in file_summary:\n            file_summary[file_path] = {\"baseline\": 0, \"new\": 0, \"total\": 0}\n        file_summary[file_path][\"new\"] += 1\n\n    # Only report files with truly new issues\n    return {k: v for k, v in file_summary.items() if v[\"new\"] &gt; 0}\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.get_instance_count","title":"<code>get_instance_count(warning)</code>","text":"<p>Get the count of instances, handling both list and dict formats.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def get_instance_count(warning: dict) -&gt; int:\n    \"\"\"Get the count of instances, handling both list and dict formats.\"\"\"\n    instances = warning.get(\"instances\", [])\n    if isinstance(instances, dict):\n        return len(instances)\n    else:\n        return len(instances)\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.get_system_adaptive_jobs","title":"<code>get_system_adaptive_jobs(cpu_count, physical_cores, is_ci, avg_cpu)</code>","text":"<p>Determine the optimal number of parallel jobs based on system resources.</p> <p>Parameters:</p> Name Type Description Default <code>cpu_count</code> <code>int</code> <p>Total CPU cores.</p> required <code>physical_cores</code> <code>int | None</code> <p>Physical CPU cores.</p> required <code>is_ci</code> <code>bool</code> <p>Whether running in a CI environment.</p> required <code>avg_cpu</code> <code>float</code> <p>Average CPU usage.</p> required <p>Returns:</p> Type Description <code>tuple[int, str]</code> <p>tuple[int, str]: Number of jobs and reason for selection.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def get_system_adaptive_jobs(\n    cpu_count: int, physical_cores: int | None, is_ci: bool, avg_cpu: float\n) -&gt; tuple[int, str]:\n    \"\"\"Determine the optimal number of parallel jobs based on system resources.\n\n    Args:\n        cpu_count (int): Total CPU cores.\n        physical_cores (int | None): Physical CPU cores.\n        is_ci (bool): Whether running in a CI environment.\n        avg_cpu (float): Average CPU usage.\n\n    Returns:\n        tuple[int, str]: Number of jobs and reason for selection.\n    \"\"\"\n    if is_ci:\n        # In CI environments, be more aggressive with job allocation since runners are dedicated\n        jobs = min(max(cpu_count - 1, 2), 4)\n        reason = \"auto-detected for CI environment (aggressive)\"\n        if HAS_PSUTIL:\n            try:\n                cpu_usage = psutil.cpu_percent(interval=0.5)\n                mem_usage = psutil.virtual_memory().percent\n                if cpu_usage &gt; 90 or mem_usage &gt; 95:\n                    jobs = min(max(cpu_count // 2, 2), 4)\n                    reason = (\n                        f\"auto-detected for CI, high CPU ({cpu_usage:.1f}%) or memory ({mem_usage:.1f}%) - conservative\"\n                    )\n                # For low load, keep the aggressive setting\n            except Exception as e:\n                logging.debug(f\"Failed to check system usage in CI: {e}\")\n        return jobs, reason\n\n    max_jobs = min(physical_cores or 24, cpu_count - 2) if physical_cores else min(cpu_count - 2, 24)\n    if HAS_PSUTIL:\n        try:\n            cpu_usage = psutil.cpu_percent(interval=0.5)\n            mem_usage = psutil.virtual_memory().percent\n            if cpu_usage &lt; 70 and mem_usage &lt; 90:\n                jobs = max_jobs\n                reason = f\"auto-detected, low CPU ({cpu_usage:.1f}%), low memory ({mem_usage:.1f}%)\"\n            elif cpu_usage &lt; 90 and mem_usage &lt; 95:\n                jobs = max(int(max_jobs * 0.85), 6)\n                reason = f\"auto-detected, moderate CPU ({cpu_usage:.1f}%), moderate memory ({mem_usage:.1f}%)\"\n            else:\n                jobs = max(int(max_jobs * 0.6), 6)\n                reason = f\"auto-detected, high CPU ({cpu_usage:.1f}%) or memory ({mem_usage:.1f}%)\"\n            job_levels = [6, 8, 12, 16, 20, 24]\n            jobs = min(job_levels, key=lambda x: abs(x - jobs))\n        except Exception as e:\n            logging.debug(f\"Failed to check system usage: {e}\")\n            jobs = max_jobs\n            reason = \"auto-detected, system usage check failed\"\n    else:\n        jobs = max_jobs\n        reason = \"auto-detected, no psutil available\"\n\n    return jobs, reason\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.handle_termination","title":"<code>handle_termination(signum=None, frame=None)</code>","text":"<p>Handle termination signals by gracefully shutting down subprocesses.</p> <p>Parameters:</p> Name Type Description Default <code>signum</code> <code>int | None</code> <p>Signal number (e.g., SIGINT).</p> <code>None</code> <code>frame</code> <code>FrameType | None</code> <p>Current stack frame.</p> <code>None</code> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def handle_termination(signum: int | None = None, frame: FrameType | None = None) -&gt; None:\n    \"\"\"Handle termination signals by gracefully shutting down subprocesses.\n\n    Args:\n        signum (int | None): Signal number (e.g., SIGINT).\n        frame (FrameType | None): Current stack frame.\n    \"\"\"\n    logging.warning(\"Termination signal received. Shutting down gracefully...\")\n    stop_event.set()\n    with running_processes_lock:\n        for proc in list(running_processes):\n            try:\n                logging.info(f\"Terminating subprocess: {proc.pid}\")\n                proc.terminate()\n            except Exception:\n                logging.exception(f\"Failed to terminate process {proc.pid}\")\n    logging.info(\"Exiting...\")\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.initialize_compilation","title":"<code>initialize_compilation(args, cpu_count, physical_cores, is_ci)</code>","text":"<p>Initialize compilation settings and tasks.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Command-line arguments.</p> required <code>cpu_count</code> <code>int</code> <p>Total CPU cores.</p> required <code>physical_cores</code> <code>int | None</code> <p>Physical CPU cores.</p> required <code>is_ci</code> <code>bool</code> <p>Whether running in a CI environment.</p> required <p>Returns:</p> Type Description <code>tuple[int, int, str, list[tuple]]</code> <p>tuple[int, int, str, list[tuple]]: Max workers, target jobs, jobs reason, and tasks.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def initialize_compilation(\n    args: argparse.Namespace, cpu_count: int, physical_cores: int | None, is_ci: bool\n) -&gt; tuple[int, int, str, list[tuple]]:\n    \"\"\"Initialize compilation settings and tasks.\n\n    Args:\n        args (argparse.Namespace): Command-line arguments.\n        cpu_count (int): Total CPU cores.\n        physical_cores (int | None): Physical CPU cores.\n        is_ci (bool): Whether running in a CI environment.\n\n    Returns:\n        tuple[int, int, str, list[tuple]]: Max workers, target jobs, jobs reason, and tasks.\n    \"\"\"\n    max_workers = min(physical_cores or 24, cpu_count - 2) if physical_cores else min(cpu_count - 2, 24)\n    default_jobs = 4\n    if args.jobs and args.jobs != default_jobs:\n        target_jobs = args.jobs\n        jobs_reason = \"user-specified\"\n    else:\n        target_jobs, jobs_reason = get_system_adaptive_jobs(cpu_count, physical_cores, is_ci, avg_cpu=0.0)\n\n    if not args.fxc:\n        args.fxc = shutil.which(\"fxc.exe\")\n        if not args.fxc:\n            logging.error(\"fxc.exe not found in PATH. Please specify with --fxc.\")\n            return max_workers, target_jobs, jobs_reason, []\n\n    if not os.path.exists(args.shader_dir):\n        logging.error(f\"Shader directory or file not found: {args.shader_dir}\")\n        return max_workers, target_jobs, jobs_reason, []\n    if not os.path.exists(args.config):\n        logging.error(f\"Configuration file not found: {args.config}\")\n        return max_workers, target_jobs, jobs_reason, []\n\n    tasks = []\n    config_tasks = parse_shader_configs(args.config)\n    # If shader_dir is a file, only compile that file (with all its variants from config)\n    if os.path.isfile(args.shader_dir):\n        shader_file_path = os.path.abspath(args.shader_dir)\n        shader_file_name = os.path.basename(shader_file_path)\n        found = False\n        for file_name, shader_type, entry_name, defines in config_tasks:\n            if os.path.basename(file_name) == shader_file_name:\n                found = True\n                tasks.append((shader_file_path, shader_type, entry_name, defines))\n        if not found:\n            logging.error(f\"Shader file {shader_file_name} not found in config {args.config}\")\n            return max_workers, target_jobs, jobs_reason, []\n    else:\n        # Directory mode (existing logic)\n        for file_name, shader_type, entry_name, defines in config_tasks:\n            shader_file = os.path.join(args.shader_dir, file_name)\n            if not os.path.exists(shader_file):\n                logging.error(f\"Shader file not found: {file_name}\")\n                continue\n            tasks.append((shader_file, shader_type, entry_name, defines))\n\n    if not tasks:\n        logging.error(\"No valid shader compilation tasks found\")\n        return max_workers, target_jobs, jobs_reason, []\n\n    os.makedirs(args.output_dir, exist_ok=True)\n    return max_workers, target_jobs, jobs_reason, tasks\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.load_baseline_warnings","title":"<code>load_baseline_warnings(config_file)</code>","text":"<p>Load baseline warnings from a YAML configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of baseline warnings.</p> Example <p>load_baseline_warnings(\"shader_defines.yaml\") {'x3206:implicit truncation': {'code': 'X3206', 'message': 'implicit truncation', 'instances': {...}}}</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def load_baseline_warnings(config_file: str) -&gt; dict:\n    \"\"\"Load baseline warnings from a YAML configuration file.\n\n    Args:\n        config_file (str): Path to the YAML configuration file.\n\n    Returns:\n        dict: Dictionary of baseline warnings.\n\n    Example:\n        &gt;&gt;&gt; load_baseline_warnings(\"shader_defines.yaml\")\n        {'x3206:implicit truncation': {'code': 'X3206', 'message': 'implicit truncation', 'instances': {...}}}\n    \"\"\"\n    baseline_warnings = {}\n    if config_file and os.path.exists(config_file):\n        try:\n            with open(config_file, encoding=\"utf-8\") as f:\n                config_data = yaml.safe_load(f)\n                if config_data and \"warnings\" in config_data:\n                    baseline_warnings = {k.lower(): v for k, v in config_data[\"warnings\"].items()}\n        except Exception as e:\n            logging.warning(f\"Failed to load baseline warnings from {config_file}: {e}\")\n    return baseline_warnings\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.log_new_issues","title":"<code>log_new_issues(new_warnings, errors, results, output_dir, defines_lookup)</code>","text":"<p>Log new warnings and errors to a unified file.</p> <p>Parameters:</p> Name Type Description Default <code>new_warnings</code> <code>list[dict]</code> <p>List of new warnings.</p> required <code>errors</code> <code>dict</code> <p>Dictionary of compilation errors.</p> required <code>results</code> <code>list[dict]</code> <p>Compilation results.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the log file.</p> required <code>defines_lookup</code> <code>dict</code> <p>Lookup table for shader defines.</p> required Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def log_new_issues(\n    new_warnings: list[dict], errors: dict, results: list[dict], output_dir: str, defines_lookup: dict\n) -&gt; None:\n    \"\"\"Log new warnings and errors to a unified file.\n\n    Args:\n        new_warnings (list[dict]): List of new warnings.\n        errors (dict): Dictionary of compilation errors.\n        results (list[dict]): Compilation results.\n        output_dir (str): Directory to save the log file.\n        defines_lookup (dict): Lookup table for shader defines.\n    \"\"\"\n\n    issue_logger = logging.getLogger(\"new_issues\")\n    issue_logger.setLevel(logging.INFO)\n    issue_handler = logging.FileHandler(os.path.join(output_dir, \"new_issues.log\"), mode=\"w\", encoding=\"utf-8\")\n    issue_handler.setFormatter(logging.Formatter(\"%(message)s\"))\n    issue_logger.addHandler(issue_handler)\n    issue_logger.propagate = False\n\n    # Calculate totals\n    total_warnings = sum(get_instance_count(w) for w in new_warnings)\n    total_warning_entries = sum(\n        sum(len(loc_data[\"entries\"]) for loc_data in w[\"instances\"].values()) for w in new_warnings\n    )\n    total_errors = sum(len(e) for e in errors.values())\n\n    # Header with summary\n    issue_logger.info(\"=\" * 80)\n    issue_logger.info(\"NEW SHADER COMPILATION ISSUES DETECTED\")\n    issue_logger.info(\"=\" * 80)\n    issue_logger.info(f\"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    issue_logger.info(\n        f\"New warnings: {len(new_warnings)} types, {total_warnings} instances, {total_warning_entries} shader combinations\"\n    )\n    issue_logger.info(f\"Compilation errors: {total_errors} total errors\")\n    issue_logger.info(\"=\" * 80)\n    issue_logger.info(\"\")\n\n    # Log compilation errors first (they're more critical)\n    if errors:\n        issue_logger.info(\"COMPILATION ERRORS\")\n        issue_logger.info(\"=\" * 60)\n        issue_logger.info(\"\")\n\n        for shader_key, error_data in errors.items():\n            shader_file = normalize_path(shader_key.split(\":\")[0])\n            entry_point = \":\".join(shader_key.split(\":\")[1:]) if \":\" in shader_key else \"unknown\"\n            shader_type = error_data.get(\"type\", \"unknown\")\n\n            issue_logger.info(f\"ERROR in {shader_file} (entry: {entry_point}, type: {shader_type}):\")\n            issue_logger.info(\"-\" * 40)\n\n            # Group errors by location for better context\n            for location, error_instances in error_data[\"instances\"].items():\n                issue_logger.info(f\"  Location: {location}\")\n                for error in error_instances:\n                    issue_logger.info(f\"    Error Code: {error['code']}\")\n                    issue_logger.info(f\"    Message: {error['message']}\")\n                    if error.get(\"context\"):\n                        issue_logger.info(\n                            f\"    Context: {error['context']['shader_type']} - {error['context']['entry_point']}\"\n                        )\n                    issue_logger.info(\"\")\n\n            issue_logger.info(\"=\" * 40)\n            issue_logger.info(\"\")\n\n    # Log new warnings\n    if new_warnings:\n        issue_logger.info(\"NEW WARNINGS\")\n        issue_logger.info(\"=\" * 60)\n        issue_logger.info(\"\")\n\n        # Sort warnings by total entry count (impact) - highest first\n        sorted_warnings = sorted(\n            new_warnings,\n            key=lambda w: sum(len(loc_data[\"entries\"]) for loc_data in w[\"instances\"].values()),\n            reverse=True,\n        )\n\n        for i, warning in enumerate(sorted_warnings, 1):\n            # Calculate total entries for this warning\n            warning_entry_count = sum(len(loc_data[\"entries\"]) for loc_data in warning[\"instances\"].values())\n\n            issue_logger.info(f\"WARNING #{i}: {warning['code']} - {warning['message']}\")\n            issue_logger.info(f\"Affected shader combinations: {warning_entry_count}\")\n            issue_logger.info(\"-\" * 60)\n\n            # Show each location where this warning occurs\n            for location, location_data in warning[\"instances\"].items():\n                entry_count = len(location_data[\"entries\"])\n                issue_logger.info(f\"Location: {location} ({entry_count} combinations)\")\n\n                # Show the actual compilation output for this location\n                for result in results:\n                    shader_key = f\"{normalize_path(os.path.basename(result['file']))}:{result['entry']}\"\n                    if shader_key in location_data[\"entries\"]:\n                        if result.get(\"log\"):\n                            log_lines = result[\"log\"].splitlines()\n                            warning_lines = [\n                                line for line in log_lines if warning[\"code\"] in line and location.split(\":\")[0] in line\n                            ]\n                            if warning_lines:\n                                warning_line_index = log_lines.index(warning_lines[0])\n                                context_start = max(0, warning_line_index - 2)\n                                context_end = min(len(log_lines), warning_line_index + 3)\n                                context = log_lines[context_start:context_end]\n                                issue_logger.info(\"  Compiler output context:\")\n                                for ctx_line in context:\n                                    issue_logger.info(f\"    {ctx_line}\")\n                        break\n\n                issue_logger.info(\"\")\n\n            issue_logger.info(\"=\" * 60)\n            issue_logger.info(\"\")\n\n    # Summary section\n    issue_logger.info(\"SUMMARY\")\n    issue_logger.info(\"=\" * 60)\n    if total_errors &gt; 0:\n        issue_logger.info(f\"ACTION REQUIRED: Fix {total_errors} compilation errors before proceeding.\")\n    if total_warnings &gt; 0:\n        issue_logger.info(\n            f\"RECOMMENDED: Address {total_warnings} new warnings across {total_warning_entries} shader combinations.\"\n        )\n    if total_errors == 0 and total_warnings == 0:\n        issue_logger.info(\"No new issues detected - compilation is clean!\")\n    issue_logger.info(\"=\" * 80)\n\n    issue_handler.close()\n    issue_logger.removeHandler(issue_handler)\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.main","title":"<code>main()</code>","text":"<p>Main entry point for the shader compilation script.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Exit code (0 for success, 1 for errors or excessive warnings).</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def main() -&gt; int:\n    \"\"\"Main entry point for the shader compilation script.\n\n    Returns:\n        int: Exit code (0 for success, 1 for errors or excessive warnings).\n    \"\"\"\n    default_jobs = 4\n    args = parse_arguments(default_jobs)\n    cpu_count, physical_cores, is_ci = setup_environment(args)\n    try:\n        results = run_compilation(args, cpu_count, physical_cores, is_ci)\n    except KeyboardInterrupt:\n        logging.warning(\"Keyboard interrupt received\")\n        handle_termination()\n        results = []\n\n    if stop_event.is_set() and results:\n        suppress_warnings = [code.strip() for code in args.suppress_warnings.split(\",\") if code.strip()]\n        exit_code, total_new_warnings, error_count = analyze_and_report_results(\n            results, args.config, args.output_dir, suppress_warnings, args.max_warnings\n        )\n        logging.warning(\"Compilation was interrupted\")\n        return exit_code\n\n    suppress_warnings = [code.strip() for code in args.suppress_warnings.split(\",\") if code.strip()]\n    exit_code, total_new_warnings, error_count = analyze_and_report_results(\n        results, args.config, args.output_dir, suppress_warnings, args.max_warnings\n    )\n    return exit_code\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.manage_jobs","title":"<code>manage_jobs(target_jobs, cpu_count, physical_cores, is_ci, cpu_usages, completed_tasks, last_check, check_interval, jobs_reason)</code>","text":"<p>Adjust the number of parallel jobs based on system resources.</p> <p>Parameters:</p> Name Type Description Default <code>target_jobs</code> <code>int</code> <p>Current number of jobs.</p> required <code>cpu_count</code> <code>int</code> <p>Total CPU cores.</p> required <code>physical_cores</code> <code>int | None</code> <p>Physical CPU cores.</p> required <code>is_ci</code> <code>bool</code> <p>Whether running in a CI environment.</p> required <code>cpu_usages</code> <code>deque</code> <p>Recent CPU usage measurements.</p> required <code>completed_tasks</code> <code>int</code> <p>Number of completed tasks.</p> required <code>last_check</code> <code>float</code> <p>Time of last job adjustment.</p> required <code>check_interval</code> <code>float</code> <p>Interval between adjustments.</p> required <code>jobs_reason</code> <code>str</code> <p>Reason for current job count.</p> required <p>Returns:</p> Type Description <code>tuple[int, str, float]</code> <p>tuple[int, str, float]: Adjusted jobs, reason, and last check time.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def manage_jobs(\n    target_jobs: int,\n    cpu_count: int,\n    physical_cores: int | None,\n    is_ci: bool,\n    cpu_usages: deque,\n    completed_tasks: int,\n    last_check: float,\n    check_interval: float,\n    jobs_reason: str,\n) -&gt; tuple[int, str, float]:\n    \"\"\"Adjust the number of parallel jobs based on system resources.\n\n    Args:\n        target_jobs (int): Current number of jobs.\n        cpu_count (int): Total CPU cores.\n        physical_cores (int | None): Physical CPU cores.\n        is_ci (bool): Whether running in a CI environment.\n        cpu_usages (deque): Recent CPU usage measurements.\n        completed_tasks (int): Number of completed tasks.\n        last_check (float): Time of last job adjustment.\n        check_interval (float): Interval between adjustments.\n        jobs_reason (str): Reason for current job count.\n\n    Returns:\n        tuple[int, str, float]: Adjusted jobs, reason, and last check time.\n    \"\"\"\n    current_time = time.time()\n    if HAS_PSUTIL and jobs_reason != \"user-specified\" and current_time - last_check &gt;= check_interval:\n        new_jobs, reason = adjust_target_jobs(\n            target_jobs,\n            cpu_count,\n            physical_cores,\n            is_ci,\n            cpu_usages,\n            completed_tasks,\n        )\n        if new_jobs != target_jobs:\n            logging.info(f\"Adjusted jobs to {new_jobs} ({reason})\")\n            return new_jobs, reason, current_time\n    return target_jobs, jobs_reason, last_check\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.normalize_path","title":"<code>normalize_path(file_path)</code>","text":"<p>Normalize a file path by removing the 'Shaders' prefix and standardizing path separators.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The file path to normalize</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized file path</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def normalize_path(file_path: str) -&gt; str:\n    \"\"\"Normalize a file path by removing the 'Shaders' prefix and standardizing path separators.\n\n    Args:\n        file_path: The file path to normalize\n\n    Returns:\n        The normalized file path\n    \"\"\"\n    if not file_path:\n        return \"\"\n    # Always normalize slashes first\n    file_path = file_path.replace(\"\\\\\", \"/\")\n    # Split on 'Shaders' (case-insensitive) followed by a slash or end of string\n    parts = re.split(r\"(?i)Shaders(?:/|$)\", file_path)\n    if len(parts) &gt; 1:\n        normalized = parts[-1].strip(\"/\")\n        return normalized\n    # If no 'Shaders' found, return the path with normalized slashes\n    return file_path\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.parse_args_for_defaults","title":"<code>parse_args_for_defaults()</code>","text":"<p>Parse command-line arguments to extract default values.</p> <p>Returns:</p> Type Description <code>dict[str, object]</code> <p>dict[str, any]: Dictionary of default argument values.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def parse_args_for_defaults() -&gt; dict[str, object]:\n    \"\"\"Parse command-line arguments to extract default values.\n\n    Returns:\n        dict[str, any]: Dictionary of default argument values.\n    \"\"\"\n    arg_dict = {}\n    args = [arg for arg in sys.argv[1:] if arg != \"--ignore-gooey\"]\n    i = 0\n    while i &lt; len(args):\n        arg = args[i]\n        if arg in [\n            \"--fxc\",\n            \"--shader-dir\",\n            \"--output-dir\",\n            \"--config\",\n            \"--suppress-warnings\",\n            \"--optimization-level\",\n        ]:\n            if i + 1 &lt; len(args) and not args[i + 1].startswith(\"-\"):\n                arg_dict[arg.lstrip(\"-\")] = args[i + 1]\n                i += 2\n            else:\n                i += 1\n        elif arg in [\"--jobs\", \"--max-warnings\"]:\n            if i + 1 &lt; len(args):\n                try:\n                    arg_dict[arg.lstrip(\"-\")] = int(args[i + 1])\n                    i += 2\n                except ValueError:\n                    i += 2\n            else:\n                i += 1\n        elif arg in [\n            \"-d\",\n            \"--debug\",\n            \"-g\",\n            \"--gui\",\n            \"--strip-debug-defines\",\n            \"--force-partial-precision\",\n        ]:\n            arg_dict[arg.lstrip(\"-\")] = True\n            i += 1\n        else:\n            i += 1\n    return arg_dict\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.parse_arguments","title":"<code>parse_arguments(default_jobs)</code>","text":"<p>Parse command-line arguments for the shader compiler.</p> <p>Parameters:</p> Name Type Description Default <code>default_jobs</code> <code>int</code> <p>Default number of parallel jobs.</p> required <p>Returns:</p> Type Description <code>Namespace</code> <p>argparse.Namespace: Parsed command-line arguments.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def parse_arguments(default_jobs: int) -&gt; argparse.Namespace:\n    \"\"\"Parse command-line arguments for the shader compiler.\n\n    Args:\n        default_jobs (int): Default number of parallel jobs.\n\n    Returns:\n        argparse.Namespace: Parsed command-line arguments.\n    \"\"\"\n    defaults = parse_args_for_defaults()\n    is_gui_mode = HAS_GOOEY and (\"--gui\" in sys.argv or \"-g\" in sys.argv)\n    parser_class = GooeyParser if is_gui_mode else argparse.ArgumentParser\n    parser = parser_class(description=\"Compile shaders using fxc.exe.\")\n    parser.add_argument(\n        \"--fxc\",\n        default=defaults.get(\"fxc\"),\n        help=\"Path to fxc.exe (optional if it's in PATH)\",\n    )\n    parser.add_argument(\n        \"--shader-dir\",\n        default=defaults.get(\"shader-dir\", \"build/aio/Shaders\"),\n        help=\"Directory containing shader files\",\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        default=defaults.get(\"output-dir\", \"build/ShaderCache\"),\n        help=\"Output directory for compiled shaders\",\n    )\n    parser.add_argument(\n        \"--config\",\n        required=True,\n        help=\"Shader defines YAML file\",\n    )\n    parser.add_argument(\n        \"--jobs\",\n        type=int,\n        default=defaults.get(\"jobs\", default_jobs),\n        help=\"Number of parallel jobs (default: dynamic, based on system usage)\",\n    )\n    parser.add_argument(\n        \"--max-warnings\",\n        type=int,\n        default=defaults.get(\"max-warnings\", 0),\n        help=\"Warning control: positive=max new warnings, negative=must eliminate N baseline warnings, 0=no new warnings\",\n    )\n    parser.add_argument(\n        \"--suppress-warnings\",\n        default=defaults.get(\"suppress-warnings\", \"\"),\n        help=\"Comma-separated list of warning codes to suppress (e.g., X1519,X3206)\",\n    )\n    parser.add_argument(\n        \"--strip-debug-defines\",\n        action=\"store_true\",\n        default=defaults.get(\"strip-debug-defines\", False),\n        help=\"Strip debug defines for release shaders\",\n    )\n    parser.add_argument(\n        \"--optimization-level\",\n        default=defaults.get(\n            \"optimization-level\",\n            \"3\" if defaults.get(\"strip-debug-defines\", False) else \"1\",\n        ),\n        choices=[\"0\", \"1\", \"2\", \"3\"],\n        help=\"Optimization level (0=none, 1=default, 2=aggressive, 3=max)\",\n    )\n    parser.add_argument(\n        \"--force-partial-precision\",\n        action=\"store_true\",\n        default=defaults.get(\"force-partial-precision\", False),\n        help=\"Force partial precision (16-bit floats) for performance\",\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--debug\",\n        action=\"store_true\",\n        default=defaults.get(\"debug\", False),\n        help=\"Enable debug output\",\n    )\n    parser.add_argument(\n        \"--debug-defines\",\n        default=defaults.get(\n            \"debug-defines\", \"DEBUG,_DEBUG,D3D_DEBUG_INFO,D3DCOMPILE_DEBUG,D3DCOMPILE_SKIP_OPTIMIZATION\"\n        ),\n        help=\"Comma-separated list of defines to treat as debug (default: common debug defines)\",\n    )\n    parser.add_argument(\n        \"--extra-includes\",\n        default=defaults.get(\"extra-includes\", \"\"),\n        help=\"Comma-separated list of additional include directories for fxc.exe\",\n    )\n    if not is_gui_mode:\n        parser.add_argument(\"-g\", \"--gui\", action=\"store_true\", help=\"Run with GUI\")\n    args = parser.parse_args()\n\n    # Validate jobs argument\n    if args.jobs &lt;= 0:\n        parser.error(\"--jobs must be a positive integer\")\n\n    # Pre-compute debug defines set to avoid repeated parsing\n    if args.debug_defines and args.debug_defines.strip():\n        args.debug_defines_set = {d.strip().upper() for d in args.debug_defines.split(\",\") if d.strip()} or None\n    else:\n        args.debug_defines_set = None\n\n    return args\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.parse_shader_configs","title":"<code>parse_shader_configs(config_file)</code>","text":"<p>Parse shader configurations from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to the YAML configuration file.</p> required <p>Returns:</p> Type Description <code>list[tuple]</code> <p>list[tuple]: List of tuples containing (file_name, shader_type, entry_name, defines).</p> Example <p>parse_shader_configs(\"shader_defines.yaml\") [('test.hlsl', 'PSHADER', 'main:1234', ['A=1'])]</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def parse_shader_configs(config_file: str) -&gt; list[tuple]:\n    \"\"\"Parse shader configurations from a YAML file.\n\n    Args:\n        config_file (str): Path to the YAML configuration file.\n\n    Returns:\n        list[tuple]: List of tuples containing (file_name, shader_type, entry_name, defines).\n\n    Example:\n        &gt;&gt;&gt; parse_shader_configs(\"shader_defines.yaml\")\n        [('test.hlsl', 'PSHADER', 'main:1234', ['A=1'])]\n    \"\"\"\n    with open(config_file, encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f)\n\n    if not data or \"shaders\" not in data:\n        logging.error(\"Invalid shader configuration: missing 'shaders' section\")\n        return []\n\n    tasks = []\n    for shader in data[\"shaders\"]:\n        file_name = shader[\"file\"]\n        if \"configs\" not in shader:\n            logging.warning(f\"Skipping shader {file_name}: missing 'configs' section\")\n            continue\n        for shader_type, config in shader[\"configs\"].items():\n            if \"entries\" not in config:\n                logging.warning(f\"Skipping {shader_type} in {file_name}: missing 'entries'\")\n                continue\n            common_defines = config.get(\"common_defines\", [])\n            if not isinstance(common_defines, list):\n                logging.warning(f\"common_defines is not a list in shader config: {file_name}\")\n                common_defines = []\n            for entry in config[\"entries\"]:\n                entry_name = entry[\"entry\"]\n                entry_defines = entry.get(\"defines\", [])\n                if not isinstance(entry_defines, list):\n                    logging.warning(f\"entry defines is not a list in shader config: {file_name}\")\n                    entry_defines = []\n                defines = flatten_defines(common_defines + entry_defines)\n                tasks.append((file_name, shader_type, entry_name, defines))\n\n    return tasks\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.process_completed_futures","title":"<code>process_completed_futures(completed_futures, futures, results, completion_times, pbar, target_jobs, jobs_reason, window_seconds)</code>","text":"<p>Process completed compilation tasks.</p> <p>Parameters:</p> Name Type Description Default <code>completed_futures</code> <code>list</code> <p>List of completed futures.</p> required <code>futures</code> <code>dict</code> <p>Mapping of futures to tasks.</p> required <code>results</code> <code>list[dict]</code> <p>List of compilation results.</p> required <code>completion_times</code> <code>deque</code> <p>Recent completion times.</p> required <code>pbar</code> <code>tqdm</code> <p>Progress bar.</p> required <code>target_jobs</code> <code>int</code> <p>Target number of jobs.</p> required <code>jobs_reason</code> <code>str</code> <p>Reason for current job count.</p> required <code>window_seconds</code> <code>float</code> <p>Time window for rate calculation.</p> required <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>tuple[int, int]: Updated active tasks and completed tasks count.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def process_completed_futures(\n    completed_futures: list,\n    futures: dict,\n    results: list[dict],\n    completion_times: deque,\n    pbar: tqdm,\n    target_jobs: int,\n    jobs_reason: str,\n    window_seconds: float,\n) -&gt; tuple[int, int]:\n    \"\"\"Process completed compilation tasks.\n\n    Args:\n        completed_futures (list): List of completed futures.\n        futures (dict): Mapping of futures to tasks.\n        results (list[dict]): List of compilation results.\n        completion_times (deque): Recent completion times.\n        pbar (tqdm): Progress bar.\n        target_jobs (int): Target number of jobs.\n        jobs_reason (str): Reason for current job count.\n        window_seconds (float): Time window for rate calculation.\n\n    Returns:\n        tuple[int, int]: Updated active tasks and completed tasks count.\n    \"\"\"\n    active_tasks = 0\n    completed_tasks = 0\n    for future in completed_futures:\n        task = futures.pop(future)\n        active_tasks -= 1\n        completed_tasks += 1\n        try:\n            result = future.result()\n            if result:\n                results.append(result)\n        except Exception:\n            logging.exception(f\"Error compiling {task[0]}:{task[2]}\")\n\n        current_time = time.time()\n        completion_times.append((current_time, 1))\n        while completion_times and current_time - completion_times[0][0] &gt; window_seconds:\n            completion_times.popleft()\n\n        total_in_window = sum(count for _, count in completion_times)\n        if len(completion_times) &gt; 1:\n            window_duration = current_time - completion_times[0][0]\n            rate = total_in_window / window_duration if window_duration &gt; 0 else 0\n            postfix = {\"rate\": f\"{rate:.2f} shaders/s\"}\n            if jobs_reason != \"user-specified\":\n                postfix[\"jobs\"] = target_jobs\n            pbar.set_postfix(postfix)\n\n        pbar.update(1)\n        logging.debug(\n            f\"Completed task: active_tasks={active_tasks}, futures={len(futures)}, fxc_processes={count_fxc_processes()}\"\n        )\n    return active_tasks, completed_tasks\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.process_single_error","title":"<code>process_single_error(line, result, errors)</code>","text":"<p>Process a single error line from compilation output.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def process_single_error(line: str, result: dict, errors: dict) -&gt; dict:\n    \"\"\"Process a single error line from compilation output.\"\"\"\n    handler = ErrorHandler(result)\n    return handler.process(line, errors)\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.process_single_warning","title":"<code>process_single_warning(line, result, baseline_warnings, suppress_warnings, all_warnings, new_warnings_dict, suppressed_warnings_count)</code>","text":"<p>Process a single warning line from compilation output.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def process_single_warning(\n    line: str,\n    result: dict,\n    baseline_warnings: dict,\n    suppress_warnings: list[str],\n    all_warnings: dict,\n    new_warnings_dict: dict,\n    suppressed_warnings_count: int,\n) -&gt; tuple[dict, dict, int]:\n    \"\"\"Process a single warning line from compilation output.\"\"\"\n    handler = WarningHandler(result)\n    return handler.process(\n        line, baseline_warnings, suppress_warnings, all_warnings, new_warnings_dict, suppressed_warnings_count\n    )\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.process_warnings_and_errors","title":"<code>process_warnings_and_errors(results, baseline_warnings, suppress_warnings, defines_lookup)</code>","text":"<p>Process warnings and errors from shader compilation results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[dict]</code> <p>List of compilation results.</p> required <code>baseline_warnings</code> <code>dict</code> <p>Baseline warnings for comparison.</p> required <code>suppress_warnings</code> <code>list[str]</code> <p>Warning codes to suppress.</p> required <code>defines_lookup</code> <code>dict</code> <p>Lookup table for shader defines.</p> required <p>Returns:</p> Type Description <code>tuple[list[dict], dict, dict, int]</code> <p>tuple[list[dict], dict, dict, int]: New warnings, all warnings, errors, and suppressed warning count.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def process_warnings_and_errors(\n    results: list[dict],\n    baseline_warnings: dict,\n    suppress_warnings: list[str],\n    defines_lookup: dict,\n) -&gt; tuple[list[dict], dict, dict, int]:\n    \"\"\"Process warnings and errors from shader compilation results.\n\n    Args:\n        results (list[dict]): List of compilation results.\n        baseline_warnings (dict): Baseline warnings for comparison.\n        suppress_warnings (list[str]): Warning codes to suppress.\n        defines_lookup (dict): Lookup table for shader defines.\n\n    Returns:\n        tuple[list[dict], dict, dict, int]: New warnings, all warnings, errors, and suppressed warning count.\n    \"\"\"\n    all_warnings = {}\n    errors = {}\n    new_warnings_dict = {}\n    suppressed_warnings_count = 0\n    suppress_warnings = [code.lower() for code in (suppress_warnings or [])]\n\n    for result in results:\n        if not result.get(\"log\"):\n            continue\n        log_lines = result[\"log\"].splitlines()\n        for line in log_lines:\n            all_warnings, new_warnings_dict, suppressed_warnings_count = process_single_warning(\n                line,\n                result,\n                baseline_warnings,\n                suppress_warnings,\n                all_warnings,\n                new_warnings_dict,\n                suppressed_warnings_count,\n            )\n            errors = process_single_error(line, result, errors)\n\n    return (\n        list(new_warnings_dict.values()),\n        all_warnings,\n        errors,\n        suppressed_warnings_count,\n    )\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.run_compilation","title":"<code>run_compilation(args, cpu_count, physical_cores, is_ci)</code>","text":"<p>Run parallel shader compilation.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Command-line arguments.</p> required <code>cpu_count</code> <code>int</code> <p>Total CPU cores.</p> required <code>physical_cores</code> <code>int | None</code> <p>Physical CPU cores.</p> required <code>is_ci</code> <code>bool</code> <p>Whether running in a CI environment.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of compilation results.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def run_compilation(args: argparse.Namespace, cpu_count: int, physical_cores: int | None, is_ci: bool) -&gt; list[dict]:\n    \"\"\"Run parallel shader compilation.\n\n    Args:\n        args (argparse.Namespace): Command-line arguments.\n        cpu_count (int): Total CPU cores.\n        physical_cores (int | None): Physical CPU cores.\n        is_ci (bool): Whether running in a CI environment.\n\n    Returns:\n        list[dict]: List of compilation results.\n    \"\"\"\n    max_workers, target_jobs, jobs_reason, tasks = initialize_compilation(args, cpu_count, physical_cores, is_ci)\n    if not tasks:\n        return []\n\n    logging.info(\n        f\"Starting compilation of {len(tasks)} shader variants with {max_workers} max workers, {target_jobs} active jobs ({jobs_reason})\"\n    )\n\n    results = []\n    completion_times = deque()\n    window_seconds = 10\n    active_tasks = 0\n    futures = {}\n    task_iterator = iter(tasks)\n    last_check = time.time()\n    check_interval = 10\n    cpu_usages = deque(maxlen=10)\n    completed_tasks = 0\n\n    with (\n        concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor,\n        tqdm(total=len(tasks), desc=\"Compiling shaders\", unit=\"shader\") as pbar,\n    ):\n        while futures or task_iterator:\n            if stop_event.is_set():\n                break\n\n            target_jobs, jobs_reason, last_check = manage_jobs(\n                target_jobs,\n                cpu_count,\n                physical_cores,\n                is_ci,\n                cpu_usages,\n                completed_tasks,\n                last_check,\n                check_interval,\n                jobs_reason,\n            )\n\n            active_tasks, task_iterator = submit_tasks(\n                executor, task_iterator, active_tasks, target_jobs, args, futures, args.shader_dir\n            )\n\n            completed_futures = [f for f in futures if f.done()]\n            active_tasks, new_completed = process_completed_futures(\n                completed_futures, futures, results, completion_times, pbar, target_jobs, jobs_reason, window_seconds\n            )\n            completed_tasks += new_completed\n\n            if futures and active_tasks &gt;= target_jobs:\n                time.sleep(0.1)\n\n    return results\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.setup_environment","title":"<code>setup_environment(args)</code>","text":"<p>Set up the environment for shader compilation.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Command-line arguments.</p> required <p>Returns:</p> Type Description <code>tuple[int, int | None, bool]</code> <p>tuple[int, int | None, bool]: CPU count, physical cores, and CI environment flag.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def setup_environment(args: argparse.Namespace) -&gt; tuple[int, int | None, bool]:\n    \"\"\"Set up the environment for shader compilation.\n\n    Args:\n        args (argparse.Namespace): Command-line arguments.\n\n    Returns:\n        tuple[int, int | None, bool]: CPU count, physical cores, and CI environment flag.\n    \"\"\"\n    logging.basicConfig(\n        level=logging.DEBUG if args.debug else logging.INFO,\n        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n        handlers=[logging.StreamHandler(sys.stderr)],\n    )\n    signal.signal(signal.SIGINT, handle_termination)\n    signal.signal(signal.SIGTERM, handle_termination)\n    is_ci = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n    cpu_count = os.cpu_count() or 4\n    physical_cores = (\n        psutil.cpu_count(logical=False) if HAS_PSUTIL and psutil.cpu_count(logical=False) is not None else None\n    )\n    return cpu_count, physical_cores, is_ci\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.submit_tasks","title":"<code>submit_tasks(executor, task_iterator, active_tasks, target_jobs, args, futures, shader_dir)</code>","text":"<p>Submit compilation tasks to the executor.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>ThreadPoolExecutor</code> <p>Thread pool executor.</p> required <code>task_iterator</code> <code>any</code> <p>Iterator over tasks.</p> required <code>active_tasks</code> <code>int</code> <p>Number of active tasks.</p> required <code>target_jobs</code> <code>int</code> <p>Target number of jobs.</p> required <code>args</code> <code>Namespace</code> <p>Command-line arguments.</p> required <code>futures</code> <code>dict</code> <p>Mapping of futures to tasks.</p> required <code>shader_dir</code> <code>str</code> <p>Directory containing shader files.</p> required <p>Returns:</p> Type Description <code>tuple[int, object]</code> <p>tuple[int, any]: Updated active tasks and task iterator.</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def submit_tasks(\n    executor: concurrent.futures.ThreadPoolExecutor,\n    task_iterator: object,  # Iterator cannot be typed precisely without collections.abc\n    active_tasks: int,\n    target_jobs: int,\n    args: argparse.Namespace,\n    futures: dict,\n    shader_dir: str,\n) -&gt; tuple[int, object]:\n    \"\"\"Submit compilation tasks to the executor.\n\n    Args:\n        executor (concurrent.futures.ThreadPoolExecutor): Thread pool executor.\n        task_iterator (any): Iterator over tasks.\n        active_tasks (int): Number of active tasks.\n        target_jobs (int): Target number of jobs.\n        args (argparse.Namespace): Command-line arguments.\n        futures (dict): Mapping of futures to tasks.\n        shader_dir (str): Directory containing shader files.\n\n    Returns:\n        tuple[int, any]: Updated active tasks and task iterator.\n    \"\"\"\n    while active_tasks &lt; target_jobs and task_iterator:\n        try:\n            task = next(task_iterator)\n            # Parse extra includes from args\n            extra_includes = [s for s in (args.extra_includes.split(\",\") if args.extra_includes else []) if s.strip()]\n            future = executor.submit(\n                compile_shader,\n                args.fxc,\n                task[0],\n                task[1],\n                task[2],\n                task[3],\n                os.path.abspath(args.output_dir),\n                shader_dir,\n                args.debug,\n                args.strip_debug_defines,\n                args.optimization_level,\n                args.force_partial_precision,\n                args.debug_defines_set,\n                extra_includes,  # Pass extra includes\n            )\n            futures[future] = task\n            active_tasks += 1\n            logging.debug(f\"Submitted task: active_tasks={active_tasks}, futures={len(futures)}\")\n        except StopIteration:\n            task_iterator = None\n            break\n    return active_tasks, task_iterator\n</code></pre>"},{"location":"modules/#hlslkit.compile_shaders.validate_shader_inputs","title":"<code>validate_shader_inputs(fxc_path, shader_file, output_dir, defines, shader_dir)</code>","text":"<p>Validate inputs for shader compilation.</p> <p>Parameters:</p> Name Type Description Default <code>fxc_path</code> <code>str</code> <p>Path to fxc.exe.</p> required <code>shader_file</code> <code>str</code> <p>Path to the shader file.</p> required <code>output_dir</code> <code>str</code> <p>Output directory for compiled shaders.</p> required <code>defines</code> <code>list[str]</code> <p>List of preprocessor defines.</p> required <code>shader_dir</code> <code>str</code> <p>Directory containing shader files.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: Error message if validation fails, else None.</p> Example <p>validate_shader_inputs(\"fxc.exe\", \"test.hlsl\", \"build\", [], \"src\")        None</p> Source code in <code>hlslkit/compile_shaders.py</code> <pre><code>def validate_shader_inputs(\n    fxc_path: str, shader_file: str, output_dir: str, defines: list[str], shader_dir: str\n) -&gt; str | None:\n    \"\"\"Validate inputs for shader compilation.\n\n    Args:\n        fxc_path (str): Path to fxc.exe.\n        shader_file (str): Path to the shader file.\n        output_dir (str): Output directory for compiled shaders.\n        defines (list[str]): List of preprocessor defines.\n        shader_dir (str): Directory containing shader files.\n\n    Returns:\n        str | None: Error message if validation fails, else None.\n\n    Example:\n        &gt;&gt;&gt; validate_shader_inputs(\"fxc.exe\", \"test.hlsl\", \"build\", [], \"src\")        None\n    \"\"\"\n    fxc_executable = shutil.which(fxc_path)\n    if not fxc_executable:\n        return \"fxc.exe not found in PATH or specified path\"\n    # Determine if shader_dir is a file or directory\n    if os.path.isfile(shader_dir):\n        shader_file_path = os.path.abspath(shader_file)\n    else:\n        shader_file_path = os.path.join(shader_dir, os.path.basename(shader_file))\n    if not os.path.isfile(shader_file_path) or not shader_file.endswith((\".hlsl\", \".hlsli\")):\n        return f\"Invalid shader file: {shader_file}\"\n    abs_output_dir = os.path.abspath(output_dir)\n    if not os.path.isdir(abs_output_dir):\n        return f\"Invalid output directory: {output_dir}\"\n    valid_define_pattern = re.compile(r\"^[a-zA-Z_][a-zA-Z0-9_]*(?:=[\\w\\d]+)?$\")\n    invalid_defines = [d for d in defines if not valid_define_pattern.match(d)]\n    if invalid_defines:\n        return f\"Invalid defines: {invalid_defines}\"\n    return None\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.CompilationTask","title":"<code>CompilationTask</code>  <code>dataclass</code>","text":"<p>A shader compilation task extracted from the log.</p> <p>Attributes:</p> Name Type Description <code>process_id</code> <code>str</code> <p>Process ID of the compilation task.</p> <code>entry_point</code> <code>str</code> <p>Shader entry point (e.g., 'main:vertex:1234').</p> <code>file_path</code> <code>str</code> <p>Path to the shader file.</p> <code>defines</code> <code>list[str]</code> <p>Preprocessor defines used.</p> <code>start_time</code> <code>datetime</code> <p>Start time of the compilation.</p> <code>end_time</code> <code>datetime | None</code> <p>End time of the compilation, if completed.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>@dataclass\nclass CompilationTask:\n    \"\"\"A shader compilation task extracted from the log.\n\n    Attributes:\n        process_id (str): Process ID of the compilation task.\n        entry_point (str): Shader entry point (e.g., 'main:vertex:1234').\n        file_path (str): Path to the shader file.\n        defines (list[str]): Preprocessor defines used.\n        start_time (datetime): Start time of the compilation.\n        end_time (datetime | None): End time of the compilation, if completed.\n    \"\"\"\n\n    process_id: str\n    entry_point: str\n    file_path: str\n    defines: list[str]\n    start_time: datetime\n    end_time: datetime | None = None\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.collect_tasks","title":"<code>collect_tasks(lines)</code>","text":"<p>Collect compilation tasks from log lines.</p> <p>Parameters:</p> Name Type Description Default <code>lines</code> <code>list[str]</code> <p>List of log lines to parse.</p> required <p>Returns:</p> Type Description <code>list[CompilationTask]</code> <p>list[CompilationTask]: List of extracted compilation tasks.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def collect_tasks(lines: list[str]) -&gt; list[CompilationTask]:\n    \"\"\"Collect compilation tasks from log lines.\n\n    Args:\n        lines (list[str]): List of log lines to parse.\n\n    Returns:\n        list[CompilationTask]: List of extracted compilation tasks.\n    \"\"\"\n    tasks = []\n    compile_regex = re.compile(\n        r\"\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\] \\[(\\d+)\\] \\[D\\] Compiling (.*?)\\s+([^:]+:[^:]+:[0-9a-fA-F]+)\\s+to\\s+(.*)$\"\n    )\n    compiled_shader_regex = re.compile(\n        r\"\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\] \\[(\\d+)\\] \\[D\\] Compiled shader ([^:]+:[^:]+:[0-9a-fA-F]+)\"\n    )\n    completed_regex = re.compile(\n        r\"\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\] \\[(\\d+)\\] \\[D\\] Adding Completed shader to map: ([^:]+:[^:]+:[0-9a-fA-F]+)(?::.*)?$\"\n    )\n\n    for line in lines:\n        compile_match = compile_regex.match(line)\n        if compile_match:\n            timestamp, process_id, file_path, entry_point, compile_args = compile_match.groups()\n            defines = re.findall(r\"\\S+=[\\w\\d]+|\\S+\", compile_args.strip())\n            tasks.append(\n                CompilationTask(\n                    process_id=process_id,\n                    entry_point=entry_point,\n                    file_path=file_path,\n                    defines=defines,\n                    start_time=parse_timestamp(line),\n                )\n            )\n            continue\n\n        compiled_match = compiled_shader_regex.match(line)\n        if compiled_match:\n            timestamp, process_id, entry_point = compiled_match.groups()\n            for task in reversed(tasks):\n                if task.process_id == process_id and task.entry_point == entry_point and task.end_time is None:\n                    task.end_time = parse_timestamp(line)\n                    break\n            continue\n\n        completed_match = completed_regex.match(line)\n        if completed_match:\n            timestamp, process_id, entry_point = completed_match.groups()\n            for task in reversed(tasks):\n                if task.process_id == process_id and task.entry_point == entry_point and task.end_time is None:\n                    task.end_time = parse_timestamp(line)\n                    break\n    return tasks\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.collect_warnings_and_errors","title":"<code>collect_warnings_and_errors(lines, tasks, warnings, errors, total_logs)</code>","text":"<p>Collect warnings and errors from log lines.</p> <p>Parameters:</p> Name Type Description Default <code>lines</code> <code>list[str]</code> <p>List of log lines.</p> required <code>tasks</code> <code>list[CompilationTask]</code> <p>List of compilation tasks.</p> required <code>warnings</code> <code>dict</code> <p>Dictionary to store warnings.</p> required <code>errors</code> <code>dict</code> <p>Dictionary to store errors.</p> required <code>total_logs</code> <code>int</code> <p>Total number of log blocks to process.</p> required <p>Returns:</p> Type Description <code>tuple[dict, dict]</code> <p>tuple[dict, dict]: Updated warnings and errors dictionaries.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def collect_warnings_and_errors(\n    lines: list[str],\n    tasks: list[CompilationTask],\n    warnings: dict,\n    errors: dict,\n    total_logs: int,\n) -&gt; tuple[dict, dict]:\n    \"\"\"Collect warnings and errors from log lines.\n\n    Args:\n        lines (list[str]): List of log lines.\n        tasks (list[CompilationTask]): List of compilation tasks.\n        warnings (dict): Dictionary to store warnings.\n        errors (dict): Dictionary to store errors.\n        total_logs (int): Total number of log blocks to process.\n\n    Returns:\n        tuple[dict, dict]: Updated warnings and errors dictionaries.\n    \"\"\"\n    warning_entry_regex = re.compile(r\"^(.*?)\\((\\d+(?:,\\d+(?:-\\d+)?|\\:\\d+)?)\\): warning (\\w+): (.+)$\")\n    error_e_regex = re.compile(\n        r\"\\[\\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\] \\[(\\d+)\\] \\[E\\] Failed to compile Pixel shader ([^:]+::[0-9a-fA-F]+):\\n(.*?)\\((\\d+(?:,\\d+(?:-\\d+)?))\\): error (\\w+): (.+)$\",\n        re.DOTALL,\n    )\n    error_w_regex = re.compile(\n        r\"\\[\\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\] \\[(\\d+)\\] \\[W\\] Shader compilation failed:\\n(.*?):(\\d+(?::\\d+))\\: (\\w+): (.+)$\",\n        re.DOTALL,\n    )\n    completed_regex = re.compile(\n        r\"\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\] \\[(\\d+)\\] \\[D\\] Adding Completed shader to map: ([^:]+:[^:]+:[0-9a-fA-F]+)(?::.*)?$\"\n    )\n\n    with tqdm(total=total_logs, desc=\"Parsing logs (warnings/errors)\", unit=\"block\") as pbar:\n        i = 0\n        while i &lt; len(lines):\n            line = lines[i].strip()\n            shader_log_match = re.match(r\"\\[(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\] \\[(\\d+)\\] \\[D\\] Shader logs:\", line)\n            if shader_log_match:\n                timestamp, current_process_id = shader_log_match.groups()\n                current_time = parse_timestamp(line)\n                current_warnings = []\n                pbar.update(1)\n                i += 1\n                while i &lt; len(lines):\n                    next_line = lines[i].strip()\n                    if not next_line or next_line.startswith(\"[\"):\n                        break\n                    warning_match = warning_entry_regex.match(next_line)\n                    if warning_match:\n                        file_path, line_info, warning_code, warning_msg = warning_match.groups()\n                        norm_file_path = normalize_path(file_path)\n                        location = f\"{norm_file_path}:{line_info}\"\n                        current_warnings.append((warning_code, warning_msg, location))\n                    i += 1\n\n                for task in reversed(tasks):\n                    if (\n                        task.process_id == current_process_id\n                        and task.start_time &lt;= current_time\n                        and (task.end_time is None or task.end_time &gt;= current_time)\n                    ):\n                        entry_point = task.entry_point\n                        for code, message, location in current_warnings:\n                            warning_key = f\"{code}:{message}\".lower()\n                            if warning_key not in warnings:\n                                warnings[warning_key] = {\n                                    \"code\": code,\n                                    \"message\": message,\n                                    \"instances\": {},\n                                }\n                            location_lower = location.lower()\n                            if location_lower not in warnings[warning_key][\"instances\"]:\n                                warnings[warning_key][\"instances\"][location_lower] = {\"entries\": []}\n                            if entry_point not in warnings[warning_key][\"instances\"][location_lower][\"entries\"]:\n                                warnings[warning_key][\"instances\"][location_lower][\"entries\"].append(entry_point)\n                        break\n                continue\n\n            match = error_e_regex.search(line)\n            if match:\n                process_id = match.group(1)\n                entry_point = match.group(2).replace(\"::\", \":\")\n                file_path = match.group(3)\n                line_info = match.group(4)\n                error_code = match.group(5)\n                error_msg = match.group(6)\n                for task in tasks:\n                    if task.process_id == process_id and task.entry_point == entry_point:\n                        file_name = normalize_path(task.file_path)\n                        key = f\"{file_name}:{entry_point}\"\n                        if key not in errors:\n                            errors[key] = []\n                        norm_file_path = normalize_path(file_path)\n                        error_text = f\"{error_code}: {error_msg} ({norm_file_path}:{line_info})\"\n                        if error_text not in errors[key]:\n                            errors[key].append(error_text)\n                        break\n                pbar.update(1)\n\n            match = error_w_regex.search(line)\n            if match:\n                process_id = match.group(1)\n                file_path = match.group(2)\n                line_info = match.group(3)\n                error_code = match.group(4)\n                error_msg = match.group(5)\n                for task in tasks:\n                    if task.process_id == process_id:\n                        file_name = normalize_path(task.file_path)\n                        entry_point = task.entry_point\n                        key = f\"{file_name}:{entry_point}\"\n                        if key not in errors:\n                            errors[key] = []\n                        norm_file_path = normalize_path(file_path)\n                        error_text = f\"{error_code}: {error_msg} ({norm_file_path}:{line_info})\"\n                        if error_text not in errors[key]:\n                            errors[key].append(error_text)\n                        break\n                pbar.update(1)\n\n            match = completed_regex.search(line)\n            if match:\n                pbar.update(1)\n            i += 1\n\n    return warnings, errors\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.compute_common_defines","title":"<code>compute_common_defines(shader_configs)</code>","text":"<p>Compute common defines across shaders, types, and files.</p> <p>Parameters:</p> Name Type Description Default <code>shader_configs</code> <code>dict</code> <p>Shader configurations.</p> required <p>Returns:     tuple[list, dict, dict]: Global common defines, type-specific common defines, and file-type-specific common defines.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def compute_common_defines(shader_configs: dict) -&gt; tuple[list, dict, dict]:\n    \"\"\"Compute common defines across shaders, types, and files.\n\n    Args:\n        shader_configs (dict): Shader configurations.\n    Returns:\n        tuple[list, dict, dict]: Global common defines, type-specific common defines, and file-type-specific common defines.\n    \"\"\"\n    all_defines = [\n        config[\"defines\"]\n        for file_configs in shader_configs.values()\n        for configs in file_configs.values()\n        for config in configs\n    ]\n    global_common = sorted(set.intersection(*[set(d) for d in all_defines])) if all_defines else []\n    defines_by_type = {\"PSHADER\": [], \"VSHADER\": [], \"CSHADER\": []}\n    for file_configs in shader_configs.values():\n        for shader_type, configs in file_configs.items():\n            for config in configs:\n                defines_by_type[shader_type].append(config[\"defines\"])\n    type_common = {}\n    for shader_type, defines_list in defines_by_type.items():\n        type_common[shader_type] = sorted(set.intersection(*[set(d) for d in defines_list])) if defines_list else []\n        type_common[shader_type] = sorted([d for d in type_common[shader_type] if d not in global_common])\n    file_type_common = {}\n    for file_name, file_configs in shader_configs.items():\n        file_type_common[file_name] = {\"PSHADER\": [], \"VSHADER\": [], \"CSHADER\": []}\n        for shader_type, configs in file_configs.items():\n            file_type_common[file_name][shader_type] = sorted(\n                set.intersection(*[set(c[\"defines\"]) for c in configs]) if configs else set()\n            )\n            file_type_common[file_name][shader_type] = sorted([\n                d\n                for d in file_type_common[file_name][shader_type]\n                if d not in global_common and d not in type_common[shader_type]\n            ])\n    return global_common, type_common, file_type_common\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.count_compiling_lines","title":"<code>count_compiling_lines(log_file)</code>","text":"<p>Count the number of compilation log lines in a file.</p> <p>Parameters:</p> Name Type Description Default <code>log_file</code> <code>str</code> <p>Path to the log file.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of lines containing '[D] Compiling'.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def count_compiling_lines(log_file: str) -&gt; int:\n    \"\"\"Count the number of compilation log lines in a file.\n\n    Args:\n        log_file (str): Path to the log file.\n\n    Returns:\n        int: Number of lines containing '[D] Compiling'.\n    \"\"\"\n    with open(log_file, encoding=\"utf-8\") as f:\n        return sum(1 for line in f if \"[D] Compiling\" in line)\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.count_log_blocks","title":"<code>count_log_blocks(log_file)</code>","text":"<p>Count the number of log blocks (warnings, errors, completions) in a file.</p> <p>Parameters:</p> Name Type Description Default <code>log_file</code> <code>str</code> <p>Path to the log file.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of log blocks.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def count_log_blocks(log_file: str) -&gt; int:\n    \"\"\"Count the number of log blocks (warnings, errors, completions) in a file.\n\n    Args:\n        log_file (str): Path to the log file.\n\n    Returns:\n        int: Number of log blocks.\n    \"\"\"\n    with open(log_file, encoding=\"utf-8\") as f:\n        return sum(\n            1\n            for line in f\n            if any(\n                keyword in line\n                for keyword in (\n                    \"[D] Shader logs:\",\n                    \"[E] Failed to compile\",\n                    \"[W] Shader compilation failed\",\n                    \"[D] Adding Completed shader\",\n                )\n            )\n        )\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.generate_yaml_data","title":"<code>generate_yaml_data(shader_configs, warnings, errors)</code>","text":"<p>Generate YAML data structure from shader configurations, warnings, and errors.</p> <p>Parameters:</p> Name Type Description Default <code>shader_configs</code> <code>dict</code> <p>Shader configurations.</p> required <code>warnings</code> <code>dict</code> <p>Compilation warnings.</p> required <code>errors</code> <code>dict</code> <p>Compilation errors.</p> required <p>Returns:     dict: YAML-compatible data structure.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def generate_yaml_data(shader_configs: dict, warnings: dict, errors: dict) -&gt; dict:\n    \"\"\"Generate YAML data structure from shader configurations, warnings, and errors.\n\n    Args:\n        shader_configs (dict): Shader configurations.\n        warnings (dict): Compilation warnings.\n        errors (dict): Compilation errors.\n    Returns:\n        dict: YAML-compatible data structure.\n    \"\"\"\n    global_common, type_common, file_type_common = compute_common_defines(shader_configs)\n    yaml_data = {\n        \"common_defines\": sorted(global_common),\n        \"common_pshader_defines\": sorted(type_common[\"PSHADER\"]),\n        \"common_vshader_defines\": sorted(type_common[\"VSHADER\"]),\n        \"common_cshader_defines\": sorted(type_common[\"CSHADER\"]),\n        \"file_common_defines\": file_type_common,\n        \"warnings\": warnings,\n        \"errors\": errors,\n        \"shaders\": [],\n    }\n    for file_name, file_configs in shader_configs.items():\n        shader_entry = {\"file\": file_name, \"configs\": {}}\n        for shader_type, configs in file_configs.items():\n            if configs:\n                common_defines = []\n                for defines in [\n                    global_common,\n                    type_common[shader_type],\n                    file_type_common[file_name][shader_type],\n                ]:\n                    if isinstance(defines, list):\n                        common_defines.extend(defines)\n                    elif isinstance(defines, (set, tuple)):\n                        common_defines.extend(list(defines))\n                shader_entry[\"configs\"][shader_type] = {\n                    \"common_defines\": sorted(common_defines),\n                    \"entries\": [\n                        {\n                            \"entry\": config[\"entry\"],\n                            \"defines\": sorted([\n                                d\n                                for d in config[\"defines\"]\n                                if d not in global_common\n                                and d not in type_common[shader_type]\n                                and d not in file_type_common[file_name][shader_type]\n                            ]),\n                        }\n                        for config in configs\n                    ],\n                }\n        if shader_entry[\"configs\"]:\n            yaml_data[\"shaders\"].append(shader_entry)\n    return yaml_data\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.get_shader_type_from_entry","title":"<code>get_shader_type_from_entry(entry_point)</code>","text":"<p>Determine the shader type from an entry point string.</p> <p>Parameters:</p> Name Type Description Default <code>entry_point</code> <code>str</code> <p>The entry point string (e.g., 'main:vertex:1234').</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The shader type (VSHADER, PSHADER, CSHADER, or UNKNOWN).</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def get_shader_type_from_entry(entry_point: str) -&gt; str:\n    \"\"\"Determine the shader type from an entry point string.\n\n    Args:\n        entry_point (str): The entry point string (e.g., 'main:vertex:1234').\n\n    Returns:\n        str: The shader type (VSHADER, PSHADER, CSHADER, or UNKNOWN).\n    \"\"\"\n    parts = entry_point.split(\":\")\n    if len(parts) &gt;= 3:\n        shader_type = parts[1].lower()\n        shader_types = {\"vertex\": \"VSHADER\", \"pixel\": \"PSHADER\", \"compute\": \"CSHADER\"}\n        return shader_types.get(shader_type, \"UNKNOWN\")\n    return \"UNKNOWN\"\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.main","title":"<code>main()</code>","text":"<p>Main entry point for generating shader defines from a log file.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Exit code (0 for success, 1 for errors).</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def main() -&gt; int:\n    \"\"\"Main entry point for generating shader defines from a log file.\n\n    Returns:\n        int: Exit code (0 for success, 1 for errors).\n    \"\"\"\n    args = parse_arguments()\n    log_level = getattr(logging, getattr(args, \"log_level\", \"INFO\").upper(), logging.INFO)\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n        handlers=[logging.StreamHandler(sys.stderr)],\n    )\n    if args.debug:\n        logging.getLogger().setLevel(logging.DEBUG)\n\n    if not os.path.exists(args.log):\n        logging.error(f\"Log file not found: {args.log}\")\n        return 1\n\n    start_time = time.time()\n    shader_configs, warnings, errors = parse_log(args.log)\n    yaml_data = generate_yaml_data(shader_configs, warnings, errors)\n    save_yaml(yaml_data, args.output)\n\n    total_variants = sum(len(configs) for file_configs in shader_configs.values() for configs in file_configs.values())\n    logging.info(\n        f\"Generated {args.output} in {time.time() - start_time:.2f} seconds \"\n        f\"with {len(yaml_data['shaders'])} shaders, {total_variants} variants, \"\n        f\"{len(warnings)} unique warnings, and {sum(len(e) for e in errors.values())} errors\"\n    )\n    return 0\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.normalize_path","title":"<code>normalize_path(file_path)</code>","text":"<p>Normalize a file path by standardizing separators and extracting relative path.    This function ensures paths are consistent across platforms, converting backslashes to forward slashes and attempting to extract the relative path from the <code>Shaders</code> directory (case-insensitive). If the <code>Shaders</code> directory is not found, the path is returned as-is with normalized separators.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The file path to normalize.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The normalized file path, relative to the Shaders directory if present.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def normalize_path(file_path: str) -&gt; str:\n    \"\"\"Normalize a file path by standardizing separators and extracting relative path.    This function ensures paths are consistent across platforms, converting backslashes to forward slashes\n    and attempting to extract the relative path from the `Shaders` directory (case-insensitive). If the\n    `Shaders` directory is not found, the path is returned as-is with normalized separators.\n\n    Args:\n        file_path (str): The file path to normalize.\n\n    Returns:\n        str: The normalized file path, relative to the Shaders directory if present.\n    \"\"\"\n    file_path = file_path.replace(\"\\\\\", \"/\")\n    file_path = re.sub(r\"/+\", \"/\", file_path)\n    pattern = r\"(?i).*?\\bShaders[/](.*)\"\n    match = re.search(pattern, file_path)\n    if match:\n        norm_path = match.group(1).replace(\"\\\\\", \"/\")\n        logging.debug(f\"Normalized path (Shaders found): {file_path} -&gt; {norm_path}\")\n        return norm_path\n    norm_path = file_path.replace(\"\\\\\", \"/\")\n    logging.debug(f\"Normalized path (no Shaders, using as-is): {file_path} -&gt; {norm_path}\")\n    return norm_path\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.optimize_anchor_deduplication","title":"<code>optimize_anchor_deduplication(yaml_data)</code>","text":"<p>Pre-process data to maximize anchor reuse with O(n log n) complexity. Returns (optimized_data, stats_dict)</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def optimize_anchor_deduplication(yaml_data):\n    \"\"\"Pre-process data to maximize anchor reuse with O(n log n) complexity.\n    Returns (optimized_data, stats_dict)\n    \"\"\"\n    list_counts = {}\n    list_instances = []\n\n    def make_hashable(obj):\n        if isinstance(obj, list):\n            return tuple(make_hashable(x) for x in obj)\n        elif isinstance(obj, dict):\n            return tuple(sorted((k, make_hashable(v)) for k, v in obj.items()))\n        else:\n            return obj\n\n    def collect_lists(obj, path=None):\n        if path is None:\n            path = []\n        if isinstance(obj, list):\n            try:\n                key = make_hashable(obj)\n                if key not in list_counts:\n                    list_counts[key] = 0\n                list_counts[key] += 1\n                list_instances.append((key, obj, path))\n                logging.debug(f\"List at {path}: {obj} -&gt; key: {key}\")\n            except (TypeError, RecursionError):\n                logging.debug(f\"Skipping unhashable list at {path}: {obj}\")\n                pass\n            for idx, item in enumerate(obj):\n                collect_lists(item, [*path, idx])\n        elif isinstance(obj, dict):\n            for k, v in obj.items():\n                collect_lists(v, [*path, k])\n\n    collect_lists(yaml_data)\n    logging.debug(f\"Found {len(list_counts)} unique lists\")\n    for key, count in list_counts.items():\n        logging.debug(f\"List {key} appears {count} times\")\n\n    def reconstruct_list(obj):\n        if isinstance(obj, tuple):\n            return [reconstruct_list(x) for x in obj]\n        else:\n            return obj\n\n    sorted_lists = sorted([(key, count, len(key)) for key, count in list_counts.items()], key=lambda x: (-x[1], -x[2]))\n    shared_objects = {}\n    anchor_ref_count = 0\n    for key, count, _size in sorted_lists:\n        if count &gt; 1:\n            shared_objects[key] = reconstruct_list(key)\n            logging.debug(f\"Creating shared object for {key} (used {count} times)\")\n            anchor_ref_count += count - 1  # Each additional use is an alias\n    logging.debug(f\"Created {len(shared_objects)} shared objects\")\n\n    def replace_with_shared(obj, path=None):\n        if path is None:\n            path = []\n        if isinstance(obj, list):\n            try:\n                key = make_hashable(obj)\n                if key in shared_objects:\n                    logging.debug(f\"Replacing list at {path} with shared object\")\n                    return shared_objects[key]\n            except (TypeError, RecursionError):\n                pass\n            return [replace_with_shared(x, [*path, i]) for i, x in enumerate(obj)]\n        elif isinstance(obj, dict):\n            return {k: replace_with_shared(v, [*path, k]) for k, v in obj.items()}\n        else:\n            return obj\n\n    optimized = replace_with_shared(yaml_data)\n    stats = {\n        \"total_lists\": len(list_instances),\n        \"unique_lists\": len(list_counts),\n        \"anchored_lists\": len(shared_objects),\n        \"anchor_references\": anchor_ref_count,\n        \"max_anchor_usage\": max([count for key, count in list_counts.items() if count &gt; 1], default=0),\n    }\n    return optimized, stats\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.parse_arguments","title":"<code>parse_arguments()</code>","text":"<p>Parse command-line arguments for the shader defines generator.</p> <p>Returns:</p> Type Description <code>Namespace</code> <p>argparse.Namespace: Parsed command-line arguments.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def parse_arguments() -&gt; argparse.Namespace:\n    \"\"\"Parse command-line arguments for the shader defines generator.\n\n    Returns:\n        argparse.Namespace: Parsed command-line arguments.\n    \"\"\"\n    is_gui_mode = HAS_GOOEY and (\"--gui\" in sys.argv or \"-g\" in sys.argv)\n    parser_class = GooeyParser if is_gui_mode else argparse.ArgumentParser\n    parser = parser_class(description=\"Generate shader defines from CommunityShaders log.\")\n    parser.add_argument(\n        \"--log\",\n        default=\"CommunityShaders.log\",\n        help=\"Path to CommunityShaders log file\",\n    )\n    parser.add_argument(\n        \"--output\",\n        default=\"shader_defines.yaml\",\n        help=\"Output YAML file for shader defines\",\n    )\n    parser.add_argument(\"-d\", \"--debug\", action=\"store_true\", help=\"Enable debug logging\")\n    parser.add_argument(\n        \"--log-level\",\n        default=\"INFO\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        help=\"Set the logging level (default: INFO)\",\n    )\n    if not is_gui_mode:\n        parser.add_argument(\"-g\", \"--gui\", action=\"store_true\", help=\"Run with GUI\")\n    return parser.parse_args()\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.parse_log","title":"<code>parse_log(log_file, update_configs=None, update_warnings=None, update_errors=None)</code>","text":"<p>Parse a CommunityShaders log file to extract shader configurations, warnings, and errors.</p> <p>Parameters:</p> Name Type Description Default <code>log_file</code> <code>str</code> <p>Path to the log file.</p> required <code>update_configs</code> <code>dict | None</code> <p>Existing shader configurations to update (optional).</p> <code>None</code> <code>update_warnings</code> <code>dict | None</code> <p>Existing warnings to update (optional).</p> <code>None</code> <code>update_errors</code> <code>dict | None</code> <p>Existing errors to update (optional).</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict, dict, dict]</code> <p>tuple[dict, dict, dict]: Shader configurations, warnings, and errors.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def parse_log(\n    log_file: str,\n    update_configs: dict | None = None,\n    update_warnings: dict | None = None,\n    update_errors: dict | None = None,\n) -&gt; tuple[dict, dict, dict]:\n    \"\"\"Parse a CommunityShaders log file to extract shader configurations, warnings, and errors.\n\n    Args:\n        log_file (str): Path to the log file.\n        update_configs (dict | None): Existing shader configurations to update (optional).\n        update_warnings (dict | None): Existing warnings to update (optional).\n        update_errors (dict | None): Existing errors to update (optional).\n\n    Returns:\n        tuple[dict, dict, dict]: Shader configurations, warnings, and errors.\n    \"\"\"\n    shader_configs = update_configs or {}\n    warnings = update_warnings or {}\n    errors = update_errors or {}\n\n    with open(log_file, encoding=\"utf-8\") as f:\n        lines = f.readlines()\n\n    tasks = collect_tasks(lines)\n    shader_configs = populate_configs(tasks, shader_configs)\n    total_logs = count_log_blocks(log_file)\n    warnings, errors = collect_warnings_and_errors(lines, tasks, warnings, errors, total_logs)\n\n    return shader_configs, warnings, errors\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.parse_timestamp","title":"<code>parse_timestamp(line)</code>","text":"<p>Parse a timestamp from a log line.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>str</code> <p>The log line containing a timestamp.</p> required <p>Returns:</p> Name Type Description <code>datetime</code> <code>datetime</code> <p>The parsed datetime object.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def parse_timestamp(line: str) -&gt; datetime:\n    \"\"\"Parse a timestamp from a log line.\n\n    Args:\n        line (str): The log line containing a timestamp.\n\n    Returns:\n        datetime: The parsed datetime object.\n    \"\"\"\n    timestamp_str = line[1:13]\n    return datetime.strptime(timestamp_str, \"%H:%M:%S.%f\")\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.populate_configs","title":"<code>populate_configs(tasks, shader_configs)</code>","text":"<p>Populate shader configurations from compilation tasks.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>list[CompilationTask]</code> <p>List of compilation tasks.</p> required <code>shader_configs</code> <code>dict</code> <p>Dictionary to store shader configurations.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated shader configurations.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def populate_configs(tasks: list[CompilationTask], shader_configs: dict) -&gt; dict:\n    \"\"\"Populate shader configurations from compilation tasks.\n\n    Args:\n        tasks (list[CompilationTask]): List of compilation tasks.\n        shader_configs (dict): Dictionary to store shader configurations.\n\n    Returns:\n        dict: Updated shader configurations.\n    \"\"\"\n    total_lines = len(tasks)\n    with tqdm(total=total_lines, desc=\"Parsing compiling lines\", unit=\"line\") as pbar:\n        for task in tasks:\n            file_path = normalize_path(task.file_path)\n            file_name = file_path\n            entry_point = task.entry_point\n            shader_type = get_shader_type_from_entry(entry_point)\n            defines = sorted(task.defines)  # Sort for better anchor deduplication\n\n            if file_name not in shader_configs:\n                shader_configs[file_name] = {\n                    \"PSHADER\": [],\n                    \"VSHADER\": [],\n                    \"CSHADER\": [],\n                }\n            config = {\"entry\": entry_point, \"defines\": defines}\n            existing_configs = [c for c in shader_configs[file_name][shader_type] if c[\"entry\"] == config[\"entry\"]]\n            if existing_configs:\n                existing = existing_configs[0]\n                if set(existing[\"defines\"]) != set(config[\"defines\"]):\n                    print(\n                        f\"Warning: Updating defines for {file_name} {config['entry']} ({shader_type}): {existing['defines']} -&gt; {config['defines']}\"\n                    )\n                    existing[\"defines\"] = config[\"defines\"]\n            else:\n                shader_configs[file_name][shader_type].append(config)\n            pbar.update(1)\n    return shader_configs\n</code></pre>"},{"location":"modules/#hlslkit.generate_shader_defines.save_yaml","title":"<code>save_yaml(yaml_data, output_file)</code>","text":"<p>Save the YAML data to a file, using anchors for repeated lists to reduce repetition. The data is pre-processed by optimize_anchor_deduplication() to maximize anchor usage with O(n log n) complexity instead of the previous \u0398(n\u00b2) approach. Args:     yaml_data (dict): YAML data to save.     output_file (str): Path to the output YAML file.</p> Source code in <code>hlslkit/generate_shader_defines.py</code> <pre><code>def save_yaml(yaml_data: dict, output_file: str) -&gt; None:\n    \"\"\"Save the YAML data to a file, using anchors for repeated lists to reduce repetition.\n    The data is pre-processed by optimize_anchor_deduplication() to maximize anchor usage\n    with O(n log n) complexity instead of the previous \u0398(n\u00b2) approach.\n    Args:\n        yaml_data (dict): YAML data to save.\n        output_file (str): Path to the output YAML file.\n    \"\"\"\n\n    class OptimizedAnchorDumper(yaml.SafeDumper):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self._anchor_map = {}\n            self._next_anchor_id = 1\n\n        def ignore_aliases(self, data):\n            return not isinstance(data, list)\n\n        def represent_sequence(self, tag, sequence, flow_style=None):\n            obj_id = id(sequence)\n            if obj_id in self._anchor_map:\n                anchor = self._anchor_map[obj_id]\n            else:\n                anchor = None\n                if hasattr(self, \"_seen_objects\"):\n                    if obj_id in self._seen_objects:\n                        anchor = f\"anchor_{self._next_anchor_id}\"\n                        self._next_anchor_id += 1\n                        self._anchor_map[obj_id] = anchor\n                    else:\n                        self._seen_objects.add(obj_id)\n                else:\n                    self._seen_objects = {obj_id}\n            node = super().represent_sequence(tag, sequence, flow_style)\n            if anchor:\n                node.anchor = anchor  # type: ignore[attr-defined]\n            return node\n\n    deduped, stats = optimize_anchor_deduplication(yaml_data)\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        yaml.dump(\n            deduped,\n            f,\n            Dumper=OptimizedAnchorDumper,\n            sort_keys=False,\n            allow_unicode=True,\n        )\n    logging.info(\n        f\"Deduplication stats: {stats['total_lists']} total lists, {stats['unique_lists']} unique \"\n        f\"({stats['unique_lists'] / stats['total_lists'] * 100:.1f}% unique), \"\n        f\"{stats['anchored_lists']} anchored ({stats['anchored_lists'] / stats['total_lists'] * 100:.1f}% of total), \"\n        f\"{stats['anchor_references']} references saved \"\n        f\"(reduction: {stats['anchor_references'] / (stats['total_lists'] - stats['unique_lists']) * 100:.1f}% of duplicates)\"\n        if stats[\"total_lists\"] &gt; stats[\"unique_lists\"]\n        else \"no duplicates found\"\n    )\n</code></pre>"}]}